{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7da7efb0ce94c78b4594a7f8eed4820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0895190f97004b8699363aa6d8f13fc3",
              "IPY_MODEL_908f7a29c409437099530f08e3be27a5",
              "IPY_MODEL_b9405c8d40a2402bbadd769f37760d96"
            ],
            "layout": "IPY_MODEL_3e506bf19c9f465cb3df893968149118"
          }
        },
        "0895190f97004b8699363aa6d8f13fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b317cc50fbcd44ea89bb9c3d1bb553a9",
            "placeholder": "​",
            "style": "IPY_MODEL_04e50d3d8fac4bc7a99de4fff654e1bd",
            "value": "Downloading (…)rocessor_config.json: 100%"
          }
        },
        "908f7a29c409437099530f08e3be27a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30767f6ac92d4e13bba51024761c7a9e",
            "max": 362,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0808f35390934b3b9453c0fc9692a0a7",
            "value": 362
          }
        },
        "b9405c8d40a2402bbadd769f37760d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f82c99c1ec249a68e44c213ce2bf815",
            "placeholder": "​",
            "style": "IPY_MODEL_f568b20307a0428aa7f2b1a933f7a708",
            "value": " 362/362 [00:00&lt;00:00, 14.0kB/s]"
          }
        },
        "3e506bf19c9f465cb3df893968149118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b317cc50fbcd44ea89bb9c3d1bb553a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04e50d3d8fac4bc7a99de4fff654e1bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30767f6ac92d4e13bba51024761c7a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0808f35390934b3b9453c0fc9692a0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f82c99c1ec249a68e44c213ce2bf815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f568b20307a0428aa7f2b1a933f7a708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56d1355146c14fa79be2966b6c0dff1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_493d109e9aca40139f7cd88c54b81890",
              "IPY_MODEL_44164d915b5843dbad80d8766f074c8e",
              "IPY_MODEL_a9e248a637424f54a070b1dbca2f4de3"
            ],
            "layout": "IPY_MODEL_326a8dcafd2e4deb8e0363daa11d3fcd"
          }
        },
        "493d109e9aca40139f7cd88c54b81890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33882ebfbf9f47d2b25c78d51b59935e",
            "placeholder": "​",
            "style": "IPY_MODEL_bb6eda59b35146bba2fa4db25a68f129",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "44164d915b5843dbad80d8766f074c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_017d3640c3ce4f8cb1f3036ddf4f9669",
            "max": 518,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec89932a7b2a4c038471f2ce8f7858c9",
            "value": 518
          }
        },
        "a9e248a637424f54a070b1dbca2f4de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da71f60e1aaa46b2a11a367d73b90b1f",
            "placeholder": "​",
            "style": "IPY_MODEL_28f13468b4044ef0bbd161cd2cda68ee",
            "value": " 518/518 [00:00&lt;00:00, 28.0kB/s]"
          }
        },
        "326a8dcafd2e4deb8e0363daa11d3fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33882ebfbf9f47d2b25c78d51b59935e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb6eda59b35146bba2fa4db25a68f129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "017d3640c3ce4f8cb1f3036ddf4f9669": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec89932a7b2a4c038471f2ce8f7858c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da71f60e1aaa46b2a11a367d73b90b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28f13468b4044ef0bbd161cd2cda68ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0b34f4c3a8244e28c9eb8760bc0de3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_485653624828448196dc9d6998eb131a",
              "IPY_MODEL_0c03ec9bf54b44bcb32ca4a2c9b4704c",
              "IPY_MODEL_6834adf4dd2f43cd9a9cfa7155f75bdc"
            ],
            "layout": "IPY_MODEL_07649bf39b464764807e036adae12a42"
          }
        },
        "485653624828448196dc9d6998eb131a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16f9a839d8014c0b9e47b27a3807452e",
            "placeholder": "​",
            "style": "IPY_MODEL_c31fd517557e47f8a2250eb65a46b4b3",
            "value": "Downloading (…)tencepiece.bpe.model: 100%"
          }
        },
        "0c03ec9bf54b44bcb32ca4a2c9b4704c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01fd786e43ca427493a1bb70bd39e4fa",
            "max": 1296245,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e19f320f11fc41b4b638e5eccfa7f075",
            "value": 1296245
          }
        },
        "6834adf4dd2f43cd9a9cfa7155f75bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d68780b19b6c492c95d9a4595844b265",
            "placeholder": "​",
            "style": "IPY_MODEL_fe98383e6a534edb96f413eda0b7fd80",
            "value": " 1.30M/1.30M [00:01&lt;00:00, 1.26MB/s]"
          }
        },
        "07649bf39b464764807e036adae12a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f9a839d8014c0b9e47b27a3807452e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c31fd517557e47f8a2250eb65a46b4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01fd786e43ca427493a1bb70bd39e4fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e19f320f11fc41b4b638e5eccfa7f075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d68780b19b6c492c95d9a4595844b265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe98383e6a534edb96f413eda0b7fd80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c589ee988a1c4762a5221db3d47a99ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_327ff9a62eaf4ad88dfff8fbcc1ae9e8",
              "IPY_MODEL_cdf3e362232a4a69b7289bb284421982",
              "IPY_MODEL_55d56ca19fbc48c888ceafb646c54f18"
            ],
            "layout": "IPY_MODEL_459bdc7fd38342d48e308b085bd7b099"
          }
        },
        "327ff9a62eaf4ad88dfff8fbcc1ae9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feef2349dea34f068dcfd62893c980fc",
            "placeholder": "​",
            "style": "IPY_MODEL_577f1b786cac47718d01c8d8637c9d4c",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "cdf3e362232a4a69b7289bb284421982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf5e2dc702c4124b7e7ec54ce8540fd",
            "max": 4011031,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b40ad846c9394458b4ee3ee3392729c4",
            "value": 4011031
          }
        },
        "55d56ca19fbc48c888ceafb646c54f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90f55df0523a4bfd9e2e37e790875c32",
            "placeholder": "​",
            "style": "IPY_MODEL_825b1967d4ff4906a14acc53331b185b",
            "value": " 4.01M/4.01M [00:01&lt;00:00, 3.88MB/s]"
          }
        },
        "459bdc7fd38342d48e308b085bd7b099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feef2349dea34f068dcfd62893c980fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "577f1b786cac47718d01c8d8637c9d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdf5e2dc702c4124b7e7ec54ce8540fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b40ad846c9394458b4ee3ee3392729c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90f55df0523a4bfd9e2e37e790875c32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "825b1967d4ff4906a14acc53331b185b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b167cf4d38d40289e0ec054399e9900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d22bb5d3c94498081b3e5337731d712",
              "IPY_MODEL_a84035229c094abaa7172a5f1627c6dc",
              "IPY_MODEL_a11b4537cda04d38a52dea0e53357967"
            ],
            "layout": "IPY_MODEL_f3538b9bb06843039a1207acd9d778a3"
          }
        },
        "7d22bb5d3c94498081b3e5337731d712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4acffc4e311b4d41a37e5fe07940ed6b",
            "placeholder": "​",
            "style": "IPY_MODEL_1eef202031114694bbce84de4fd8fb63",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "a84035229c094abaa7172a5f1627c6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5103c097ea214509af751f393b0a2e24",
            "max": 71,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a371d7ecb96946888d9d7163a62d0368",
            "value": 71
          }
        },
        "a11b4537cda04d38a52dea0e53357967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e731e618f7a493c840714d2db9d32b4",
            "placeholder": "​",
            "style": "IPY_MODEL_b9ec3b9a333d4fb981f30863b64163de",
            "value": " 71.0/71.0 [00:00&lt;00:00, 5.57kB/s]"
          }
        },
        "f3538b9bb06843039a1207acd9d778a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4acffc4e311b4d41a37e5fe07940ed6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eef202031114694bbce84de4fd8fb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5103c097ea214509af751f393b0a2e24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a371d7ecb96946888d9d7163a62d0368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e731e618f7a493c840714d2db9d32b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9ec3b9a333d4fb981f30863b64163de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89366503d419492ca16a1b40951521a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0753e140e84449f6a56d8ca4acce3fbd",
              "IPY_MODEL_b351e7efc16d4132b85487877bf9f613",
              "IPY_MODEL_843091a9af8f4f2ab5f6a6442c220160"
            ],
            "layout": "IPY_MODEL_6dff655e42b141beb9ebd9df0636427b"
          }
        },
        "0753e140e84449f6a56d8ca4acce3fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23f444d1824943bd92b2cc9dd0446ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_8ad9069f9e1e4ba0a6d979398043da72",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "b351e7efc16d4132b85487877bf9f613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de62717e67784090b5e17d58a289d139",
            "max": 355,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d97d904460544caca98954ae3eb8ef43",
            "value": 355
          }
        },
        "843091a9af8f4f2ab5f6a6442c220160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bce75faf782452ea8c431206c202880",
            "placeholder": "​",
            "style": "IPY_MODEL_f9e3c44cbc1245d4812fcfbdf4151a3c",
            "value": " 355/355 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "6dff655e42b141beb9ebd9df0636427b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f444d1824943bd92b2cc9dd0446ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad9069f9e1e4ba0a6d979398043da72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de62717e67784090b5e17d58a289d139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d97d904460544caca98954ae3eb8ef43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bce75faf782452ea8c431206c202880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9e3c44cbc1245d4812fcfbdf4151a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fc0cac0ca2f46f3bdd315324a8d2b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e475b5295df24701b9cd12ba497826c4",
              "IPY_MODEL_48a6726353a245b9a1e3b27343a489a2",
              "IPY_MODEL_48a3738424db426eb2106b491de58e99"
            ],
            "layout": "IPY_MODEL_22af3ee22d494b45830fe9a4ba3dddc6"
          }
        },
        "e475b5295df24701b9cd12ba497826c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_262f179f388f447398cd5af036e51d6f",
            "placeholder": "​",
            "style": "IPY_MODEL_c56c7f8bceaf4991b74f81dc3a2d0628",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "48a6726353a245b9a1e3b27343a489a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25c5520285524620b9446a2a276d7771",
            "max": 4742,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_383d622d710c4c13b9eb71a4f243592b",
            "value": 4742
          }
        },
        "48a3738424db426eb2106b491de58e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dd8cf7481fe4894bfbd927f56c6c3ec",
            "placeholder": "​",
            "style": "IPY_MODEL_16c929bc12884fcca3a789f9e9e68b3b",
            "value": " 4.74k/4.74k [00:00&lt;00:00, 272kB/s]"
          }
        },
        "22af3ee22d494b45830fe9a4ba3dddc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262f179f388f447398cd5af036e51d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c56c7f8bceaf4991b74f81dc3a2d0628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25c5520285524620b9446a2a276d7771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "383d622d710c4c13b9eb71a4f243592b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dd8cf7481fe4894bfbd927f56c6c3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c929bc12884fcca3a789f9e9e68b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "553f8b9e057c4837853d51c84be188e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6d6d49913a34c22a616cfa1ec64a41d",
              "IPY_MODEL_25fbdf3bb3de49db8e9820627e81aa49",
              "IPY_MODEL_620cb763c74c4f4d9ca67a4f9246d426"
            ],
            "layout": "IPY_MODEL_b630efed74bf4795b8ec6a6cbb5aa01b"
          }
        },
        "b6d6d49913a34c22a616cfa1ec64a41d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af8a5a01c304e6fbe8af404e44464d7",
            "placeholder": "​",
            "style": "IPY_MODEL_8571eb9788ae433b94710d6cce90a264",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "25fbdf3bb3de49db8e9820627e81aa49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a06f37ad473463c96d71ca2407cdd92",
            "max": 809168699,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03e69a18706b4e108bbe562eabf1c2ce",
            "value": 809168699
          }
        },
        "620cb763c74c4f4d9ca67a4f9246d426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc03a1669ae24e9e89e1569084884dcb",
            "placeholder": "​",
            "style": "IPY_MODEL_2d181dd8a2e748c89756b2f6b9785617",
            "value": " 809M/809M [00:39&lt;00:00, 19.5MB/s]"
          }
        },
        "b630efed74bf4795b8ec6a6cbb5aa01b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8af8a5a01c304e6fbe8af404e44464d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8571eb9788ae433b94710d6cce90a264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a06f37ad473463c96d71ca2407cdd92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03e69a18706b4e108bbe562eabf1c2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc03a1669ae24e9e89e1569084884dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d181dd8a2e748c89756b2f6b9785617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b59886a7bbd44b31aab08558b3d68a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e173f8a809504c38941bd99de8fe9dff",
              "IPY_MODEL_a95eddf1d1d743a993ad546e7fc91201",
              "IPY_MODEL_38adf038c15443c7b2b6bfbf7c0a1e7f"
            ],
            "layout": "IPY_MODEL_9dc559b1a9984a2fb2d4210835ceb84f"
          }
        },
        "e173f8a809504c38941bd99de8fe9dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bafea91fb0f486aaf36acc449ea9bbf",
            "placeholder": "​",
            "style": "IPY_MODEL_fe6be0e8e2e44128845849e25a65736a",
            "value": "Map: 100%"
          }
        },
        "a95eddf1d1d743a993ad546e7fc91201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b64b5f661f54e75aa2c3309f90ddcb6",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c7351e30d9844cf864f6d3525d1302b",
            "value": 200
          }
        },
        "38adf038c15443c7b2b6bfbf7c0a1e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f070e43f514dbe8482d0f79e280fdf",
            "placeholder": "​",
            "style": "IPY_MODEL_1299a5cda6294361a273fa5f8a2f4b8e",
            "value": " 200/200 [00:00&lt;00:00, 7211.48 examples/s]"
          }
        },
        "9dc559b1a9984a2fb2d4210835ceb84f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bafea91fb0f486aaf36acc449ea9bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe6be0e8e2e44128845849e25a65736a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b64b5f661f54e75aa2c3309f90ddcb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7351e30d9844cf864f6d3525d1302b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4f070e43f514dbe8482d0f79e280fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1299a5cda6294361a273fa5f8a2f4b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Contents and Objectives:\n",
        "This notebook contains details of\n",
        "- transformers package\n",
        "- 'DonutFeatureExtractor', 'DonutProcessor'\n",
        "- processor, processor.tokenizer behaviour\n",
        "- VisionEncoderDecoderModel\n",
        "- processor.tokenizer.convert_tokens_to_ids\n",
        "- Decoder Input id creation and prompt creation\n",
        "- decoder_input_ids\n",
        "- huggingface datasets class and it's behaviour\n",
        "- VisionEncoderDecoderConfig\n",
        "- Explanation of decoder_input_ids and labels tensor\n",
        "- explanation of add_tokens\n"
      ],
      "metadata": {
        "id": "0faxCJj9szzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important Resource:\n",
        "- https://chatbotslife.com/information-extraction-with-donut-1a7267ab6993\n",
        "- https://www.analyticsvidhya.com/blog/2023/03/revolutionizing-document-processing-through-docvqa/\n",
        "- https://colab.research.google.com/drive/16iPnVD68oMnCqxHcLaq9qn9zkRaIGeab?usp=sharing#scrollTo=NimOFZS9FPqV\n",
        "- https://www.philschmid.de/fine-tuning-donut\n",
        "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Donut/DocVQA/Fine_tune_Donut_on_DocVQA.ipynb"
      ],
      "metadata": {
        "id": "n0j5yuPQtfXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "OCSqThbN-i4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21814013-56cb-42a8-d686-e5a2b54d466f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.4/492.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUbXJp5W8_Mc",
        "outputId": "96a5eafe-7b6a-4b42-9d95-de4ce24c9a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.65.0)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUXZjN5U6Xe9",
        "outputId": "1b4495e8-c07c-41a8-a1ff-1e90c010b628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing"
      ],
      "metadata": {
        "id": "_JQ_DaYLQjOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import inspect\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "# from donut import DonutModel\n",
        "from transformers import DonutProcessor, VisionEncoderDecoderModel"
      ],
      "metadata": {
        "id": "q9YZualYgvHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83d3047-9144-41a3-fba4-a05fef66c080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path_main = '/content/drive/MyDrive/docVqa_dataset/train/train_v1.0.json'"
      ],
      "metadata": {
        "id": "4PLX5DIF5ROo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the JSON file\n",
        "with open(train_path_main, 'r') as file:\n",
        "  # Load the contents of the file into a dictionary\n",
        "  data_train = json.load(file)\n",
        "\n",
        "## Convert 'data' list into dataframe by passing into the dataframe\n",
        "df_train = pd.DataFrame(data_train['data'])\n",
        "\n",
        "## Creating new dataframe having full image path\n",
        "\n",
        "base_train_path = '/content/drive/MyDrive/docVqa_dataset/train'\n",
        "\n",
        "# Define a function to join the base path with the image path\n",
        "def join_paths(image_path):\n",
        "    return os.path.join(base_train_path, image_path)\n",
        "\n",
        "# Apply the function to create a new column with the full image path\n",
        "df_train['full_path_image'] = df_train['image'].apply(join_paths)\n",
        "\n",
        "## Making subset of the training dataframe\n",
        "df_sub = df_train.iloc[:200]\n",
        "\n",
        "print(df_sub.shape)\n",
        "print(df_sub.head())\n",
        "\n",
        "## Convert dataframe into dataset object\n",
        "hfdataset_sub = Dataset.from_pandas(df_sub)\n",
        "\n",
        "print(hfdataset_sub.shape)\n",
        "\n",
        "print(len(hfdataset_sub))\n",
        "\n",
        "print(hfdataset_sub.column_names)\n",
        "print(hfdataset_sub[0])"
      ],
      "metadata": {
        "id": "jf4DHv6Q5RLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfc9ad2-41c7-4b36-aa10-dfde6e1591f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 9)\n",
            "   questionId                                           question  \\\n",
            "0         337         what is the date mentioned in this letter?   \n",
            "1         338  what is the contact person name mentioned in l...   \n",
            "2         339            Which corporation's letterhead is this?   \n",
            "3         340                      Who is in  cc in this letter?   \n",
            "4         341               what is the subject of  this letter?   \n",
            "\n",
            "                      image  docId ucsf_document_id ucsf_document_page_no  \\\n",
            "0  documents/xnbl0037_1.png    279         xnbl0037                     1   \n",
            "1  documents/xnbl0037_1.png    279         xnbl0037                     1   \n",
            "2  documents/mxcj0037_1.png    280         mxcj0037                     1   \n",
            "3  documents/mxcj0037_1.png    280         mxcj0037                     1   \n",
            "4  documents/mxcj0037_1.png    280         mxcj0037                     1   \n",
            "\n",
            "                                        answers data_split  \\\n",
            "0                                      [1/8/93]      train   \n",
            "1                        [P. Carter, p. carter]      train   \n",
            "2      [Brown & Williamson Tobacco Corporation]      train   \n",
            "3                                  [T.F. Riehl]      train   \n",
            "4  [Review of existing Brainstorming Ideas/483]      train   \n",
            "\n",
            "                                     full_path_image  \n",
            "0  /content/drive/MyDrive/docVqa_dataset/train/do...  \n",
            "1  /content/drive/MyDrive/docVqa_dataset/train/do...  \n",
            "2  /content/drive/MyDrive/docVqa_dataset/train/do...  \n",
            "3  /content/drive/MyDrive/docVqa_dataset/train/do...  \n",
            "4  /content/drive/MyDrive/docVqa_dataset/train/do...  \n",
            "(200, 9)\n",
            "200\n",
            "['questionId', 'question', 'image', 'docId', 'ucsf_document_id', 'ucsf_document_page_no', 'answers', 'data_split', 'full_path_image']\n",
            "{'questionId': 337, 'question': 'what is the date mentioned in this letter?', 'image': 'documents/xnbl0037_1.png', 'docId': 279, 'ucsf_document_id': 'xnbl0037', 'ucsf_document_page_no': '1', 'answers': ['1/8/93'], 'data_split': 'train', 'full_path_image': '/content/drive/MyDrive/docVqa_dataset/train/documents/xnbl0037_1.png'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Digging details of donut and visionencoder decoder class from transformers module/package"
      ],
      "metadata": {
        "id": "zm6hUs1Uim62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details of transformers packackage:"
      ],
      "metadata": {
        "id": "EftyNoEj6ICI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "gIORyU-3J_Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(transformers))"
      ],
      "metadata": {
        "id": "FLmOdpezJ_ME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cc2c30-d5fe-425c-db19-40310f300fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ALIGN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALIGN_PRETRAINED_MODEL_ARCHIVE_LIST', 'ALL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALTCLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALTCLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'ASTConfig', 'ASTFeatureExtractor', 'ASTForAudioClassification', 'ASTModel', 'ASTPreTrainedModel', 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'AUTOFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'AUTOFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'Adafactor', 'AdamW', 'AdamWeightDecay', 'AdaptiveEmbedding', 'AddedToken', 'Agent', 'AlbertConfig', 'AlbertForMaskedLM', 'AlbertForMultipleChoice', 'AlbertForPreTraining', 'AlbertForQuestionAnswering', 'AlbertForSequenceClassification', 'AlbertForTokenClassification', 'AlbertModel', 'AlbertPreTrainedModel', 'AlbertTokenizer', 'AlbertTokenizerFast', 'AlignConfig', 'AlignModel', 'AlignPreTrainedModel', 'AlignProcessor', 'AlignTextConfig', 'AlignTextModel', 'AlignVisionConfig', 'AlignVisionModel', 'AltCLIPConfig', 'AltCLIPModel', 'AltCLIPPreTrainedModel', 'AltCLIPProcessor', 'AltCLIPTextConfig', 'AltCLIPTextModel', 'AltCLIPVisionConfig', 'AltCLIPVisionModel', 'AudioClassificationPipeline', 'AutoBackbone', 'AutoConfig', 'AutoFeatureExtractor', 'AutoImageProcessor', 'AutoModel', 'AutoModelForAudioClassification', 'AutoModelForAudioFrameClassification', 'AutoModelForAudioXVector', 'AutoModelForCTC', 'AutoModelForCausalLM', 'AutoModelForDepthEstimation', 'AutoModelForDocumentQuestionAnswering', 'AutoModelForImageClassification', 'AutoModelForImageSegmentation', 'AutoModelForInstanceSegmentation', 'AutoModelForMaskGeneration', 'AutoModelForMaskedImageModeling', 'AutoModelForMaskedLM', 'AutoModelForMultipleChoice', 'AutoModelForNextSentencePrediction', 'AutoModelForObjectDetection', 'AutoModelForPreTraining', 'AutoModelForQuestionAnswering', 'AutoModelForSemanticSegmentation', 'AutoModelForSeq2SeqLM', 'AutoModelForSequenceClassification', 'AutoModelForSpeechSeq2Seq', 'AutoModelForTableQuestionAnswering', 'AutoModelForTextEncoding', 'AutoModelForTokenClassification', 'AutoModelForUniversalSegmentation', 'AutoModelForVideoClassification', 'AutoModelForVision2Seq', 'AutoModelForVisualQuestionAnswering', 'AutoModelForZeroShotImageClassification', 'AutoModelForZeroShotObjectDetection', 'AutoModelWithLMHead', 'AutoProcessor', 'AutoTokenizer', 'AutoformerConfig', 'AutoformerForPrediction', 'AutoformerModel', 'AutoformerPreTrainedModel', 'AutomaticSpeechRecognitionPipeline', 'AzureOpenAiAgent', 'BARK_PRETRAINED_MODEL_ARCHIVE_LIST', 'BART_PRETRAINED_MODEL_ARCHIVE_LIST', 'BEIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIGBIRD_PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIGBIRD_PEGASUS_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIG_BIRD_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIG_BIRD_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIOGPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIOGPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLENDERBOT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLENDERBOT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLENDERBOT_SMALL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLENDERBOT_SMALL_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLIP_2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLIP_2_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLOOM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLOOM_PRETRAINED_MODEL_ARCHIVE_LIST', 'BRIDGETOWER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BRIDGETOWER_PRETRAINED_MODEL_ARCHIVE_LIST', 'BarkCausalModel', 'BarkCoarseConfig', 'BarkCoarseModel', 'BarkConfig', 'BarkFineConfig', 'BarkFineModel', 'BarkModel', 'BarkPreTrainedModel', 'BarkProcessor', 'BarkSemanticConfig', 'BarkSemanticModel', 'BartConfig', 'BartForCausalLM', 'BartForConditionalGeneration', 'BartForQuestionAnswering', 'BartForSequenceClassification', 'BartModel', 'BartPretrainedModel', 'BartTokenizer', 'BartTokenizerFast', 'BarthezTokenizer', 'BarthezTokenizerFast', 'BartphoTokenizer', 'BasicTokenizer', 'BatchEncoding', 'BatchFeature', 'BeamScorer', 'BeamSearchScorer', 'BeitConfig', 'BeitFeatureExtractor', 'BeitForImageClassification', 'BeitForMaskedImageModeling', 'BeitForSemanticSegmentation', 'BeitImageProcessor', 'BeitModel', 'BeitPreTrainedModel', 'BertConfig', 'BertForMaskedLM', 'BertForMultipleChoice', 'BertForNextSentencePrediction', 'BertForPreTraining', 'BertForQuestionAnswering', 'BertForSequenceClassification', 'BertForTokenClassification', 'BertGenerationConfig', 'BertGenerationDecoder', 'BertGenerationEncoder', 'BertGenerationPreTrainedModel', 'BertGenerationTokenizer', 'BertJapaneseTokenizer', 'BertLMHeadModel', 'BertLayer', 'BertModel', 'BertPreTrainedModel', 'BertTokenizer', 'BertTokenizerFast', 'BertweetTokenizer', 'BigBirdConfig', 'BigBirdForCausalLM', 'BigBirdForMaskedLM', 'BigBirdForMultipleChoice', 'BigBirdForPreTraining', 'BigBirdForQuestionAnswering', 'BigBirdForSequenceClassification', 'BigBirdForTokenClassification', 'BigBirdLayer', 'BigBirdModel', 'BigBirdPegasusConfig', 'BigBirdPegasusForCausalLM', 'BigBirdPegasusForConditionalGeneration', 'BigBirdPegasusForQuestionAnswering', 'BigBirdPegasusForSequenceClassification', 'BigBirdPegasusModel', 'BigBirdPegasusPreTrainedModel', 'BigBirdPreTrainedModel', 'BigBirdTokenizer', 'BigBirdTokenizerFast', 'BioGptConfig', 'BioGptForCausalLM', 'BioGptForSequenceClassification', 'BioGptForTokenClassification', 'BioGptModel', 'BioGptPreTrainedModel', 'BioGptTokenizer', 'BitBackbone', 'BitConfig', 'BitForImageClassification', 'BitImageProcessor', 'BitModel', 'BitPreTrainedModel', 'BitsAndBytesConfig', 'BlenderbotConfig', 'BlenderbotForCausalLM', 'BlenderbotForConditionalGeneration', 'BlenderbotModel', 'BlenderbotPreTrainedModel', 'BlenderbotSmallConfig', 'BlenderbotSmallForCausalLM', 'BlenderbotSmallForConditionalGeneration', 'BlenderbotSmallModel', 'BlenderbotSmallPreTrainedModel', 'BlenderbotSmallTokenizer', 'BlenderbotSmallTokenizerFast', 'BlenderbotTokenizer', 'BlenderbotTokenizerFast', 'Blip2Config', 'Blip2ForConditionalGeneration', 'Blip2Model', 'Blip2PreTrainedModel', 'Blip2Processor', 'Blip2QFormerConfig', 'Blip2QFormerModel', 'Blip2VisionConfig', 'Blip2VisionModel', 'BlipConfig', 'BlipForConditionalGeneration', 'BlipForImageTextRetrieval', 'BlipForQuestionAnswering', 'BlipImageProcessor', 'BlipModel', 'BlipPreTrainedModel', 'BlipProcessor', 'BlipTextConfig', 'BlipTextModel', 'BlipVisionConfig', 'BlipVisionModel', 'BloomConfig', 'BloomForCausalLM', 'BloomForQuestionAnswering', 'BloomForSequenceClassification', 'BloomForTokenClassification', 'BloomModel', 'BloomPreTrainedModel', 'BloomTokenizerFast', 'BridgeTowerConfig', 'BridgeTowerForContrastiveLearning', 'BridgeTowerForImageAndTextRetrieval', 'BridgeTowerForMaskedLM', 'BridgeTowerImageProcessor', 'BridgeTowerModel', 'BridgeTowerPreTrainedModel', 'BridgeTowerProcessor', 'BridgeTowerTextConfig', 'BridgeTowerVisionConfig', 'ByT5Tokenizer', 'CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CANINE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CANINE_PRETRAINED_MODEL_ARCHIVE_LIST', 'CHINESE_CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CHINESE_CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'CLAP_PRETRAINED_MODEL_ARCHIVE_LIST', 'CLIPConfig', 'CLIPFeatureExtractor', 'CLIPImageProcessor', 'CLIPModel', 'CLIPPreTrainedModel', 'CLIPProcessor', 'CLIPSEG_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CLIPSEG_PRETRAINED_MODEL_ARCHIVE_LIST', 'CLIPSegConfig', 'CLIPSegForImageSegmentation', 'CLIPSegModel', 'CLIPSegPreTrainedModel', 'CLIPSegProcessor', 'CLIPSegTextConfig', 'CLIPSegTextModel', 'CLIPSegVisionConfig', 'CLIPSegVisionModel', 'CLIPTextConfig', 'CLIPTextModel', 'CLIPTextModelWithProjection', 'CLIPTokenizer', 'CLIPTokenizerFast', 'CLIPVisionConfig', 'CLIPVisionModel', 'CLIPVisionModelWithProjection', 'CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'CODEGEN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONDITIONAL_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONDITIONAL_DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONFIG_MAPPING', 'CONFIG_NAME', 'CONVBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONVBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONVNEXTV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONVNEXTV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONVNEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONVNEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CPMANT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CPMANT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CTRLConfig', 'CTRLForSequenceClassification', 'CTRLLMHeadModel', 'CTRLModel', 'CTRLPreTrainedModel', 'CTRLTokenizer', 'CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CTRL_PRETRAINED_MODEL_ARCHIVE_LIST', 'CVT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CVT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CamembertConfig', 'CamembertForCausalLM', 'CamembertForMaskedLM', 'CamembertForMultipleChoice', 'CamembertForQuestionAnswering', 'CamembertForSequenceClassification', 'CamembertForTokenClassification', 'CamembertModel', 'CamembertPreTrainedModel', 'CamembertTokenizer', 'CamembertTokenizerFast', 'CanineConfig', 'CanineForMultipleChoice', 'CanineForQuestionAnswering', 'CanineForSequenceClassification', 'CanineForTokenClassification', 'CanineLayer', 'CanineModel', 'CaninePreTrainedModel', 'CanineTokenizer', 'CharSpan', 'CharacterTokenizer', 'ChineseCLIPConfig', 'ChineseCLIPFeatureExtractor', 'ChineseCLIPImageProcessor', 'ChineseCLIPModel', 'ChineseCLIPPreTrainedModel', 'ChineseCLIPProcessor', 'ChineseCLIPTextConfig', 'ChineseCLIPTextModel', 'ChineseCLIPVisionConfig', 'ChineseCLIPVisionModel', 'ClapAudioConfig', 'ClapAudioModel', 'ClapAudioModelWithProjection', 'ClapConfig', 'ClapFeatureExtractor', 'ClapModel', 'ClapPreTrainedModel', 'ClapProcessor', 'ClapTextConfig', 'ClapTextModel', 'ClapTextModelWithProjection', 'CodeGenConfig', 'CodeGenForCausalLM', 'CodeGenModel', 'CodeGenPreTrainedModel', 'CodeGenTokenizer', 'CodeGenTokenizerFast', 'ConditionalDetrConfig', 'ConditionalDetrFeatureExtractor', 'ConditionalDetrForObjectDetection', 'ConditionalDetrForSegmentation', 'ConditionalDetrImageProcessor', 'ConditionalDetrModel', 'ConditionalDetrPreTrainedModel', 'ConstrainedBeamSearchScorer', 'Constraint', 'ConstraintListState', 'Conv1D', 'ConvBertConfig', 'ConvBertForMaskedLM', 'ConvBertForMultipleChoice', 'ConvBertForQuestionAnswering', 'ConvBertForSequenceClassification', 'ConvBertForTokenClassification', 'ConvBertLayer', 'ConvBertModel', 'ConvBertPreTrainedModel', 'ConvBertTokenizer', 'ConvBertTokenizerFast', 'ConvNextBackbone', 'ConvNextConfig', 'ConvNextFeatureExtractor', 'ConvNextForImageClassification', 'ConvNextImageProcessor', 'ConvNextModel', 'ConvNextPreTrainedModel', 'ConvNextV2Backbone', 'ConvNextV2Config', 'ConvNextV2ForImageClassification', 'ConvNextV2Model', 'ConvNextV2PreTrainedModel', 'Conversation', 'ConversationalPipeline', 'CpmAntConfig', 'CpmAntForCausalLM', 'CpmAntModel', 'CpmAntPreTrainedModel', 'CpmAntTokenizer', 'CpmTokenizer', 'CpmTokenizerFast', 'CsvPipelineDataFormat', 'CvtConfig', 'CvtForImageClassification', 'CvtModel', 'CvtPreTrainedModel', 'DATA2VEC_AUDIO_PRETRAINED_MODEL_ARCHIVE_LIST', 'DATA2VEC_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DATA2VEC_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DATA2VEC_VISION_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DATA2VEC_VISION_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEBERTA_V2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'DECISION_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DECISION_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEFORMABLE_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEFORMABLE_DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DETA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DETA_PRETRAINED_MODEL_ARCHIVE_LIST', 'DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'DINAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DINAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DONUT_SWIN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DONUT_SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPRConfig', 'DPRContextEncoder', 'DPRContextEncoderTokenizer', 'DPRContextEncoderTokenizerFast', 'DPRPreTrainedModel', 'DPRPretrainedContextEncoder', 'DPRPretrainedQuestionEncoder', 'DPRPretrainedReader', 'DPRQuestionEncoder', 'DPRQuestionEncoderTokenizer', 'DPRQuestionEncoderTokenizerFast', 'DPRReader', 'DPRReaderOutput', 'DPRReaderTokenizer', 'DPRReaderTokenizerFast', 'DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPTConfig', 'DPTFeatureExtractor', 'DPTForDepthEstimation', 'DPTForSemanticSegmentation', 'DPTImageProcessor', 'DPTModel', 'DPTPreTrainedModel', 'DPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'Data2VecAudioConfig', 'Data2VecAudioForAudioFrameClassification', 'Data2VecAudioForCTC', 'Data2VecAudioForSequenceClassification', 'Data2VecAudioForXVector', 'Data2VecAudioModel', 'Data2VecAudioPreTrainedModel', 'Data2VecTextConfig', 'Data2VecTextForCausalLM', 'Data2VecTextForMaskedLM', 'Data2VecTextForMultipleChoice', 'Data2VecTextForQuestionAnswering', 'Data2VecTextForSequenceClassification', 'Data2VecTextForTokenClassification', 'Data2VecTextModel', 'Data2VecTextPreTrainedModel', 'Data2VecVisionConfig', 'Data2VecVisionForImageClassification', 'Data2VecVisionForSemanticSegmentation', 'Data2VecVisionModel', 'Data2VecVisionPreTrainedModel', 'DataCollator', 'DataCollatorForLanguageModeling', 'DataCollatorForPermutationLanguageModeling', 'DataCollatorForSOP', 'DataCollatorForSeq2Seq', 'DataCollatorForTokenClassification', 'DataCollatorForWholeWordMask', 'DataCollatorWithPadding', 'DataProcessor', 'DebertaConfig', 'DebertaForMaskedLM', 'DebertaForQuestionAnswering', 'DebertaForSequenceClassification', 'DebertaForTokenClassification', 'DebertaModel', 'DebertaPreTrainedModel', 'DebertaTokenizer', 'DebertaTokenizerFast', 'DebertaV2Config', 'DebertaV2ForMaskedLM', 'DebertaV2ForMultipleChoice', 'DebertaV2ForQuestionAnswering', 'DebertaV2ForSequenceClassification', 'DebertaV2ForTokenClassification', 'DebertaV2Model', 'DebertaV2PreTrainedModel', 'DebertaV2Tokenizer', 'DebertaV2TokenizerFast', 'DecisionTransformerConfig', 'DecisionTransformerGPT2Model', 'DecisionTransformerGPT2PreTrainedModel', 'DecisionTransformerModel', 'DecisionTransformerPreTrainedModel', 'DefaultDataCollator', 'DefaultFlowCallback', 'DeformableDetrConfig', 'DeformableDetrFeatureExtractor', 'DeformableDetrForObjectDetection', 'DeformableDetrImageProcessor', 'DeformableDetrModel', 'DeformableDetrPreTrainedModel', 'DeiTConfig', 'DeiTFeatureExtractor', 'DeiTForImageClassification', 'DeiTForImageClassificationWithTeacher', 'DeiTForMaskedImageModeling', 'DeiTImageProcessor', 'DeiTModel', 'DeiTPreTrainedModel', 'DepthEstimationPipeline', 'DetaConfig', 'DetaForObjectDetection', 'DetaImageProcessor', 'DetaModel', 'DetaPreTrainedModel', 'DetrConfig', 'DetrFeatureExtractor', 'DetrForObjectDetection', 'DetrForSegmentation', 'DetrImageProcessor', 'DetrModel', 'DetrPreTrainedModel', 'DinatBackbone', 'DinatConfig', 'DinatForImageClassification', 'DinatModel', 'DinatPreTrainedModel', 'DisjunctiveConstraint', 'DistilBertConfig', 'DistilBertForMaskedLM', 'DistilBertForMultipleChoice', 'DistilBertForQuestionAnswering', 'DistilBertForSequenceClassification', 'DistilBertForTokenClassification', 'DistilBertModel', 'DistilBertPreTrainedModel', 'DistilBertTokenizer', 'DistilBertTokenizerFast', 'DocumentQuestionAnsweringPipeline', 'DonutFeatureExtractor', 'DonutImageProcessor', 'DonutProcessor', 'DonutSwinConfig', 'DonutSwinModel', 'DonutSwinPreTrainedModel', 'DummyObject', 'EFFICIENTFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'EFFICIENTFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'EFFICIENTNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'EFFICIENTNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ELECTRA_PRETRAINED_MODEL_ARCHIVE_LIST', 'ENCODEC_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ENCODEC_PRETRAINED_MODEL_ARCHIVE_LIST', 'ERNIE_M_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ERNIE_M_PRETRAINED_MODEL_ARCHIVE_LIST', 'ERNIE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ERNIE_PRETRAINED_MODEL_ARCHIVE_LIST', 'ESM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ESM_PRETRAINED_MODEL_ARCHIVE_LIST', 'EarlyStoppingCallback', 'EfficientFormerConfig', 'EfficientFormerForImageClassification', 'EfficientFormerForImageClassificationWithTeacher', 'EfficientFormerImageProcessor', 'EfficientFormerModel', 'EfficientFormerPreTrainedModel', 'EfficientNetConfig', 'EfficientNetForImageClassification', 'EfficientNetImageProcessor', 'EfficientNetModel', 'EfficientNetPreTrainedModel', 'ElectraConfig', 'ElectraForCausalLM', 'ElectraForMaskedLM', 'ElectraForMultipleChoice', 'ElectraForPreTraining', 'ElectraForQuestionAnswering', 'ElectraForSequenceClassification', 'ElectraForTokenClassification', 'ElectraModel', 'ElectraPreTrainedModel', 'ElectraTokenizer', 'ElectraTokenizerFast', 'EncodecConfig', 'EncodecFeatureExtractor', 'EncodecModel', 'EncodecPreTrainedModel', 'EncoderDecoderConfig', 'EncoderDecoderModel', 'ErnieConfig', 'ErnieForCausalLM', 'ErnieForMaskedLM', 'ErnieForMultipleChoice', 'ErnieForNextSentencePrediction', 'ErnieForPreTraining', 'ErnieForQuestionAnswering', 'ErnieForSequenceClassification', 'ErnieForTokenClassification', 'ErnieMConfig', 'ErnieMForInformationExtraction', 'ErnieMForMultipleChoice', 'ErnieMForQuestionAnswering', 'ErnieMForSequenceClassification', 'ErnieMForTokenClassification', 'ErnieMModel', 'ErnieMPreTrainedModel', 'ErnieMTokenizer', 'ErnieModel', 'ErniePreTrainedModel', 'EsmConfig', 'EsmFoldPreTrainedModel', 'EsmForMaskedLM', 'EsmForProteinFolding', 'EsmForSequenceClassification', 'EsmForTokenClassification', 'EsmModel', 'EsmPreTrainedModel', 'EsmTokenizer', 'EvalPrediction', 'FALCON_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FALCON_PRETRAINED_MODEL_ARCHIVE_LIST', 'FEATURE_EXTRACTOR_MAPPING', 'FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FLAUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'FLAVA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FLAVA_PRETRAINED_MODEL_ARCHIVE_LIST', 'FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_MASKED_LM_MAPPING', 'FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'FLAX_MODEL_FOR_PRETRAINING_MAPPING', 'FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING', 'FLAX_MODEL_MAPPING', 'FLAX_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'FNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'FNetConfig', 'FNetForMaskedLM', 'FNetForMultipleChoice', 'FNetForNextSentencePrediction', 'FNetForPreTraining', 'FNetForQuestionAnswering', 'FNetForSequenceClassification', 'FNetForTokenClassification', 'FNetLayer', 'FNetModel', 'FNetPreTrainedModel', 'FNetTokenizer', 'FNetTokenizerFast', 'FOCALNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FOCALNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'FSMTConfig', 'FSMTForConditionalGeneration', 'FSMTModel', 'FSMTTokenizer', 'FSMT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FUNNEL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FUNNEL_PRETRAINED_MODEL_ARCHIVE_LIST', 'FalconConfig', 'FalconForCausalLM', 'FalconForQuestionAnswering', 'FalconForSequenceClassification', 'FalconForTokenClassification', 'FalconModel', 'FalconPreTrainedModel', 'FeatureExtractionMixin', 'FeatureExtractionPipeline', 'FillMaskPipeline', 'FlaubertConfig', 'FlaubertForMultipleChoice', 'FlaubertForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FlaubertForSequenceClassification', 'FlaubertForTokenClassification', 'FlaubertModel', 'FlaubertPreTrainedModel', 'FlaubertTokenizer', 'FlaubertWithLMHeadModel', 'FlavaConfig', 'FlavaFeatureExtractor', 'FlavaForPreTraining', 'FlavaImageCodebook', 'FlavaImageCodebookConfig', 'FlavaImageConfig', 'FlavaImageModel', 'FlavaImageProcessor', 'FlavaModel', 'FlavaMultimodalConfig', 'FlavaMultimodalModel', 'FlavaPreTrainedModel', 'FlavaProcessor', 'FlavaTextConfig', 'FlavaTextModel', 'FlaxAlbertForMaskedLM', 'FlaxAlbertForMultipleChoice', 'FlaxAlbertForPreTraining', 'FlaxAlbertForQuestionAnswering', 'FlaxAlbertForSequenceClassification', 'FlaxAlbertForTokenClassification', 'FlaxAlbertModel', 'FlaxAlbertPreTrainedModel', 'FlaxAutoModel', 'FlaxAutoModelForCausalLM', 'FlaxAutoModelForImageClassification', 'FlaxAutoModelForMaskedLM', 'FlaxAutoModelForMultipleChoice', 'FlaxAutoModelForNextSentencePrediction', 'FlaxAutoModelForPreTraining', 'FlaxAutoModelForQuestionAnswering', 'FlaxAutoModelForSeq2SeqLM', 'FlaxAutoModelForSequenceClassification', 'FlaxAutoModelForSpeechSeq2Seq', 'FlaxAutoModelForTokenClassification', 'FlaxAutoModelForVision2Seq', 'FlaxBartDecoderPreTrainedModel', 'FlaxBartForCausalLM', 'FlaxBartForConditionalGeneration', 'FlaxBartForQuestionAnswering', 'FlaxBartForSequenceClassification', 'FlaxBartModel', 'FlaxBartPreTrainedModel', 'FlaxBeitForImageClassification', 'FlaxBeitForMaskedImageModeling', 'FlaxBeitModel', 'FlaxBeitPreTrainedModel', 'FlaxBertForCausalLM', 'FlaxBertForMaskedLM', 'FlaxBertForMultipleChoice', 'FlaxBertForNextSentencePrediction', 'FlaxBertForPreTraining', 'FlaxBertForQuestionAnswering', 'FlaxBertForSequenceClassification', 'FlaxBertForTokenClassification', 'FlaxBertModel', 'FlaxBertPreTrainedModel', 'FlaxBigBirdForCausalLM', 'FlaxBigBirdForMaskedLM', 'FlaxBigBirdForMultipleChoice', 'FlaxBigBirdForPreTraining', 'FlaxBigBirdForQuestionAnswering', 'FlaxBigBirdForSequenceClassification', 'FlaxBigBirdForTokenClassification', 'FlaxBigBirdModel', 'FlaxBigBirdPreTrainedModel', 'FlaxBlenderbotForConditionalGeneration', 'FlaxBlenderbotModel', 'FlaxBlenderbotPreTrainedModel', 'FlaxBlenderbotSmallForConditionalGeneration', 'FlaxBlenderbotSmallModel', 'FlaxBlenderbotSmallPreTrainedModel', 'FlaxCLIPModel', 'FlaxCLIPPreTrainedModel', 'FlaxCLIPTextModel', 'FlaxCLIPTextPreTrainedModel', 'FlaxCLIPVisionModel', 'FlaxCLIPVisionPreTrainedModel', 'FlaxDistilBertForMaskedLM', 'FlaxDistilBertForMultipleChoice', 'FlaxDistilBertForQuestionAnswering', 'FlaxDistilBertForSequenceClassification', 'FlaxDistilBertForTokenClassification', 'FlaxDistilBertModel', 'FlaxDistilBertPreTrainedModel', 'FlaxElectraForCausalLM', 'FlaxElectraForMaskedLM', 'FlaxElectraForMultipleChoice', 'FlaxElectraForPreTraining', 'FlaxElectraForQuestionAnswering', 'FlaxElectraForSequenceClassification', 'FlaxElectraForTokenClassification', 'FlaxElectraModel', 'FlaxElectraPreTrainedModel', 'FlaxEncoderDecoderModel', 'FlaxForcedBOSTokenLogitsProcessor', 'FlaxForcedEOSTokenLogitsProcessor', 'FlaxGPT2LMHeadModel', 'FlaxGPT2Model', 'FlaxGPT2PreTrainedModel', 'FlaxGPTJForCausalLM', 'FlaxGPTJModel', 'FlaxGPTJPreTrainedModel', 'FlaxGPTNeoForCausalLM', 'FlaxGPTNeoModel', 'FlaxGPTNeoPreTrainedModel', 'FlaxGenerationMixin', 'FlaxLogitsProcessor', 'FlaxLogitsProcessorList', 'FlaxLogitsWarper', 'FlaxLongT5ForConditionalGeneration', 'FlaxLongT5Model', 'FlaxLongT5PreTrainedModel', 'FlaxMBartForConditionalGeneration', 'FlaxMBartForQuestionAnswering', 'FlaxMBartForSequenceClassification', 'FlaxMBartModel', 'FlaxMBartPreTrainedModel', 'FlaxMT5EncoderModel', 'FlaxMT5ForConditionalGeneration', 'FlaxMT5Model', 'FlaxMarianMTModel', 'FlaxMarianModel', 'FlaxMarianPreTrainedModel', 'FlaxMinLengthLogitsProcessor', 'FlaxOPTForCausalLM', 'FlaxOPTModel', 'FlaxOPTPreTrainedModel', 'FlaxPegasusForConditionalGeneration', 'FlaxPegasusModel', 'FlaxPegasusPreTrainedModel', 'FlaxPreTrainedModel', 'FlaxRegNetForImageClassification', 'FlaxRegNetModel', 'FlaxRegNetPreTrainedModel', 'FlaxResNetForImageClassification', 'FlaxResNetModel', 'FlaxResNetPreTrainedModel', 'FlaxRoFormerForMaskedLM', 'FlaxRoFormerForMultipleChoice', 'FlaxRoFormerForQuestionAnswering', 'FlaxRoFormerForSequenceClassification', 'FlaxRoFormerForTokenClassification', 'FlaxRoFormerModel', 'FlaxRoFormerPreTrainedModel', 'FlaxRobertaForCausalLM', 'FlaxRobertaForMaskedLM', 'FlaxRobertaForMultipleChoice', 'FlaxRobertaForQuestionAnswering', 'FlaxRobertaForSequenceClassification', 'FlaxRobertaForTokenClassification', 'FlaxRobertaModel', 'FlaxRobertaPreLayerNormForCausalLM', 'FlaxRobertaPreLayerNormForMaskedLM', 'FlaxRobertaPreLayerNormForMultipleChoice', 'FlaxRobertaPreLayerNormForQuestionAnswering', 'FlaxRobertaPreLayerNormForSequenceClassification', 'FlaxRobertaPreLayerNormForTokenClassification', 'FlaxRobertaPreLayerNormModel', 'FlaxRobertaPreLayerNormPreTrainedModel', 'FlaxRobertaPreTrainedModel', 'FlaxSpeechEncoderDecoderModel', 'FlaxT5EncoderModel', 'FlaxT5ForConditionalGeneration', 'FlaxT5Model', 'FlaxT5PreTrainedModel', 'FlaxTemperatureLogitsWarper', 'FlaxTopKLogitsWarper', 'FlaxTopPLogitsWarper', 'FlaxViTForImageClassification', 'FlaxViTModel', 'FlaxViTPreTrainedModel', 'FlaxVisionEncoderDecoderModel', 'FlaxVisionTextDualEncoderModel', 'FlaxWav2Vec2ForCTC', 'FlaxWav2Vec2ForPreTraining', 'FlaxWav2Vec2Model', 'FlaxWav2Vec2PreTrainedModel', 'FlaxWhisperForAudioClassification', 'FlaxWhisperForConditionalGeneration', 'FlaxWhisperModel', 'FlaxWhisperPreTrainedModel', 'FlaxXGLMForCausalLM', 'FlaxXGLMModel', 'FlaxXGLMPreTrainedModel', 'FlaxXLMRobertaForCausalLM', 'FlaxXLMRobertaForMaskedLM', 'FlaxXLMRobertaForMultipleChoice', 'FlaxXLMRobertaForQuestionAnswering', 'FlaxXLMRobertaForSequenceClassification', 'FlaxXLMRobertaForTokenClassification', 'FlaxXLMRobertaModel', 'FlaxXLMRobertaPreTrainedModel', 'FocalNetBackbone', 'FocalNetConfig', 'FocalNetForImageClassification', 'FocalNetForMaskedImageModeling', 'FocalNetModel', 'FocalNetPreTrainedModel', 'ForcedBOSTokenLogitsProcessor', 'ForcedEOSTokenLogitsProcessor', 'FunnelBaseModel', 'FunnelConfig', 'FunnelForMaskedLM', 'FunnelForMultipleChoice', 'FunnelForPreTraining', 'FunnelForQuestionAnswering', 'FunnelForSequenceClassification', 'FunnelForTokenClassification', 'FunnelModel', 'FunnelPreTrainedModel', 'FunnelTokenizer', 'FunnelTokenizerFast', 'GIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'GLPNConfig', 'GLPNFeatureExtractor', 'GLPNForDepthEstimation', 'GLPNImageProcessor', 'GLPNModel', 'GLPNPreTrainedModel', 'GLPN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GLPN_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT2Config', 'GPT2DoubleHeadsModel', 'GPT2ForQuestionAnswering', 'GPT2ForSequenceClassification', 'GPT2ForTokenClassification', 'GPT2LMHeadModel', 'GPT2Model', 'GPT2PreTrainedModel', 'GPT2Tokenizer', 'GPT2TokenizerFast', 'GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT2_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPTBigCodeConfig', 'GPTBigCodeForCausalLM', 'GPTBigCodeForSequenceClassification', 'GPTBigCodeForTokenClassification', 'GPTBigCodeModel', 'GPTBigCodePreTrainedModel', 'GPTJConfig', 'GPTJForCausalLM', 'GPTJForQuestionAnswering', 'GPTJForSequenceClassification', 'GPTJModel', 'GPTJPreTrainedModel', 'GPTJ_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPTJ_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPTNeoConfig', 'GPTNeoForCausalLM', 'GPTNeoForQuestionAnswering', 'GPTNeoForSequenceClassification', 'GPTNeoForTokenClassification', 'GPTNeoModel', 'GPTNeoPreTrainedModel', 'GPTNeoXConfig', 'GPTNeoXForCausalLM', 'GPTNeoXForQuestionAnswering', 'GPTNeoXForSequenceClassification', 'GPTNeoXForTokenClassification', 'GPTNeoXJapaneseConfig', 'GPTNeoXJapaneseForCausalLM', 'GPTNeoXJapaneseLayer', 'GPTNeoXJapaneseModel', 'GPTNeoXJapanesePreTrainedModel', 'GPTNeoXJapaneseTokenizer', 'GPTNeoXLayer', 'GPTNeoXModel', 'GPTNeoXPreTrainedModel', 'GPTNeoXTokenizerFast', 'GPTSAN_JAPANESE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPTSAN_JAPANESE_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPTSanJapaneseConfig', 'GPTSanJapaneseForConditionalGeneration', 'GPTSanJapaneseModel', 'GPTSanJapanesePreTrainedModel', 'GPTSanJapaneseTokenizer', 'GPTSw3Tokenizer', 'GPT_BIGCODE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_BIGCODE_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT_NEOX_JAPANESE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEOX_JAPANESE_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT_NEOX_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEOX_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT_NEO_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEO_PRETRAINED_MODEL_ARCHIVE_LIST', 'GRAPHORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GRAPHORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'GROUPVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'GenerationConfig', 'GenerationMixin', 'GitConfig', 'GitForCausalLM', 'GitModel', 'GitPreTrainedModel', 'GitProcessor', 'GitVisionConfig', 'GitVisionModel', 'GlueDataTrainingArguments', 'GlueDataset', 'GradientAccumulator', 'GraphormerConfig', 'GraphormerForGraphClassification', 'GraphormerModel', 'GraphormerPreTrainedModel', 'GroupViTConfig', 'GroupViTModel', 'GroupViTPreTrainedModel', 'GroupViTTextConfig', 'GroupViTTextModel', 'GroupViTVisionConfig', 'GroupViTVisionModel', 'HUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'HUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'HammingDiversityLogitsProcessor', 'HerbertTokenizer', 'HerbertTokenizerFast', 'HfAgent', 'HfArgumentParser', 'HubertConfig', 'HubertForCTC', 'HubertForSequenceClassification', 'HubertModel', 'HubertPreTrainedModel', 'IBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'IBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'IBertConfig', 'IBertForMaskedLM', 'IBertForMultipleChoice', 'IBertForQuestionAnswering', 'IBertForSequenceClassification', 'IBertForTokenClassification', 'IBertModel', 'IBertPreTrainedModel', 'IMAGEGPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'IMAGEGPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'IMAGE_PROCESSOR_MAPPING', 'INFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'INFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'INSTRUCTBLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'INSTRUCTBLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'ImageClassificationPipeline', 'ImageFeatureExtractionMixin', 'ImageGPTConfig', 'ImageGPTFeatureExtractor', 'ImageGPTForCausalImageModeling', 'ImageGPTForImageClassification', 'ImageGPTImageProcessor', 'ImageGPTModel', 'ImageGPTPreTrainedModel', 'ImageProcessingMixin', 'ImageSegmentationPipeline', 'ImageToTextPipeline', 'InfNanRemoveLogitsProcessor', 'InformerConfig', 'InformerForPrediction', 'InformerModel', 'InformerPreTrainedModel', 'InputExample', 'InputFeatures', 'InstructBlipConfig', 'InstructBlipForConditionalGeneration', 'InstructBlipPreTrainedModel', 'InstructBlipProcessor', 'InstructBlipQFormerConfig', 'InstructBlipQFormerModel', 'InstructBlipVisionConfig', 'InstructBlipVisionModel', 'IntervalStrategy', 'JUKEBOX_PRETRAINED_CONFIG_ARCHIVE_MAP', 'JUKEBOX_PRETRAINED_MODEL_ARCHIVE_LIST', 'JsonPipelineDataFormat', 'JukeboxConfig', 'JukeboxModel', 'JukeboxPreTrainedModel', 'JukeboxPrior', 'JukeboxPriorConfig', 'JukeboxTokenizer', 'JukeboxVQVAE', 'JukeboxVQVAEConfig', 'KerasMetricCallback', 'LAYOUTLMV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLMV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'LAYOUTLMV3_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLMV3_PRETRAINED_MODEL_ARCHIVE_LIST', 'LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'LEDConfig', 'LEDForConditionalGeneration', 'LEDForQuestionAnswering', 'LEDForSequenceClassification', 'LEDModel', 'LEDPreTrainedModel', 'LEDTokenizer', 'LEDTokenizerFast', 'LED_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LED_PRETRAINED_MODEL_ARCHIVE_LIST', 'LEVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'LILT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LILT_PRETRAINED_MODEL_ARCHIVE_LIST', 'LLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LONGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LONGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'LONGT5_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LONGT5_PRETRAINED_MODEL_ARCHIVE_LIST', 'LUKE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LUKE_PRETRAINED_MODEL_ARCHIVE_LIST', 'LXMERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LayoutLMConfig', 'LayoutLMForMaskedLM', 'LayoutLMForQuestionAnswering', 'LayoutLMForSequenceClassification', 'LayoutLMForTokenClassification', 'LayoutLMModel', 'LayoutLMPreTrainedModel', 'LayoutLMTokenizer', 'LayoutLMTokenizerFast', 'LayoutLMv2Config', 'LayoutLMv2FeatureExtractor', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv2ImageProcessor', 'LayoutLMv2Model', 'LayoutLMv2PreTrainedModel', 'LayoutLMv2Processor', 'LayoutLMv2Tokenizer', 'LayoutLMv2TokenizerFast', 'LayoutLMv3Config', 'LayoutLMv3FeatureExtractor', 'LayoutLMv3ForQuestionAnswering', 'LayoutLMv3ForSequenceClassification', 'LayoutLMv3ForTokenClassification', 'LayoutLMv3ImageProcessor', 'LayoutLMv3Model', 'LayoutLMv3PreTrainedModel', 'LayoutLMv3Processor', 'LayoutLMv3Tokenizer', 'LayoutLMv3TokenizerFast', 'LayoutXLMProcessor', 'LayoutXLMTokenizer', 'LayoutXLMTokenizerFast', 'LevitConfig', 'LevitFeatureExtractor', 'LevitForImageClassification', 'LevitForImageClassificationWithTeacher', 'LevitImageProcessor', 'LevitModel', 'LevitPreTrainedModel', 'LiltConfig', 'LiltForQuestionAnswering', 'LiltForSequenceClassification', 'LiltForTokenClassification', 'LiltModel', 'LiltPreTrainedModel', 'LineByLineTextDataset', 'LineByLineWithRefDataset', 'LineByLineWithSOPTextDataset', 'LlamaConfig', 'LlamaForCausalLM', 'LlamaForSequenceClassification', 'LlamaModel', 'LlamaPreTrainedModel', 'LlamaTokenizer', 'LlamaTokenizerFast', 'LocalAgent', 'LogitsProcessor', 'LogitsProcessorList', 'LogitsWarper', 'LongT5Config', 'LongT5EncoderModel', 'LongT5ForConditionalGeneration', 'LongT5Model', 'LongT5PreTrainedModel', 'LongformerConfig', 'LongformerForMaskedLM', 'LongformerForMultipleChoice', 'LongformerForQuestionAnswering', 'LongformerForSequenceClassification', 'LongformerForTokenClassification', 'LongformerModel', 'LongformerPreTrainedModel', 'LongformerSelfAttention', 'LongformerTokenizer', 'LongformerTokenizerFast', 'LukeConfig', 'LukeForEntityClassification', 'LukeForEntityPairClassification', 'LukeForEntitySpanClassification', 'LukeForMaskedLM', 'LukeForMultipleChoice', 'LukeForQuestionAnswering', 'LukeForSequenceClassification', 'LukeForTokenClassification', 'LukeModel', 'LukePreTrainedModel', 'LukeTokenizer', 'LxmertConfig', 'LxmertEncoder', 'LxmertForPreTraining', 'LxmertForQuestionAnswering', 'LxmertModel', 'LxmertPreTrainedModel', 'LxmertTokenizer', 'LxmertTokenizerFast', 'LxmertVisualFeatureEncoder', 'LxmertXLayer', 'M2M100Config', 'M2M100ForConditionalGeneration', 'M2M100Model', 'M2M100PreTrainedModel', 'M2M100Tokenizer', 'M2M_100_PRETRAINED_CONFIG_ARCHIVE_MAP', 'M2M_100_PRETRAINED_MODEL_ARCHIVE_LIST', 'MARKUPLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MARKUPLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'MASK2FORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MASK2FORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'MASKFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MASKFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'MBart50Tokenizer', 'MBart50TokenizerFast', 'MBartConfig', 'MBartForCausalLM', 'MBartForConditionalGeneration', 'MBartForQuestionAnswering', 'MBartForSequenceClassification', 'MBartModel', 'MBartPreTrainedModel', 'MBartTokenizer', 'MBartTokenizerFast', 'MCTCTConfig', 'MCTCTFeatureExtractor', 'MCTCTForCTC', 'MCTCTModel', 'MCTCTPreTrainedModel', 'MCTCTProcessor', 'MCTCT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MCTCT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MEGATRON_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MEGATRON_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MEGA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MEGA_PRETRAINED_MODEL_ARCHIVE_LIST', 'MGP_STR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MGP_STR_PRETRAINED_MODEL_ARCHIVE_LIST', 'MLukeTokenizer', 'MMBTConfig', 'MMBTForClassification', 'MMBTModel', 'MOBILEBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILENET_V1_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILENET_V1_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILENET_V2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILENET_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILEVITV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILEVITV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILEVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MODEL_CARD_NAME', 'MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING', 'MODEL_FOR_AUDIO_XVECTOR_MAPPING', 'MODEL_FOR_BACKBONE_MAPPING', 'MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING', 'MODEL_FOR_CAUSAL_LM_MAPPING', 'MODEL_FOR_CTC_MAPPING', 'MODEL_FOR_DEPTH_ESTIMATION_MAPPING', 'MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'MODEL_FOR_IMAGE_SEGMENTATION_MAPPING', 'MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING', 'MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING', 'MODEL_FOR_MASKED_LM_MAPPING', 'MODEL_FOR_MASK_GENERATION_MAPPING', 'MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'MODEL_FOR_OBJECT_DETECTION_MAPPING', 'MODEL_FOR_PRETRAINING_MAPPING', 'MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING', 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_TEXT_ENCODING_MAPPING', 'MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING', 'MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING', 'MODEL_FOR_VISION_2_SEQ_MAPPING', 'MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING', 'MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING', 'MODEL_MAPPING', 'MODEL_NAMES_MAPPING', 'MODEL_WITH_LM_HEAD_MAPPING', 'MPNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MPNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'MPNetConfig', 'MPNetForMaskedLM', 'MPNetForMultipleChoice', 'MPNetForQuestionAnswering', 'MPNetForSequenceClassification', 'MPNetForTokenClassification', 'MPNetLayer', 'MPNetModel', 'MPNetPreTrainedModel', 'MPNetTokenizer', 'MPNetTokenizerFast', 'MRA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MRA_PRETRAINED_MODEL_ARCHIVE_LIST', 'MT5Config', 'MT5EncoderModel', 'MT5ForConditionalGeneration', 'MT5ForQuestionAnswering', 'MT5Model', 'MT5PreTrainedModel', 'MT5Tokenizer', 'MT5TokenizerFast', 'MUSICGEN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MUSICGEN_PRETRAINED_MODEL_ARCHIVE_LIST', 'MVP_PRETRAINED_MODEL_ARCHIVE_LIST', 'MarianConfig', 'MarianForCausalLM', 'MarianMTModel', 'MarianModel', 'MarianTokenizer', 'MarkupLMConfig', 'MarkupLMFeatureExtractor', 'MarkupLMForQuestionAnswering', 'MarkupLMForSequenceClassification', 'MarkupLMForTokenClassification', 'MarkupLMModel', 'MarkupLMPreTrainedModel', 'MarkupLMProcessor', 'MarkupLMTokenizer', 'MarkupLMTokenizerFast', 'Mask2FormerConfig', 'Mask2FormerForUniversalSegmentation', 'Mask2FormerImageProcessor', 'Mask2FormerModel', 'Mask2FormerPreTrainedModel', 'MaskFormerConfig', 'MaskFormerFeatureExtractor', 'MaskFormerForInstanceSegmentation', 'MaskFormerImageProcessor', 'MaskFormerModel', 'MaskFormerPreTrainedModel', 'MaskFormerSwinBackbone', 'MaskFormerSwinConfig', 'MaxLengthCriteria', 'MaxTimeCriteria', 'MecabTokenizer', 'MegaConfig', 'MegaForCausalLM', 'MegaForMaskedLM', 'MegaForMultipleChoice', 'MegaForQuestionAnswering', 'MegaForSequenceClassification', 'MegaForTokenClassification', 'MegaModel', 'MegaPreTrainedModel', 'MegatronBertConfig', 'MegatronBertForCausalLM', 'MegatronBertForMaskedLM', 'MegatronBertForMultipleChoice', 'MegatronBertForNextSentencePrediction', 'MegatronBertForPreTraining', 'MegatronBertForQuestionAnswering', 'MegatronBertForSequenceClassification', 'MegatronBertForTokenClassification', 'MegatronBertModel', 'MegatronBertPreTrainedModel', 'MgpstrConfig', 'MgpstrForSceneTextRecognition', 'MgpstrModel', 'MgpstrPreTrainedModel', 'MgpstrProcessor', 'MgpstrTokenizer', 'MinLengthLogitsProcessor', 'MinNewTokensLengthLogitsProcessor', 'MobileBertConfig', 'MobileBertForMaskedLM', 'MobileBertForMultipleChoice', 'MobileBertForNextSentencePrediction', 'MobileBertForPreTraining', 'MobileBertForQuestionAnswering', 'MobileBertForSequenceClassification', 'MobileBertForTokenClassification', 'MobileBertLayer', 'MobileBertModel', 'MobileBertPreTrainedModel', 'MobileBertTokenizer', 'MobileBertTokenizerFast', 'MobileNetV1Config', 'MobileNetV1FeatureExtractor', 'MobileNetV1ForImageClassification', 'MobileNetV1ImageProcessor', 'MobileNetV1Model', 'MobileNetV1PreTrainedModel', 'MobileNetV2Config', 'MobileNetV2FeatureExtractor', 'MobileNetV2ForImageClassification', 'MobileNetV2ForSemanticSegmentation', 'MobileNetV2ImageProcessor', 'MobileNetV2Model', 'MobileNetV2PreTrainedModel', 'MobileViTConfig', 'MobileViTFeatureExtractor', 'MobileViTForImageClassification', 'MobileViTForSemanticSegmentation', 'MobileViTImageProcessor', 'MobileViTModel', 'MobileViTPreTrainedModel', 'MobileViTV2Config', 'MobileViTV2ForImageClassification', 'MobileViTV2ForSemanticSegmentation', 'MobileViTV2Model', 'MobileViTV2PreTrainedModel', 'ModalEmbeddings', 'ModelCard', 'MraConfig', 'MraForMaskedLM', 'MraForMultipleChoice', 'MraForQuestionAnswering', 'MraForSequenceClassification', 'MraForTokenClassification', 'MraModel', 'MraPreTrainedModel', 'MusicgenConfig', 'MusicgenDecoderConfig', 'MusicgenForCausalLM', 'MusicgenForConditionalGeneration', 'MusicgenModel', 'MusicgenPreTrainedModel', 'MusicgenProcessor', 'MvpConfig', 'MvpForCausalLM', 'MvpForConditionalGeneration', 'MvpForQuestionAnswering', 'MvpForSequenceClassification', 'MvpModel', 'MvpPreTrainedModel', 'MvpTokenizer', 'MvpTokenizerFast', 'NAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'NEZHA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NEZHA_PRETRAINED_MODEL_ARCHIVE_LIST', 'NLLB_MOE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NLLB_MOE_PRETRAINED_MODEL_ARCHIVE_LIST', 'NYSTROMFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NYSTROMFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'NatBackbone', 'NatConfig', 'NatForImageClassification', 'NatModel', 'NatPreTrainedModel', 'NerPipeline', 'NezhaConfig', 'NezhaForMaskedLM', 'NezhaForMultipleChoice', 'NezhaForNextSentencePrediction', 'NezhaForPreTraining', 'NezhaForQuestionAnswering', 'NezhaForSequenceClassification', 'NezhaForTokenClassification', 'NezhaModel', 'NezhaPreTrainedModel', 'NllbMoeConfig', 'NllbMoeForConditionalGeneration', 'NllbMoeModel', 'NllbMoePreTrainedModel', 'NllbMoeSparseMLP', 'NllbMoeTop2Router', 'NllbTokenizer', 'NllbTokenizerFast', 'NoBadWordsLogitsProcessor', 'NoRepeatNGramLogitsProcessor', 'NystromformerConfig', 'NystromformerForMaskedLM', 'NystromformerForMultipleChoice', 'NystromformerForQuestionAnswering', 'NystromformerForSequenceClassification', 'NystromformerForTokenClassification', 'NystromformerLayer', 'NystromformerModel', 'NystromformerPreTrainedModel', 'ONEFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ONEFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'OPEN_LLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'OPTConfig', 'OPTForCausalLM', 'OPTForQuestionAnswering', 'OPTForSequenceClassification', 'OPTModel', 'OPTPreTrainedModel', 'OPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'OWLVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'OWLVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ObjectDetectionPipeline', 'OneFormerConfig', 'OneFormerForUniversalSegmentation', 'OneFormerImageProcessor', 'OneFormerModel', 'OneFormerPreTrainedModel', 'OneFormerProcessor', 'OpenAIGPTConfig', 'OpenAIGPTDoubleHeadsModel', 'OpenAIGPTForSequenceClassification', 'OpenAIGPTLMHeadModel', 'OpenAIGPTModel', 'OpenAIGPTPreTrainedModel', 'OpenAIGPTTokenizer', 'OpenAIGPTTokenizerFast', 'OpenAiAgent', 'OpenLlamaConfig', 'OpenLlamaForCausalLM', 'OpenLlamaForSequenceClassification', 'OpenLlamaModel', 'OpenLlamaPreTrainedModel', 'OwlViTConfig', 'OwlViTFeatureExtractor', 'OwlViTForObjectDetection', 'OwlViTImageProcessor', 'OwlViTModel', 'OwlViTPreTrainedModel', 'OwlViTProcessor', 'OwlViTTextConfig', 'OwlViTTextModel', 'OwlViTVisionConfig', 'OwlViTVisionModel', 'PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PEGASUS_X_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PEGASUS_X_PRETRAINED_MODEL_ARCHIVE_LIST', 'PERCEIVER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PERCEIVER_PRETRAINED_MODEL_ARCHIVE_LIST', 'PIX2STRUCT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PIX2STRUCT_PRETRAINED_MODEL_ARCHIVE_LIST', 'PLBART_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PLBART_PRETRAINED_MODEL_ARCHIVE_LIST', 'PLBartConfig', 'PLBartForCausalLM', 'PLBartForConditionalGeneration', 'PLBartForSequenceClassification', 'PLBartModel', 'PLBartPreTrainedModel', 'PLBartTokenizer', 'POOLFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'POOLFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'PROCESSOR_MAPPING', 'PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PROPHETNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'PYTORCH_PRETRAINED_BERT_CACHE', 'PYTORCH_TRANSFORMERS_CACHE', 'PegasusConfig', 'PegasusForCausalLM', 'PegasusForConditionalGeneration', 'PegasusModel', 'PegasusPreTrainedModel', 'PegasusTokenizer', 'PegasusTokenizerFast', 'PegasusXConfig', 'PegasusXForConditionalGeneration', 'PegasusXModel', 'PegasusXPreTrainedModel', 'PerceiverConfig', 'PerceiverFeatureExtractor', 'PerceiverForImageClassificationConvProcessing', 'PerceiverForImageClassificationFourier', 'PerceiverForImageClassificationLearned', 'PerceiverForMaskedLM', 'PerceiverForMultimodalAutoencoding', 'PerceiverForOpticalFlow', 'PerceiverForSequenceClassification', 'PerceiverImageProcessor', 'PerceiverLayer', 'PerceiverModel', 'PerceiverPreTrainedModel', 'PerceiverTokenizer', 'PhobertTokenizer', 'PhrasalConstraint', 'PipedPipelineDataFormat', 'Pipeline', 'PipelineDataFormat', 'PipelineTool', 'Pix2StructConfig', 'Pix2StructForConditionalGeneration', 'Pix2StructImageProcessor', 'Pix2StructPreTrainedModel', 'Pix2StructProcessor', 'Pix2StructTextConfig', 'Pix2StructTextModel', 'Pix2StructVisionConfig', 'Pix2StructVisionModel', 'PoolFormerConfig', 'PoolFormerFeatureExtractor', 'PoolFormerForImageClassification', 'PoolFormerImageProcessor', 'PoolFormerModel', 'PoolFormerPreTrainedModel', 'PreTrainedModel', 'PreTrainedTokenizer', 'PreTrainedTokenizerBase', 'PreTrainedTokenizerFast', 'PrefixConstrainedLogitsProcessor', 'PretrainedBartModel', 'PretrainedConfig', 'PretrainedFSMTModel', 'PrinterCallback', 'ProcessorMixin', 'ProgressCallback', 'ProphetNetConfig', 'ProphetNetDecoder', 'ProphetNetEncoder', 'ProphetNetForCausalLM', 'ProphetNetForConditionalGeneration', 'ProphetNetModel', 'ProphetNetPreTrainedModel', 'ProphetNetTokenizer', 'PushToHubCallback', 'PyTorchBenchmark', 'PyTorchBenchmarkArguments', 'QDQBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'QDQBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'QDQBertConfig', 'QDQBertForMaskedLM', 'QDQBertForMultipleChoice', 'QDQBertForNextSentencePrediction', 'QDQBertForQuestionAnswering', 'QDQBertForSequenceClassification', 'QDQBertForTokenClassification', 'QDQBertLMHeadModel', 'QDQBertLayer', 'QDQBertModel', 'QDQBertPreTrainedModel', 'QuestionAnsweringPipeline', 'REALM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REALM_PRETRAINED_MODEL_ARCHIVE_LIST', 'REFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'REGNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REGNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'REMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'RESNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'RESNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'RETRIBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'RETRIBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROBERTA_PRELAYERNORM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROBERTA_PRELAYERNORM_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROC_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROC_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'RWKV_PRETRAINED_CONFIG_ARCHIVE_MAP', 'RWKV_PRETRAINED_MODEL_ARCHIVE_LIST', 'RagConfig', 'RagModel', 'RagPreTrainedModel', 'RagRetriever', 'RagSequenceForGeneration', 'RagTokenForGeneration', 'RagTokenizer', 'RealmConfig', 'RealmEmbedder', 'RealmForOpenQA', 'RealmKnowledgeAugEncoder', 'RealmPreTrainedModel', 'RealmReader', 'RealmRetriever', 'RealmScorer', 'RealmTokenizer', 'RealmTokenizerFast', 'ReformerAttention', 'ReformerConfig', 'ReformerForMaskedLM', 'ReformerForQuestionAnswering', 'ReformerForSequenceClassification', 'ReformerLayer', 'ReformerModel', 'ReformerModelWithLMHead', 'ReformerPreTrainedModel', 'ReformerTokenizer', 'ReformerTokenizerFast', 'RegNetConfig', 'RegNetForImageClassification', 'RegNetModel', 'RegNetPreTrainedModel', 'RemBertConfig', 'RemBertForCausalLM', 'RemBertForMaskedLM', 'RemBertForMultipleChoice', 'RemBertForQuestionAnswering', 'RemBertForSequenceClassification', 'RemBertForTokenClassification', 'RemBertLayer', 'RemBertModel', 'RemBertPreTrainedModel', 'RemBertTokenizer', 'RemBertTokenizerFast', 'RemoteTool', 'RepetitionPenaltyLogitsProcessor', 'ResNetBackbone', 'ResNetConfig', 'ResNetForImageClassification', 'ResNetModel', 'ResNetPreTrainedModel', 'RetriBertConfig', 'RetriBertModel', 'RetriBertPreTrainedModel', 'RetriBertTokenizer', 'RetriBertTokenizerFast', 'RoCBertConfig', 'RoCBertForCausalLM', 'RoCBertForMaskedLM', 'RoCBertForMultipleChoice', 'RoCBertForPreTraining', 'RoCBertForQuestionAnswering', 'RoCBertForSequenceClassification', 'RoCBertForTokenClassification', 'RoCBertLayer', 'RoCBertModel', 'RoCBertPreTrainedModel', 'RoCBertTokenizer', 'RoFormerConfig', 'RoFormerForCausalLM', 'RoFormerForMaskedLM', 'RoFormerForMultipleChoice', 'RoFormerForQuestionAnswering', 'RoFormerForSequenceClassification', 'RoFormerForTokenClassification', 'RoFormerLayer', 'RoFormerModel', 'RoFormerPreTrainedModel', 'RoFormerTokenizer', 'RoFormerTokenizerFast', 'RobertaConfig', 'RobertaForCausalLM', 'RobertaForMaskedLM', 'RobertaForMultipleChoice', 'RobertaForQuestionAnswering', 'RobertaForSequenceClassification', 'RobertaForTokenClassification', 'RobertaModel', 'RobertaPreLayerNormConfig', 'RobertaPreLayerNormForCausalLM', 'RobertaPreLayerNormForMaskedLM', 'RobertaPreLayerNormForMultipleChoice', 'RobertaPreLayerNormForQuestionAnswering', 'RobertaPreLayerNormForSequenceClassification', 'RobertaPreLayerNormForTokenClassification', 'RobertaPreLayerNormModel', 'RobertaPreLayerNormPreTrainedModel', 'RobertaPreTrainedModel', 'RobertaTokenizer', 'RobertaTokenizerFast', 'RwkvConfig', 'RwkvForCausalLM', 'RwkvModel', 'RwkvPreTrainedModel', 'SAM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SAM_PRETRAINED_MODEL_ARCHIVE_LIST', 'SEGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'SEWConfig', 'SEWDConfig', 'SEWDForCTC', 'SEWDForSequenceClassification', 'SEWDModel', 'SEWDPreTrainedModel', 'SEWForCTC', 'SEWForSequenceClassification', 'SEWModel', 'SEWPreTrainedModel', 'SEW_D_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEW_D_PRETRAINED_MODEL_ARCHIVE_LIST', 'SEW_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEW_PRETRAINED_MODEL_ARCHIVE_LIST', 'SLOW_TO_FAST_CONVERTERS', 'SPEECHT5_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPEECHT5_PRETRAINED_HIFIGAN_CONFIG_ARCHIVE_MAP', 'SPEECHT5_PRETRAINED_MODEL_ARCHIVE_LIST', 'SPEECH_TO_TEXT_2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPEECH_TO_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPEECH_TO_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'SPIECE_UNDERLINE', 'SPLINTER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPLINTER_PRETRAINED_MODEL_ARCHIVE_LIST', 'SQUEEZEBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SQUEEZEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWIFTFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWIFTFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWIN2SR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWIN2SR_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWINV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWINV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWIN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWITCH_TRANSFORMERS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWITCH_TRANSFORMERS_PRETRAINED_MODEL_ARCHIVE_LIST', 'SamConfig', 'SamImageProcessor', 'SamMaskDecoderConfig', 'SamModel', 'SamPreTrainedModel', 'SamProcessor', 'SamPromptEncoderConfig', 'SamVisionConfig', 'SchedulerType', 'SegformerConfig', 'SegformerDecodeHead', 'SegformerFeatureExtractor', 'SegformerForImageClassification', 'SegformerForSemanticSegmentation', 'SegformerImageProcessor', 'SegformerLayer', 'SegformerModel', 'SegformerPreTrainedModel', 'Seq2SeqTrainer', 'Seq2SeqTrainingArguments', 'SequenceBiasLogitsProcessor', 'SequenceFeatureExtractor', 'SingleSentenceClassificationProcessor', 'SpecialTokensMixin', 'Speech2Text2Config', 'Speech2Text2ForCausalLM', 'Speech2Text2PreTrainedModel', 'Speech2Text2Processor', 'Speech2Text2Tokenizer', 'Speech2TextConfig', 'Speech2TextFeatureExtractor', 'Speech2TextForConditionalGeneration', 'Speech2TextModel', 'Speech2TextPreTrainedModel', 'Speech2TextProcessor', 'Speech2TextTokenizer', 'SpeechEncoderDecoderConfig', 'SpeechEncoderDecoderModel', 'SpeechT5Config', 'SpeechT5FeatureExtractor', 'SpeechT5ForSpeechToSpeech', 'SpeechT5ForSpeechToText', 'SpeechT5ForTextToSpeech', 'SpeechT5HifiGan', 'SpeechT5HifiGanConfig', 'SpeechT5Model', 'SpeechT5PreTrainedModel', 'SpeechT5Processor', 'SpeechT5Tokenizer', 'SplinterConfig', 'SplinterForPreTraining', 'SplinterForQuestionAnswering', 'SplinterLayer', 'SplinterModel', 'SplinterPreTrainedModel', 'SplinterTokenizer', 'SplinterTokenizerFast', 'SquadDataTrainingArguments', 'SquadDataset', 'SquadExample', 'SquadFeatures', 'SquadV1Processor', 'SquadV2Processor', 'SqueezeBertConfig', 'SqueezeBertForMaskedLM', 'SqueezeBertForMultipleChoice', 'SqueezeBertForQuestionAnswering', 'SqueezeBertForSequenceClassification', 'SqueezeBertForTokenClassification', 'SqueezeBertModel', 'SqueezeBertModule', 'SqueezeBertPreTrainedModel', 'SqueezeBertTokenizer', 'SqueezeBertTokenizerFast', 'StoppingCriteria', 'StoppingCriteriaList', 'SummarizationPipeline', 'SwiftFormerConfig', 'SwiftFormerForImageClassification', 'SwiftFormerModel', 'SwiftFormerPreTrainedModel', 'Swin2SRConfig', 'Swin2SRForImageSuperResolution', 'Swin2SRImageProcessor', 'Swin2SRModel', 'Swin2SRPreTrainedModel', 'SwinBackbone', 'SwinConfig', 'SwinForImageClassification', 'SwinForMaskedImageModeling', 'SwinModel', 'SwinPreTrainedModel', 'Swinv2Config', 'Swinv2ForImageClassification', 'Swinv2ForMaskedImageModeling', 'Swinv2Model', 'Swinv2PreTrainedModel', 'SwitchTransformersConfig', 'SwitchTransformersEncoderModel', 'SwitchTransformersForConditionalGeneration', 'SwitchTransformersModel', 'SwitchTransformersPreTrainedModel', 'SwitchTransformersSparseMLP', 'SwitchTransformersTop1Router', 'T5Config', 'T5EncoderModel', 'T5ForConditionalGeneration', 'T5ForQuestionAnswering', 'T5Model', 'T5PreTrainedModel', 'T5Tokenizer', 'T5TokenizerFast', 'T5_PRETRAINED_CONFIG_ARCHIVE_MAP', 'T5_PRETRAINED_MODEL_ARCHIVE_LIST', 'TABLE_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TABLE_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TAPAS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TAPAS_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF2_WEIGHTS_NAME', 'TFAdaptiveEmbedding', 'TFAlbertForMaskedLM', 'TFAlbertForMultipleChoice', 'TFAlbertForPreTraining', 'TFAlbertForQuestionAnswering', 'TFAlbertForSequenceClassification', 'TFAlbertForTokenClassification', 'TFAlbertMainLayer', 'TFAlbertModel', 'TFAlbertPreTrainedModel', 'TFAutoModel', 'TFAutoModelForAudioClassification', 'TFAutoModelForCausalLM', 'TFAutoModelForDocumentQuestionAnswering', 'TFAutoModelForImageClassification', 'TFAutoModelForMaskGeneration', 'TFAutoModelForMaskedImageModeling', 'TFAutoModelForMaskedLM', 'TFAutoModelForMultipleChoice', 'TFAutoModelForNextSentencePrediction', 'TFAutoModelForPreTraining', 'TFAutoModelForQuestionAnswering', 'TFAutoModelForSemanticSegmentation', 'TFAutoModelForSeq2SeqLM', 'TFAutoModelForSequenceClassification', 'TFAutoModelForSpeechSeq2Seq', 'TFAutoModelForTableQuestionAnswering', 'TFAutoModelForTextEncoding', 'TFAutoModelForTokenClassification', 'TFAutoModelForVision2Seq', 'TFAutoModelForZeroShotImageClassification', 'TFAutoModelWithLMHead', 'TFBartForConditionalGeneration', 'TFBartForSequenceClassification', 'TFBartModel', 'TFBartPretrainedModel', 'TFBertEmbeddings', 'TFBertForMaskedLM', 'TFBertForMultipleChoice', 'TFBertForNextSentencePrediction', 'TFBertForPreTraining', 'TFBertForQuestionAnswering', 'TFBertForSequenceClassification', 'TFBertForTokenClassification', 'TFBertLMHeadModel', 'TFBertMainLayer', 'TFBertModel', 'TFBertPreTrainedModel', 'TFBertTokenizer', 'TFBlenderbotForConditionalGeneration', 'TFBlenderbotModel', 'TFBlenderbotPreTrainedModel', 'TFBlenderbotSmallForConditionalGeneration', 'TFBlenderbotSmallModel', 'TFBlenderbotSmallPreTrainedModel', 'TFBlipForConditionalGeneration', 'TFBlipForImageTextRetrieval', 'TFBlipForQuestionAnswering', 'TFBlipModel', 'TFBlipPreTrainedModel', 'TFBlipTextModel', 'TFBlipVisionModel', 'TFCLIPModel', 'TFCLIPPreTrainedModel', 'TFCLIPTextModel', 'TFCLIPVisionModel', 'TFCTRLForSequenceClassification', 'TFCTRLLMHeadModel', 'TFCTRLModel', 'TFCTRLPreTrainedModel', 'TFCamembertForCausalLM', 'TFCamembertForMaskedLM', 'TFCamembertForMultipleChoice', 'TFCamembertForQuestionAnswering', 'TFCamembertForSequenceClassification', 'TFCamembertForTokenClassification', 'TFCamembertModel', 'TFCamembertPreTrainedModel', 'TFConvBertForMaskedLM', 'TFConvBertForMultipleChoice', 'TFConvBertForQuestionAnswering', 'TFConvBertForSequenceClassification', 'TFConvBertForTokenClassification', 'TFConvBertLayer', 'TFConvBertModel', 'TFConvBertPreTrainedModel', 'TFConvNextForImageClassification', 'TFConvNextModel', 'TFConvNextPreTrainedModel', 'TFCvtForImageClassification', 'TFCvtModel', 'TFCvtPreTrainedModel', 'TFDPRContextEncoder', 'TFDPRPretrainedContextEncoder', 'TFDPRPretrainedQuestionEncoder', 'TFDPRPretrainedReader', 'TFDPRQuestionEncoder', 'TFDPRReader', 'TFData2VecVisionForImageClassification', 'TFData2VecVisionForSemanticSegmentation', 'TFData2VecVisionModel', 'TFData2VecVisionPreTrainedModel', 'TFDebertaForMaskedLM', 'TFDebertaForQuestionAnswering', 'TFDebertaForSequenceClassification', 'TFDebertaForTokenClassification', 'TFDebertaModel', 'TFDebertaPreTrainedModel', 'TFDebertaV2ForMaskedLM', 'TFDebertaV2ForQuestionAnswering', 'TFDebertaV2ForSequenceClassification', 'TFDebertaV2ForTokenClassification', 'TFDebertaV2Model', 'TFDebertaV2PreTrainedModel', 'TFDeiTForImageClassification', 'TFDeiTForImageClassificationWithTeacher', 'TFDeiTForMaskedImageModeling', 'TFDeiTModel', 'TFDeiTPreTrainedModel', 'TFDistilBertForMaskedLM', 'TFDistilBertForMultipleChoice', 'TFDistilBertForQuestionAnswering', 'TFDistilBertForSequenceClassification', 'TFDistilBertForTokenClassification', 'TFDistilBertMainLayer', 'TFDistilBertModel', 'TFDistilBertPreTrainedModel', 'TFEfficientFormerForImageClassification', 'TFEfficientFormerForImageClassificationWithTeacher', 'TFEfficientFormerModel', 'TFEfficientFormerPreTrainedModel', 'TFElectraForMaskedLM', 'TFElectraForMultipleChoice', 'TFElectraForPreTraining', 'TFElectraForQuestionAnswering', 'TFElectraForSequenceClassification', 'TFElectraForTokenClassification', 'TFElectraModel', 'TFElectraPreTrainedModel', 'TFEncoderDecoderModel', 'TFEsmForMaskedLM', 'TFEsmForSequenceClassification', 'TFEsmForTokenClassification', 'TFEsmModel', 'TFEsmPreTrainedModel', 'TFFlaubertForMultipleChoice', 'TFFlaubertForQuestionAnsweringSimple', 'TFFlaubertForSequenceClassification', 'TFFlaubertForTokenClassification', 'TFFlaubertModel', 'TFFlaubertPreTrainedModel', 'TFFlaubertWithLMHeadModel', 'TFForcedBOSTokenLogitsProcessor', 'TFForcedEOSTokenLogitsProcessor', 'TFFunnelBaseModel', 'TFFunnelForMaskedLM', 'TFFunnelForMultipleChoice', 'TFFunnelForPreTraining', 'TFFunnelForQuestionAnswering', 'TFFunnelForSequenceClassification', 'TFFunnelForTokenClassification', 'TFFunnelModel', 'TFFunnelPreTrainedModel', 'TFGPT2DoubleHeadsModel', 'TFGPT2ForSequenceClassification', 'TFGPT2LMHeadModel', 'TFGPT2MainLayer', 'TFGPT2Model', 'TFGPT2PreTrainedModel', 'TFGPT2Tokenizer', 'TFGPTJForCausalLM', 'TFGPTJForQuestionAnswering', 'TFGPTJForSequenceClassification', 'TFGPTJModel', 'TFGPTJPreTrainedModel', 'TFGenerationMixin', 'TFGroupViTModel', 'TFGroupViTPreTrainedModel', 'TFGroupViTTextModel', 'TFGroupViTVisionModel', 'TFHubertForCTC', 'TFHubertModel', 'TFHubertPreTrainedModel', 'TFLEDForConditionalGeneration', 'TFLEDModel', 'TFLEDPreTrainedModel', 'TFLayoutLMForMaskedLM', 'TFLayoutLMForQuestionAnswering', 'TFLayoutLMForSequenceClassification', 'TFLayoutLMForTokenClassification', 'TFLayoutLMMainLayer', 'TFLayoutLMModel', 'TFLayoutLMPreTrainedModel', 'TFLayoutLMv3ForQuestionAnswering', 'TFLayoutLMv3ForSequenceClassification', 'TFLayoutLMv3ForTokenClassification', 'TFLayoutLMv3Model', 'TFLayoutLMv3PreTrainedModel', 'TFLogitsProcessor', 'TFLogitsProcessorList', 'TFLogitsWarper', 'TFLongformerForMaskedLM', 'TFLongformerForMultipleChoice', 'TFLongformerForQuestionAnswering', 'TFLongformerForSequenceClassification', 'TFLongformerForTokenClassification', 'TFLongformerModel', 'TFLongformerPreTrainedModel', 'TFLongformerSelfAttention', 'TFLxmertForPreTraining', 'TFLxmertMainLayer', 'TFLxmertModel', 'TFLxmertPreTrainedModel', 'TFLxmertVisualFeatureEncoder', 'TFMBartForConditionalGeneration', 'TFMBartModel', 'TFMBartPreTrainedModel', 'TFMPNetForMaskedLM', 'TFMPNetForMultipleChoice', 'TFMPNetForQuestionAnswering', 'TFMPNetForSequenceClassification', 'TFMPNetForTokenClassification', 'TFMPNetMainLayer', 'TFMPNetModel', 'TFMPNetPreTrainedModel', 'TFMT5EncoderModel', 'TFMT5ForConditionalGeneration', 'TFMT5Model', 'TFMarianMTModel', 'TFMarianModel', 'TFMarianPreTrainedModel', 'TFMinLengthLogitsProcessor', 'TFMobileBertForMaskedLM', 'TFMobileBertForMultipleChoice', 'TFMobileBertForNextSentencePrediction', 'TFMobileBertForPreTraining', 'TFMobileBertForQuestionAnswering', 'TFMobileBertForSequenceClassification', 'TFMobileBertForTokenClassification', 'TFMobileBertMainLayer', 'TFMobileBertModel', 'TFMobileBertPreTrainedModel', 'TFMobileViTForImageClassification', 'TFMobileViTForSemanticSegmentation', 'TFMobileViTModel', 'TFMobileViTPreTrainedModel', 'TFNoBadWordsLogitsProcessor', 'TFNoRepeatNGramLogitsProcessor', 'TFOPTForCausalLM', 'TFOPTModel', 'TFOPTPreTrainedModel', 'TFOpenAIGPTDoubleHeadsModel', 'TFOpenAIGPTForSequenceClassification', 'TFOpenAIGPTLMHeadModel', 'TFOpenAIGPTMainLayer', 'TFOpenAIGPTModel', 'TFOpenAIGPTPreTrainedModel', 'TFPegasusForConditionalGeneration', 'TFPegasusModel', 'TFPegasusPreTrainedModel', 'TFPreTrainedModel', 'TFRagModel', 'TFRagPreTrainedModel', 'TFRagSequenceForGeneration', 'TFRagTokenForGeneration', 'TFRegNetForImageClassification', 'TFRegNetModel', 'TFRegNetPreTrainedModel', 'TFRemBertForCausalLM', 'TFRemBertForMaskedLM', 'TFRemBertForMultipleChoice', 'TFRemBertForQuestionAnswering', 'TFRemBertForSequenceClassification', 'TFRemBertForTokenClassification', 'TFRemBertLayer', 'TFRemBertModel', 'TFRemBertPreTrainedModel', 'TFRepetitionPenaltyLogitsProcessor', 'TFResNetForImageClassification', 'TFResNetModel', 'TFResNetPreTrainedModel', 'TFRoFormerForCausalLM', 'TFRoFormerForMaskedLM', 'TFRoFormerForMultipleChoice', 'TFRoFormerForQuestionAnswering', 'TFRoFormerForSequenceClassification', 'TFRoFormerForTokenClassification', 'TFRoFormerLayer', 'TFRoFormerModel', 'TFRoFormerPreTrainedModel', 'TFRobertaForCausalLM', 'TFRobertaForMaskedLM', 'TFRobertaForMultipleChoice', 'TFRobertaForQuestionAnswering', 'TFRobertaForSequenceClassification', 'TFRobertaForTokenClassification', 'TFRobertaMainLayer', 'TFRobertaModel', 'TFRobertaPreLayerNormForCausalLM', 'TFRobertaPreLayerNormForMaskedLM', 'TFRobertaPreLayerNormForMultipleChoice', 'TFRobertaPreLayerNormForQuestionAnswering', 'TFRobertaPreLayerNormForSequenceClassification', 'TFRobertaPreLayerNormForTokenClassification', 'TFRobertaPreLayerNormMainLayer', 'TFRobertaPreLayerNormModel', 'TFRobertaPreLayerNormPreTrainedModel', 'TFRobertaPreTrainedModel', 'TFSamModel', 'TFSamPreTrainedModel', 'TFSegformerDecodeHead', 'TFSegformerForImageClassification', 'TFSegformerForSemanticSegmentation', 'TFSegformerModel', 'TFSegformerPreTrainedModel', 'TFSequenceSummary', 'TFSharedEmbeddings', 'TFSpeech2TextForConditionalGeneration', 'TFSpeech2TextModel', 'TFSpeech2TextPreTrainedModel', 'TFSwinForImageClassification', 'TFSwinForMaskedImageModeling', 'TFSwinModel', 'TFSwinPreTrainedModel', 'TFT5EncoderModel', 'TFT5ForConditionalGeneration', 'TFT5Model', 'TFT5PreTrainedModel', 'TFTapasForMaskedLM', 'TFTapasForQuestionAnswering', 'TFTapasForSequenceClassification', 'TFTapasModel', 'TFTapasPreTrainedModel', 'TFTemperatureLogitsWarper', 'TFTopKLogitsWarper', 'TFTopPLogitsWarper', 'TFTrainer', 'TFTrainingArguments', 'TFTransfoXLForSequenceClassification', 'TFTransfoXLLMHeadModel', 'TFTransfoXLMainLayer', 'TFTransfoXLModel', 'TFTransfoXLPreTrainedModel', 'TFViTForImageClassification', 'TFViTMAEForPreTraining', 'TFViTMAEModel', 'TFViTMAEPreTrainedModel', 'TFViTModel', 'TFViTPreTrainedModel', 'TFVisionEncoderDecoderModel', 'TFVisionTextDualEncoderModel', 'TFWav2Vec2ForCTC', 'TFWav2Vec2ForSequenceClassification', 'TFWav2Vec2Model', 'TFWav2Vec2PreTrainedModel', 'TFWhisperForConditionalGeneration', 'TFWhisperModel', 'TFWhisperPreTrainedModel', 'TFXGLMForCausalLM', 'TFXGLMModel', 'TFXGLMPreTrainedModel', 'TFXLMForMultipleChoice', 'TFXLMForQuestionAnsweringSimple', 'TFXLMForSequenceClassification', 'TFXLMForTokenClassification', 'TFXLMMainLayer', 'TFXLMModel', 'TFXLMPreTrainedModel', 'TFXLMRobertaForCausalLM', 'TFXLMRobertaForMaskedLM', 'TFXLMRobertaForMultipleChoice', 'TFXLMRobertaForQuestionAnswering', 'TFXLMRobertaForSequenceClassification', 'TFXLMRobertaForTokenClassification', 'TFXLMRobertaModel', 'TFXLMRobertaPreTrainedModel', 'TFXLMWithLMHeadModel', 'TFXLNetForMultipleChoice', 'TFXLNetForQuestionAnsweringSimple', 'TFXLNetForSequenceClassification', 'TFXLNetForTokenClassification', 'TFXLNetLMHeadModel', 'TFXLNetMainLayer', 'TFXLNetModel', 'TFXLNetPreTrainedModel', 'TF_ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_BLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CONVBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CTRL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CVT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_EFFICIENTFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ELECTRA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_FLAUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_FUNNEL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_GPT2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_HUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LAYOUTLMV3_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LONGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LXMERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MOBILEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MOBILEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_CAUSAL_LM_MAPPING', 'TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING', 'TF_MODEL_FOR_MASKED_LM_MAPPING', 'TF_MODEL_FOR_MASK_GENERATION_MAPPING', 'TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'TF_MODEL_FOR_PRETRAINING_MAPPING', 'TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING', 'TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_TEXT_ENCODING_MAPPING', 'TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_VISION_2_SEQ_MAPPING', 'TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING', 'TF_MODEL_MAPPING', 'TF_MODEL_WITH_LM_HEAD_MAPPING', 'TF_MPNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_REGNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_REMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_RESNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROBERTA_PRELAYERNORM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SAM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SEGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SPEECH_TO_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_T5_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_TAPAS_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_WAV_2_VEC_2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_WEIGHTS_NAME', 'TF_WHISPER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XGLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TIMESFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TIMESFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TIME_SERIES_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TIME_SERIES_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TOKENIZER_MAPPING', 'TRAJECTORY_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TRAJECTORY_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TRANSFORMERS_CACHE', 'TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TROCR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TROCR_PRETRAINED_MODEL_ARCHIVE_LIST', 'TVLT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TVLT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TableQuestionAnsweringPipeline', 'TableTransformerConfig', 'TableTransformerForObjectDetection', 'TableTransformerModel', 'TableTransformerPreTrainedModel', 'TapasConfig', 'TapasForMaskedLM', 'TapasForQuestionAnswering', 'TapasForSequenceClassification', 'TapasModel', 'TapasPreTrainedModel', 'TapasTokenizer', 'TapexTokenizer', 'TemperatureLogitsWarper', 'TensorFlowBenchmark', 'TensorFlowBenchmarkArguments', 'TensorType', 'Text2TextGenerationPipeline', 'TextClassificationPipeline', 'TextDataset', 'TextDatasetForNextSentencePrediction', 'TextGenerationPipeline', 'TextIteratorStreamer', 'TextStreamer', 'TimeSeriesTransformerConfig', 'TimeSeriesTransformerForPrediction', 'TimeSeriesTransformerModel', 'TimeSeriesTransformerPreTrainedModel', 'TimesformerConfig', 'TimesformerForVideoClassification', 'TimesformerModel', 'TimesformerPreTrainedModel', 'TimmBackbone', 'TimmBackboneConfig', 'TokenClassificationPipeline', 'TokenSpan', 'Tool', 'TopKLogitsWarper', 'TopPLogitsWarper', 'TrOCRConfig', 'TrOCRForCausalLM', 'TrOCRPreTrainedModel', 'TrOCRProcessor', 'Trainer', 'TrainerCallback', 'TrainerControl', 'TrainerState', 'TrainingArguments', 'TrajectoryTransformerConfig', 'TrajectoryTransformerModel', 'TrajectoryTransformerPreTrainedModel', 'TransfoXLConfig', 'TransfoXLCorpus', 'TransfoXLForSequenceClassification', 'TransfoXLLMHeadModel', 'TransfoXLModel', 'TransfoXLPreTrainedModel', 'TransfoXLTokenizer', 'TranslationPipeline', 'TvltConfig', 'TvltFeatureExtractor', 'TvltForAudioVisualClassification', 'TvltForPreTraining', 'TvltImageProcessor', 'TvltModel', 'TvltPreTrainedModel', 'TvltProcessor', 'TypicalLogitsWarper', 'UMT5Config', 'UMT5EncoderModel', 'UMT5ForConditionalGeneration', 'UMT5ForQuestionAnswering', 'UMT5Model', 'UMT5PreTrainedModel', 'UNISPEECH_PRETRAINED_CONFIG_ARCHIVE_MAP', 'UNISPEECH_PRETRAINED_MODEL_ARCHIVE_LIST', 'UNISPEECH_SAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'UNISPEECH_SAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'UniSpeechConfig', 'UniSpeechForCTC', 'UniSpeechForPreTraining', 'UniSpeechForSequenceClassification', 'UniSpeechModel', 'UniSpeechPreTrainedModel', 'UniSpeechSatConfig', 'UniSpeechSatForAudioFrameClassification', 'UniSpeechSatForCTC', 'UniSpeechSatForPreTraining', 'UniSpeechSatForSequenceClassification', 'UniSpeechSatForXVector', 'UniSpeechSatModel', 'UniSpeechSatPreTrainedModel', 'UperNetConfig', 'UperNetForSemanticSegmentation', 'UperNetPreTrainedModel', 'VAN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VAN_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIDEOMAE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIDEOMAE_PRETRAINED_MODEL_ARCHIVE_LIST', 'VILT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VILT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VISUAL_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VISUAL_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_HYBRID_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_HYBRID_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_MAE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_MAE_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_MSN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_MSN_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VanConfig', 'VanForImageClassification', 'VanModel', 'VanPreTrainedModel', 'ViTConfig', 'ViTFeatureExtractor', 'ViTForImageClassification', 'ViTForMaskedImageModeling', 'ViTHybridConfig', 'ViTHybridForImageClassification', 'ViTHybridImageProcessor', 'ViTHybridModel', 'ViTHybridPreTrainedModel', 'ViTImageProcessor', 'ViTMAEConfig', 'ViTMAEForPreTraining', 'ViTMAELayer', 'ViTMAEModel', 'ViTMAEPreTrainedModel', 'ViTMSNConfig', 'ViTMSNForImageClassification', 'ViTMSNModel', 'ViTMSNPreTrainedModel', 'ViTModel', 'ViTPreTrainedModel', 'VideoClassificationPipeline', 'VideoMAEConfig', 'VideoMAEFeatureExtractor', 'VideoMAEForPreTraining', 'VideoMAEForVideoClassification', 'VideoMAEImageProcessor', 'VideoMAEModel', 'VideoMAEPreTrainedModel', 'ViltConfig', 'ViltFeatureExtractor', 'ViltForImageAndTextRetrieval', 'ViltForImagesAndTextClassification', 'ViltForMaskedLM', 'ViltForQuestionAnswering', 'ViltForTokenClassification', 'ViltImageProcessor', 'ViltLayer', 'ViltModel', 'ViltPreTrainedModel', 'ViltProcessor', 'VisionEncoderDecoderConfig', 'VisionEncoderDecoderModel', 'VisionTextDualEncoderConfig', 'VisionTextDualEncoderModel', 'VisionTextDualEncoderProcessor', 'VisualBertConfig', 'VisualBertForMultipleChoice', 'VisualBertForPreTraining', 'VisualBertForQuestionAnswering', 'VisualBertForRegionToPhraseAlignment', 'VisualBertForVisualReasoning', 'VisualBertLayer', 'VisualBertModel', 'VisualBertPreTrainedModel', 'VisualQuestionAnsweringPipeline', 'VivitConfig', 'VivitForVideoClassification', 'VivitImageProcessor', 'VivitModel', 'VivitPreTrainedModel', 'WAV2VEC2_CONFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAV2VEC2_CONFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'WAVLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAVLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'WAV_2_VEC_2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAV_2_VEC_2_PRETRAINED_MODEL_ARCHIVE_LIST', 'WEIGHTS_NAME', 'WHISPER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WHISPER_PRETRAINED_MODEL_ARCHIVE_LIST', 'WarmUp', 'Wav2Vec2CTCTokenizer', 'Wav2Vec2Config', 'Wav2Vec2ConformerConfig', 'Wav2Vec2ConformerForAudioFrameClassification', 'Wav2Vec2ConformerForCTC', 'Wav2Vec2ConformerForPreTraining', 'Wav2Vec2ConformerForSequenceClassification', 'Wav2Vec2ConformerForXVector', 'Wav2Vec2ConformerModel', 'Wav2Vec2ConformerPreTrainedModel', 'Wav2Vec2FeatureExtractor', 'Wav2Vec2ForAudioFrameClassification', 'Wav2Vec2ForCTC', 'Wav2Vec2ForMaskedLM', 'Wav2Vec2ForPreTraining', 'Wav2Vec2ForSequenceClassification', 'Wav2Vec2ForXVector', 'Wav2Vec2Model', 'Wav2Vec2PhonemeCTCTokenizer', 'Wav2Vec2PreTrainedModel', 'Wav2Vec2Processor', 'Wav2Vec2ProcessorWithLM', 'Wav2Vec2Tokenizer', 'WavLMConfig', 'WavLMForAudioFrameClassification', 'WavLMForCTC', 'WavLMForSequenceClassification', 'WavLMForXVector', 'WavLMModel', 'WavLMPreTrainedModel', 'WhisperConfig', 'WhisperFeatureExtractor', 'WhisperForAudioClassification', 'WhisperForConditionalGeneration', 'WhisperModel', 'WhisperPreTrainedModel', 'WhisperProcessor', 'WhisperTokenizer', 'WhisperTokenizerFast', 'WordpieceTokenizer', 'XCLIPConfig', 'XCLIPModel', 'XCLIPPreTrainedModel', 'XCLIPProcessor', 'XCLIPTextConfig', 'XCLIPTextModel', 'XCLIPVisionConfig', 'XCLIPVisionModel', 'XCLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XCLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'XGLMConfig', 'XGLMForCausalLM', 'XGLMModel', 'XGLMPreTrainedModel', 'XGLMTokenizer', 'XGLMTokenizerFast', 'XGLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XGLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLMConfig', 'XLMForMultipleChoice', 'XLMForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMForSequenceClassification', 'XLMForTokenClassification', 'XLMModel', 'XLMPreTrainedModel', 'XLMProphetNetConfig', 'XLMProphetNetDecoder', 'XLMProphetNetEncoder', 'XLMProphetNetForCausalLM', 'XLMProphetNetForConditionalGeneration', 'XLMProphetNetModel', 'XLMProphetNetPreTrainedModel', 'XLMProphetNetTokenizer', 'XLMRobertaConfig', 'XLMRobertaForCausalLM', 'XLMRobertaForMaskedLM', 'XLMRobertaForMultipleChoice', 'XLMRobertaForQuestionAnswering', 'XLMRobertaForSequenceClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaModel', 'XLMRobertaPreTrainedModel', 'XLMRobertaTokenizer', 'XLMRobertaTokenizerFast', 'XLMRobertaXLConfig', 'XLMRobertaXLForCausalLM', 'XLMRobertaXLForMaskedLM', 'XLMRobertaXLForMultipleChoice', 'XLMRobertaXLForQuestionAnswering', 'XLMRobertaXLForSequenceClassification', 'XLMRobertaXLForTokenClassification', 'XLMRobertaXLModel', 'XLMRobertaXLPreTrainedModel', 'XLMTokenizer', 'XLMWithLMHeadModel', 'XLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_PROPHETNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_ROBERTA_XL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_ROBERTA_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLNetConfig', 'XLNetForMultipleChoice', 'XLNetForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XLNetForSequenceClassification', 'XLNetForTokenClassification', 'XLNetLMHeadModel', 'XLNetModel', 'XLNetPreTrainedModel', 'XLNetTokenizer', 'XLNetTokenizerFast', 'XMOD_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XMOD_PRETRAINED_MODEL_ARCHIVE_LIST', 'XmodConfig', 'XmodForCausalLM', 'XmodForMaskedLM', 'XmodForMultipleChoice', 'XmodForQuestionAnswering', 'XmodForSequenceClassification', 'XmodForTokenClassification', 'XmodModel', 'XmodPreTrainedModel', 'YOLOS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'YOLOS_PRETRAINED_MODEL_ARCHIVE_LIST', 'YOSO_PRETRAINED_CONFIG_ARCHIVE_MAP', 'YOSO_PRETRAINED_MODEL_ARCHIVE_LIST', 'YolosConfig', 'YolosFeatureExtractor', 'YolosForObjectDetection', 'YolosImageProcessor', 'YolosModel', 'YolosPreTrainedModel', 'YosoConfig', 'YosoForMaskedLM', 'YosoForMultipleChoice', 'YosoForQuestionAnswering', 'YosoForSequenceClassification', 'YosoForTokenClassification', 'YosoLayer', 'YosoModel', 'YosoPreTrainedModel', 'ZeroShotAudioClassificationPipeline', 'ZeroShotClassificationPipeline', 'ZeroShotImageClassificationPipeline', 'ZeroShotObjectDetectionPipeline', '__all__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_class_to_module', '_import_structure', '_modules', '_name', '_objects', 'activations', 'activations_tf', 'add_end_docstrings', 'add_start_docstrings', 'apply_chunking_to_forward', 'audio_utils', 'benchmark', 'benchmark.benchmark', 'benchmark.benchmark_args', 'benchmark.benchmark_args_tf', 'benchmark.benchmark_tf', 'commands', 'configuration_utils', 'convert_graph_to_onnx', 'convert_slow_tokenizer', 'convert_slow_tokenizers_checkpoints_to_fast', 'convert_tf_hub_seq_to_seq_bert_to_pytorch', 'convert_tf_weight_name_to_pt_weight_name', 'create_optimizer', 'data', 'data.data_collator', 'data.datasets', 'data.metrics', 'data.processors', 'debug_utils', 'deepspeed', 'default_data_collator', 'dependency_versions_check', 'dependency_versions_table', 'dynamic_module_utils', 'enable_full_determinism', 'feature_extraction_sequence_utils', 'feature_extraction_utils', 'file_utils', 'generation', 'generation_flax_utils', 'generation_tf_utils', 'generation_utils', 'get_constant_schedule', 'get_constant_schedule_with_warmup', 'get_cosine_schedule_with_warmup', 'get_cosine_with_hard_restarts_schedule_with_warmup', 'get_inverse_sqrt_schedule', 'get_linear_schedule_with_warmup', 'get_polynomial_decay_schedule_with_warmup', 'get_scheduler', 'glue_compute_metrics', 'glue_convert_examples_to_features', 'glue_output_modes', 'glue_processors', 'glue_tasks_num_labels', 'hf_argparser', 'hyperparameter_search', 'image_processing_utils', 'image_transforms', 'image_utils', 'integrations', 'is_apex_available', 'is_bitsandbytes_available', 'is_clearml_available', 'is_comet_available', 'is_datasets_available', 'is_decord_available', 'is_faiss_available', 'is_flax_available', 'is_keras_nlp_available', 'is_neptune_available', 'is_optuna_available', 'is_phonemizer_available', 'is_psutil_available', 'is_py3nvml_available', 'is_pyctcdecode_available', 'is_ray_available', 'is_ray_tune_available', 'is_safetensors_available', 'is_scipy_available', 'is_sentencepiece_available', 'is_sigopt_available', 'is_sklearn_available', 'is_speech_available', 'is_tensorboard_available', 'is_tensorflow_text_available', 'is_tf_available', 'is_timm_available', 'is_tokenizers_available', 'is_torch_available', 'is_torch_neuroncore_available', 'is_torch_tpu_available', 'is_torchvision_available', 'is_vision_available', 'is_wandb_available', 'keras_callbacks', 'launch_gradio_demo', 'load_pytorch_checkpoint_in_tf2_model', 'load_pytorch_model_in_tf2_model', 'load_pytorch_weights_in_tf2_model', 'load_tf2_checkpoint_in_pytorch_model', 'load_tf2_model_in_pytorch_model', 'load_tf2_weights_in_pytorch_model', 'load_tf_weights_in_albert', 'load_tf_weights_in_bert', 'load_tf_weights_in_bert_generation', 'load_tf_weights_in_big_bird', 'load_tf_weights_in_canine', 'load_tf_weights_in_convbert', 'load_tf_weights_in_electra', 'load_tf_weights_in_funnel', 'load_tf_weights_in_gpt2', 'load_tf_weights_in_gpt_neo', 'load_tf_weights_in_imagegpt', 'load_tf_weights_in_mobilebert', 'load_tf_weights_in_mobilenet_v1', 'load_tf_weights_in_mobilenet_v2', 'load_tf_weights_in_openai_gpt', 'load_tf_weights_in_qdqbert', 'load_tf_weights_in_realm', 'load_tf_weights_in_rembert', 'load_tf_weights_in_roc_bert', 'load_tf_weights_in_roformer', 'load_tf_weights_in_t5', 'load_tf_weights_in_tapas', 'load_tf_weights_in_transfo_xl', 'load_tf_weights_in_xlnet', 'load_tool', 'logging', 'modelcard', 'modeling_flax_outputs', 'modeling_flax_utils', 'modeling_outputs', 'modeling_tf_outputs', 'modeling_tf_pytorch_utils', 'modeling_tf_utils', 'modeling_utils', 'models', 'models.albert', 'models.align', 'models.altclip', 'models.audio_spectrogram_transformer', 'models.auto', 'models.autoformer', 'models.bark', 'models.bart', 'models.barthez', 'models.bartpho', 'models.beit', 'models.bert', 'models.bert_generation', 'models.bert_japanese', 'models.bertweet', 'models.big_bird', 'models.bigbird_pegasus', 'models.biogpt', 'models.bit', 'models.blenderbot', 'models.blenderbot_small', 'models.blip', 'models.blip_2', 'models.bloom', 'models.bridgetower', 'models.byt5', 'models.camembert', 'models.canine', 'models.chinese_clip', 'models.clap', 'models.clip', 'models.clipseg', 'models.codegen', 'models.conditional_detr', 'models.convbert', 'models.convnext', 'models.convnextv2', 'models.cpm', 'models.cpmant', 'models.ctrl', 'models.cvt', 'models.data2vec', 'models.deberta', 'models.deberta_v2', 'models.decision_transformer', 'models.deformable_detr', 'models.deit', 'models.deprecated', 'models.deprecated.bort', 'models.deprecated.mctct', 'models.deprecated.mmbt', 'models.deprecated.retribert', 'models.deprecated.tapex', 'models.deprecated.trajectory_transformer', 'models.deprecated.van', 'models.deta', 'models.detr', 'models.dialogpt', 'models.dinat', 'models.distilbert', 'models.dit', 'models.donut', 'models.dpr', 'models.dpt', 'models.efficientformer', 'models.efficientnet', 'models.electra', 'models.encodec', 'models.encoder_decoder', 'models.ernie', 'models.ernie_m', 'models.esm', 'models.falcon', 'models.flaubert', 'models.flava', 'models.fnet', 'models.focalnet', 'models.fsmt', 'models.funnel', 'models.git', 'models.glpn', 'models.gpt2', 'models.gpt_bigcode', 'models.gpt_neo', 'models.gpt_neox', 'models.gpt_neox_japanese', 'models.gpt_sw3', 'models.gptj', 'models.gptsan_japanese', 'models.graphormer', 'models.groupvit', 'models.herbert', 'models.hubert', 'models.ibert', 'models.imagegpt', 'models.informer', 'models.instructblip', 'models.jukebox', 'models.layoutlm', 'models.layoutlmv2', 'models.layoutlmv3', 'models.layoutxlm', 'models.led', 'models.levit', 'models.lilt', 'models.llama', 'models.longformer', 'models.longt5', 'models.luke', 'models.lxmert', 'models.m2m_100', 'models.marian', 'models.markuplm', 'models.mask2former', 'models.maskformer', 'models.mbart', 'models.mbart50', 'models.mega', 'models.megatron_bert', 'models.megatron_gpt2', 'models.mgp_str', 'models.mluke', 'models.mobilebert', 'models.mobilenet_v1', 'models.mobilenet_v2', 'models.mobilevit', 'models.mobilevitv2', 'models.mpnet', 'models.mra', 'models.mt5', 'models.musicgen', 'models.mvp', 'models.nat', 'models.nezha', 'models.nllb', 'models.nllb_moe', 'models.nystromformer', 'models.oneformer', 'models.open_llama', 'models.openai', 'models.opt', 'models.owlvit', 'models.pegasus', 'models.pegasus_x', 'models.perceiver', 'models.phobert', 'models.pix2struct', 'models.plbart', 'models.poolformer', 'models.prophetnet', 'models.qdqbert', 'models.rag', 'models.realm', 'models.reformer', 'models.regnet', 'models.rembert', 'models.resnet', 'models.roberta', 'models.roberta_prelayernorm', 'models.roc_bert', 'models.roformer', 'models.rwkv', 'models.sam', 'models.segformer', 'models.sew', 'models.sew_d', 'models.speech_encoder_decoder', 'models.speech_to_text', 'models.speech_to_text_2', 'models.speecht5', 'models.splinter', 'models.squeezebert', 'models.swiftformer', 'models.swin', 'models.swin2sr', 'models.swinv2', 'models.switch_transformers', 'models.t5', 'models.table_transformer', 'models.tapas', 'models.time_series_transformer', 'models.timesformer', 'models.timm_backbone', 'models.transfo_xl', 'models.trocr', 'models.tvlt', 'models.umt5', 'models.unispeech', 'models.unispeech_sat', 'models.upernet', 'models.videomae', 'models.vilt', 'models.vision_encoder_decoder', 'models.vision_text_dual_encoder', 'models.visual_bert', 'models.vit', 'models.vit_hybrid', 'models.vit_mae', 'models.vit_msn', 'models.vivit', 'models.wav2vec2', 'models.wav2vec2_conformer', 'models.wav2vec2_phoneme', 'models.wav2vec2_with_lm', 'models.wavlm', 'models.whisper', 'models.x_clip', 'models.xglm', 'models.xlm', 'models.xlm_prophetnet', 'models.xlm_roberta', 'models.xlm_roberta_xl', 'models.xlnet', 'models.xmod', 'models.yolos', 'models.yoso', 'onnx', 'optimization', 'optimization_tf', 'pipeline', 'pipelines', 'processing_utils', 'prune_layer', 'pytorch_utils', 'requires_backends', 'sagemaker', 'set_seed', 'shape_list', 'squad_convert_examples_to_features', 'testing_utils', 'tf_top_k_top_p_filtering', 'tf_utils', 'time_series_utils', 'tokenization_utils', 'tokenization_utils_base', 'tokenization_utils_fast', 'tools', 'top_k_top_p_filtering', 'torch_distributed_zero_first', 'trainer', 'trainer_callback', 'trainer_pt_utils', 'trainer_seq2seq', 'trainer_tf', 'trainer_utils', 'training_args', 'training_args_seq2seq', 'training_args_tf', 'utils', 'utils.bitsandbytes', 'utils.dummy_keras_nlp_objects', 'utils.dummy_tensorflow_text_objects', 'utils.quantization_config', 'xnli_compute_metrics', 'xnli_output_modes', 'xnli_processors', 'xnli_tasks_num_labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note:\n",
        "- 'transformers' module or package contains donut related class\n",
        "      - 'DonutFeatureExtractor', 'DonutProcessor'\n",
        "  - The DonutFeatureExtractor class is responsible for preprocessing the input image and [XLMRobertaTokenizer/XLMRobertaTokenizerFast] decodes the generated target tokens to the target string.\n",
        "  - The DonutProcessor wraps DonutFeatureExtractor and [XLMRobertaTokenizer/XLMRobertaTokenizerFast] into a single instance to both extract the input features and decode the predicted token ids.\n",
        "      - 'DonutImageProcessor':\n",
        "      - 'DonutSwinConfig':\n",
        "      - 'DonutSwinModel':\n",
        "      - 'DonutSwinPreTrainedModel':\n",
        "  - 'VisionEncoderDecoderConfig', 'VisionEncoderDecoderModel'  "
      ],
      "metadata": {
        "id": "_GWRmjvtcMWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(transformers.DonutFeatureExtractor))"
      ],
      "metadata": {
        "id": "Js4uDlagi42K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e9bb5c-8703-4ab4-d51c-6c08fbd9cdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_create_repo', '_get_files_timestamps', '_set_processor_class', '_upload_modified_files', 'align_long_axis', 'from_dict', 'from_json_file', 'from_pretrained', 'get_image_processor_dict', 'model_input_names', 'normalize', 'pad', 'pad_image', 'preprocess', 'push_to_hub', 'register_for_auto_class', 'rescale', 'resize', 'save_pretrained', 'thumbnail', 'to_dict', 'to_json_file', 'to_json_string']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(transformers.DonutFeatureExtractor.from_pretrained))"
      ],
      "metadata": {
        "id": "f6s6N9nvkXja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120e6386-a97e-486d-ff94-1e2a4bd37f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__func__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(transformers.DonutImageProcessor))"
      ],
      "metadata": {
        "id": "NgI1OzCNi4zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90adb8b-7b24-466d-ef3f-debb950d97b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_create_repo', '_get_files_timestamps', '_set_processor_class', '_upload_modified_files', 'align_long_axis', 'from_dict', 'from_json_file', 'from_pretrained', 'get_image_processor_dict', 'model_input_names', 'normalize', 'pad', 'pad_image', 'preprocess', 'push_to_hub', 'register_for_auto_class', 'rescale', 'resize', 'save_pretrained', 'thumbnail', 'to_dict', 'to_json_file', 'to_json_string']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(print(dir(transformers.DonutProcessor)))"
      ],
      "metadata": {
        "id": "_hPPoZlki4u8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8370dfe-4ac3-4e61-e927-324e093bacb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_create_repo', '_get_arguments_from_pretrained', '_get_files_timestamps', '_upload_modified_files', 'as_target_processor', 'attributes', 'batch_decode', 'decode', 'feature_extractor', 'feature_extractor_class', 'from_pretrained', 'image_processor_class', 'model_input_names', 'push_to_hub', 'register_for_auto_class', 'save_pretrained', 'token2json', 'tokenizer_class']\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(transformers.DonutSwinConfig))"
      ],
      "metadata": {
        "id": "NUGhZJL5i4sM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c764557-83e0-402d-fc4f-d8059252b6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_create_repo', '_dict_from_json_file', '_get_config_dict', '_get_files_timestamps', '_set_token_in_kwargs', '_upload_modified_files', 'attribute_map', 'dict_torch_dtype_to_str', 'from_dict', 'from_json_file', 'from_pretrained', 'get_config_dict', 'is_composition', 'model_type', 'name_or_path', 'num_labels', 'push_to_hub', 'register_for_auto_class', 'save_pretrained', 'to_dict', 'to_diff_dict', 'to_json_file', 'to_json_string', 'update', 'update_from_string', 'use_return_dict']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(transformers.DonutProcessor.from_pretrained))"
      ],
      "metadata": {
        "id": "DMaH6_34lAEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034416c7-511e-40b6-cf94-f6401bef6f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__func__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution of DonutProcessor module and train donut-base:\n",
        "- processor.tokenizer.convert_tokens_to_ids and add_tokens working principle"
      ],
      "metadata": {
        "id": "6_-SgCDB6USY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"naver-clova-ix/donut-base\"    ## base donut model"
      ],
      "metadata": {
        "id": "_XGCfJzYiPc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = DonutProcessor.from_pretrained(model_checkpoint)\n",
        "# processing raw data to the input format required for the Donut Model"
      ],
      "metadata": {
        "id": "BU8OUxro5Qab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "a7da7efb0ce94c78b4594a7f8eed4820",
            "0895190f97004b8699363aa6d8f13fc3",
            "908f7a29c409437099530f08e3be27a5",
            "b9405c8d40a2402bbadd769f37760d96",
            "3e506bf19c9f465cb3df893968149118",
            "b317cc50fbcd44ea89bb9c3d1bb553a9",
            "04e50d3d8fac4bc7a99de4fff654e1bd",
            "30767f6ac92d4e13bba51024761c7a9e",
            "0808f35390934b3b9453c0fc9692a0a7",
            "9f82c99c1ec249a68e44c213ce2bf815",
            "f568b20307a0428aa7f2b1a933f7a708",
            "56d1355146c14fa79be2966b6c0dff1a",
            "493d109e9aca40139f7cd88c54b81890",
            "44164d915b5843dbad80d8766f074c8e",
            "a9e248a637424f54a070b1dbca2f4de3",
            "326a8dcafd2e4deb8e0363daa11d3fcd",
            "33882ebfbf9f47d2b25c78d51b59935e",
            "bb6eda59b35146bba2fa4db25a68f129",
            "017d3640c3ce4f8cb1f3036ddf4f9669",
            "ec89932a7b2a4c038471f2ce8f7858c9",
            "da71f60e1aaa46b2a11a367d73b90b1f",
            "28f13468b4044ef0bbd161cd2cda68ee",
            "b0b34f4c3a8244e28c9eb8760bc0de3c",
            "485653624828448196dc9d6998eb131a",
            "0c03ec9bf54b44bcb32ca4a2c9b4704c",
            "6834adf4dd2f43cd9a9cfa7155f75bdc",
            "07649bf39b464764807e036adae12a42",
            "16f9a839d8014c0b9e47b27a3807452e",
            "c31fd517557e47f8a2250eb65a46b4b3",
            "01fd786e43ca427493a1bb70bd39e4fa",
            "e19f320f11fc41b4b638e5eccfa7f075",
            "d68780b19b6c492c95d9a4595844b265",
            "fe98383e6a534edb96f413eda0b7fd80",
            "c589ee988a1c4762a5221db3d47a99ef",
            "327ff9a62eaf4ad88dfff8fbcc1ae9e8",
            "cdf3e362232a4a69b7289bb284421982",
            "55d56ca19fbc48c888ceafb646c54f18",
            "459bdc7fd38342d48e308b085bd7b099",
            "feef2349dea34f068dcfd62893c980fc",
            "577f1b786cac47718d01c8d8637c9d4c",
            "bdf5e2dc702c4124b7e7ec54ce8540fd",
            "b40ad846c9394458b4ee3ee3392729c4",
            "90f55df0523a4bfd9e2e37e790875c32",
            "825b1967d4ff4906a14acc53331b185b",
            "1b167cf4d38d40289e0ec054399e9900",
            "7d22bb5d3c94498081b3e5337731d712",
            "a84035229c094abaa7172a5f1627c6dc",
            "a11b4537cda04d38a52dea0e53357967",
            "f3538b9bb06843039a1207acd9d778a3",
            "4acffc4e311b4d41a37e5fe07940ed6b",
            "1eef202031114694bbce84de4fd8fb63",
            "5103c097ea214509af751f393b0a2e24",
            "a371d7ecb96946888d9d7163a62d0368",
            "4e731e618f7a493c840714d2db9d32b4",
            "b9ec3b9a333d4fb981f30863b64163de",
            "89366503d419492ca16a1b40951521a0",
            "0753e140e84449f6a56d8ca4acce3fbd",
            "b351e7efc16d4132b85487877bf9f613",
            "843091a9af8f4f2ab5f6a6442c220160",
            "6dff655e42b141beb9ebd9df0636427b",
            "23f444d1824943bd92b2cc9dd0446ea0",
            "8ad9069f9e1e4ba0a6d979398043da72",
            "de62717e67784090b5e17d58a289d139",
            "d97d904460544caca98954ae3eb8ef43",
            "1bce75faf782452ea8c431206c202880",
            "f9e3c44cbc1245d4812fcfbdf4151a3c"
          ]
        },
        "outputId": "5c5b0be5-5355-43e9-9915-344b9b20a652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)rocessor_config.json:   0%|          | 0.00/362 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7da7efb0ce94c78b4594a7f8eed4820"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/518 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56d1355146c14fa79be2966b6c0dff1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0b34f4c3a8244e28c9eb8760bc0de3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c589ee988a1c4762a5221db3d47a99ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/71.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b167cf4d38d40289e0ec054399e9900"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89366503d419492ca16a1b40951521a0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(processor.tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kmy9JQ79aIN7",
        "outputId": "59efd8c7-f3e5-42b2-af00-68fcbe0111f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57525"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "additional_tokens = [\"<s_answer>\"]\n",
        "print(processor.tokenizer.add_tokens(additional_tokens))\n",
        "print(len(processor.tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCzaSRHfL9U",
        "outputId": "b9e4373d-6c08-46c9-ee1b-cb2d1b02bfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "57526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = VisionEncoderDecoderModel.from_pretrained(model_checkpoint)\n",
        "model_0.decoder.resize_token_embeddings(len(processor.tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "9fc0cac0ca2f46f3bdd315324a8d2b30",
            "e475b5295df24701b9cd12ba497826c4",
            "48a6726353a245b9a1e3b27343a489a2",
            "48a3738424db426eb2106b491de58e99",
            "22af3ee22d494b45830fe9a4ba3dddc6",
            "262f179f388f447398cd5af036e51d6f",
            "c56c7f8bceaf4991b74f81dc3a2d0628",
            "25c5520285524620b9446a2a276d7771",
            "383d622d710c4c13b9eb71a4f243592b",
            "6dd8cf7481fe4894bfbd927f56c6c3ec",
            "16c929bc12884fcca3a789f9e9e68b3b",
            "553f8b9e057c4837853d51c84be188e2",
            "b6d6d49913a34c22a616cfa1ec64a41d",
            "25fbdf3bb3de49db8e9820627e81aa49",
            "620cb763c74c4f4d9ca67a4f9246d426",
            "b630efed74bf4795b8ec6a6cbb5aa01b",
            "8af8a5a01c304e6fbe8af404e44464d7",
            "8571eb9788ae433b94710d6cce90a264",
            "1a06f37ad473463c96d71ca2407cdd92",
            "03e69a18706b4e108bbe562eabf1c2ce",
            "cc03a1669ae24e9e89e1569084884dcb",
            "2d181dd8a2e748c89756b2f6b9785617"
          ]
        },
        "id": "tCZ8hdyxsW3q",
        "outputId": "427b4765-da1f-413c-daaa-c7f0183ce73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.74k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fc0cac0ca2f46f3bdd315324a8d2b30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "553f8b9e057c4837853d51c84be188e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(57526, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.tokenizer.get_vocab()[\"<s_answer>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9e3RFCUfYkk",
        "outputId": "3fad2942-81d1-4982-c63d-9ce7ba84fc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(\"<s_answer>\")\n",
        "print(prompt_end_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj71LegDfef8",
        "outputId": "38b929da-70b8-4925-c4e1-0a1dea41a701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.tokenizer.__class__.from_pretrained(processor.tokenizer.name_or_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-BpiI6wnTwZ",
        "outputId": "90d67cb9-f4d0-4b40-fdb4-548762bb164f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details of 'processor' instance and how 'pixel_values' are returning its value"
      ],
      "metadata": {
        "id": "0dEL9_WE6oQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(processor)"
      ],
      "metadata": {
        "id": "P_XQQyWXi_6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a109ba8-2829-41b7-8a6c-e65f69b78d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.donut.processing_donut.DonutProcessor"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor)"
      ],
      "metadata": {
        "id": "A_993zctx2ND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f6d102-c168-4386-a0d0-e5ae07aa5432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DonutProcessor:\n",
            "- image_processor: DonutImageProcessor {\n",
            "  \"do_align_long_axis\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_pad\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"do_thumbnail\": true,\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_processor_type\": \"DonutImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"processor_class\": \"DonutProcessor\",\n",
            "  \"resample\": 2,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 2560,\n",
            "    \"width\": 1920\n",
            "  }\n",
            "}\n",
            "\n",
            "- tokenizer: XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(processor))"
      ],
      "metadata": {
        "id": "1wjr_Q9ui_37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27347394-1755-4bd6-9882-f2a26be416f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_create_repo', '_get_arguments_from_pretrained', '_get_files_timestamps', '_in_target_context_manager', '_upload_modified_files', 'as_target_processor', 'attributes', 'batch_decode', 'current_processor', 'decode', 'feature_extractor', 'feature_extractor_class', 'from_pretrained', 'image_processor', 'image_processor_class', 'model_input_names', 'push_to_hub', 'register_for_auto_class', 'save_pretrained', 'token2json', 'tokenizer', 'tokenizer_class']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking the dictionary of attribute\n",
        "print(vars(processor))"
      ],
      "metadata": {
        "id": "v3D9q5rlaJTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992fd366-28c3-429a-acd1-5d0c51e90e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image_processor': DonutImageProcessor {\n",
            "  \"do_align_long_axis\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_pad\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"do_thumbnail\": true,\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_processor_type\": \"DonutImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"processor_class\": \"DonutProcessor\",\n",
            "  \"resample\": 2,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 2560,\n",
            "    \"width\": 1920\n",
            "  }\n",
            "}\n",
            ", 'tokenizer': XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=True), 'current_processor': DonutImageProcessor {\n",
            "  \"do_align_long_axis\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_pad\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"do_thumbnail\": true,\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_processor_type\": \"DonutImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"processor_class\": \"DonutProcessor\",\n",
            "  \"resample\": 2,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 2560,\n",
            "    \"width\": 1920\n",
            "  }\n",
            "}\n",
            ", '_in_target_context_manager': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if hasattr(processor, 'pixel_values'):\n",
        "    print(processor.pixel_values)"
      ],
      "metadata": {
        "id": "9t3KCivZmr-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(processor)"
      ],
      "metadata": {
        "id": "Pc4AGR_Vi9SN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122200d2-13d9-4912-9f6a-0c1a1302433b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on DonutProcessor in module transformers.models.donut.processing_donut object:\n",
            "\n",
            "class DonutProcessor(transformers.processing_utils.ProcessorMixin)\n",
            " |  DonutProcessor(image_processor=None, tokenizer=None, **kwargs)\n",
            " |  \n",
            " |  Constructs a Donut processor which wraps a Donut image processor and an XLMRoBERTa tokenizer into a single\n",
            " |  processor.\n",
            " |  \n",
            " |  [`DonutProcessor`] offers all the functionalities of [`DonutImageProcessor`] and\n",
            " |  [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]. See the [`~DonutProcessor.__call__`] and\n",
            " |  [`~DonutProcessor.decode`] for more information.\n",
            " |  \n",
            " |  Args:\n",
            " |      image_processor ([`DonutImageProcessor`]):\n",
            " |          An instance of [`DonutImageProcessor`]. The image processor is a required input.\n",
            " |      tokenizer ([`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]):\n",
            " |          An instance of [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]. The tokenizer is a required input.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DonutProcessor\n",
            " |      transformers.processing_utils.ProcessorMixin\n",
            " |      transformers.utils.hub.PushToHubMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      When used in normal mode, this method forwards all its arguments to AutoImageProcessor's\n",
            " |      [`~AutoImageProcessor.__call__`] and returns its output. If used in the context\n",
            " |      [`~DonutProcessor.as_target_processor`] this method forwards all its arguments to DonutTokenizer's\n",
            " |      [`~DonutTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more information.\n",
            " |  \n",
            " |  __init__(self, image_processor=None, tokenizer=None, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  as_target_processor(self)\n",
            " |      Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning TrOCR.\n",
            " |  \n",
            " |  batch_decode(self, *args, **kwargs)\n",
            " |      This method forwards all its arguments to DonutTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please refer\n",
            " |      to the docstring of this method for more information.\n",
            " |  \n",
            " |  decode(self, *args, **kwargs)\n",
            " |      This method forwards all its arguments to DonutTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to the\n",
            " |      docstring of this method for more information.\n",
            " |  \n",
            " |  token2json(self, tokens, is_inner_value=False, added_vocab=None)\n",
            " |      Convert a (generated) token sequence into an ordered JSON format.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  feature_extractor\n",
            " |  \n",
            " |  feature_extractor_class\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  attributes = ['image_processor', 'tokenizer']\n",
            " |  \n",
            " |  image_processor_class = 'AutoImageProcessor'\n",
            " |  \n",
            " |  tokenizer_class = 'AutoTokenizer'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.processing_utils.ProcessorMixin:\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, use_auth_token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '10GB', create_pr: bool = False, safe_serialization: bool = False, **deprecated_kwargs) -> str\n",
            " |      Upload the processor files to the 🤗 Model Hub while synchronizing a local clone of the repo in\n",
            " |      `repo_path_or_name`.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          repo_id (`str`):\n",
            " |              The name of the repository you want to push your processor to. It should contain your organization name\n",
            " |              when pushing to a given organization.\n",
            " |          use_temp_dir (`bool`, *optional*):\n",
            " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
            " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
            " |          commit_message (`str`, *optional*):\n",
            " |              Message to commit while pushing. Will default to `\"Upload processor\"`.\n",
            " |          private (`bool`, *optional*):\n",
            " |              Whether or not the repository created should be private.\n",
            " |          use_auth_token (`bool` or `str`, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
            " |              is not specified.\n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
            " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
            " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
            " |              by a unit (like `\"5MB\"`).\n",
            " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
            " |          safe_serialization (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      from transformers import AutoProcessor\n",
            " |      \n",
            " |      processor = AutoProcessor.from_pretrained(\"bert-base-cased\")\n",
            " |      \n",
            " |      # Push the processor to your namespace with the name \"my-finetuned-bert\".\n",
            " |      processor.push_to_hub(\"my-finetuned-bert\")\n",
            " |      \n",
            " |      # Push the processor to an organization with the name \"my-finetuned-bert\".\n",
            " |      processor.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
            " |      ```\n",
            " |  \n",
            " |  save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs)\n",
            " |      Saves the attributes of this processor (feature extractor, tokenizer...) in the specified directory so that it\n",
            " |      can be reloaded using the [`~ProcessorMixin.from_pretrained`] method.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      This class method is simply calling [`~feature_extraction_utils.FeatureExtractionMixin.save_pretrained`] and\n",
            " |      [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`]. Please refer to the docstrings of the\n",
            " |      methods above for more information.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          save_directory (`str` or `os.PathLike`):\n",
            " |              Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will\n",
            " |              be created if it does not exist).\n",
            " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
            " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
            " |              namespace).\n",
            " |          kwargs (`Dict[str, Any]`, *optional*):\n",
            " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from transformers.processing_utils.ProcessorMixin:\n",
            " |  \n",
            " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) from builtins.type\n",
            " |      Instantiate a processor associated with a pretrained model.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      This class method is simply calling the feature extractor\n",
            " |      [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`], image processor\n",
            " |      [`~image_processing_utils.ImageProcessingMixin`] and the tokenizer\n",
            " |      [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] methods. Please refer to the docstrings of the\n",
            " |      methods above for more information.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
            " |              This can be either:\n",
            " |      \n",
            " |              - a string, the *model id* of a pretrained feature_extractor hosted inside a model repo on\n",
            " |                huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or\n",
            " |                namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\n",
            " |              - a path to a *directory* containing a feature extractor file saved using the\n",
            " |                [`~SequenceFeatureExtractor.save_pretrained`] method, e.g., `./my_model_directory/`.\n",
            " |              - a path or url to a saved feature extractor JSON *file*, e.g.,\n",
            " |                `./my_model_directory/preprocessor_config.json`.\n",
            " |          **kwargs\n",
            " |              Additional keyword arguments passed along to both\n",
            " |              [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`] and\n",
            " |              [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\n",
            " |  \n",
            " |  register_for_auto_class(auto_class='AutoProcessor') from builtins.type\n",
            " |      Register this class with a given auto class. This should only be used for custom feature extractors as the ones\n",
            " |      in the library are already mapped with `AutoProcessor`.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoProcessor\"`):\n",
            " |              The auto class to register this new feature extractor with.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.processing_utils.ProcessorMixin:\n",
            " |  \n",
            " |  model_input_names\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from transformers.utils.hub.PushToHubMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of parameter acceptance in processor\n",
        "params_pro = inspect.signature(processor).parameters\n",
        "param_names_pro = [param for param in params_pro.keys()]\n",
        "\n",
        "param_names_pro"
      ],
      "metadata": {
        "id": "-YmKJOmL7N9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383c993c-c7bf-4482-94e4-813932337776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['args', 'kwargs']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "call_sig = inspect.signature(processor.__call__)\n",
        "print(call_sig)"
      ],
      "metadata": {
        "id": "-paVp3yx80cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398240fc-7db2-4a25-846e-08b6368a9acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the source code of the __call__ method\n",
        "source = inspect.getsource(processor.__call__)\n",
        "print(source)"
      ],
      "metadata": {
        "id": "nSDbrWSe9Hr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2129127-e2df-48a6-fcd9-d3697bc6ec14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def __call__(self, *args, **kwargs):\n",
            "        \"\"\"\n",
            "        When used in normal mode, this method forwards all its arguments to AutoImageProcessor's\n",
            "        [`~AutoImageProcessor.__call__`] and returns its output. If used in the context\n",
            "        [`~DonutProcessor.as_target_processor`] this method forwards all its arguments to DonutTokenizer's\n",
            "        [`~DonutTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more information.\n",
            "        \"\"\"\n",
            "        # For backward compatibility\n",
            "        if self._in_target_context_manager:\n",
            "            return self.current_processor(*args, **kwargs)\n",
            "\n",
            "        images = kwargs.pop(\"images\", None)\n",
            "        text = kwargs.pop(\"text\", None)\n",
            "        if len(args) > 0:\n",
            "            images = args[0]\n",
            "            args = args[1:]\n",
            "\n",
            "        if images is None and text is None:\n",
            "            raise ValueError(\"You need to specify either an `images` or `text` input to process.\")\n",
            "\n",
            "        if images is not None:\n",
            "            inputs = self.image_processor(images, *args, **kwargs)\n",
            "        if text is not None:\n",
            "            encodings = self.tokenizer(text, **kwargs)\n",
            "\n",
            "        if text is None:\n",
            "            return inputs\n",
            "        elif images is None:\n",
            "            return encodings\n",
            "        else:\n",
            "            inputs[\"labels\"] = encodings[\"input_ids\"]\n",
            "            return inputs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image_path = \"/content/drive/MyDrive/testImage/Bills.jpeg\"\n",
        "# image = Image.open(image_path)\n",
        "# inputs = processor(image, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "RPdRDTbm4wUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(inputs)"
      ],
      "metadata": {
        "id": "Az8fPq8t9WQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs"
      ],
      "metadata": {
        "id": "W-gkIYZeXZJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note:\n",
        "- BatchFeature class returned by processor(image, return_tensors=\"pt\") is actually a dictionary-like object. It inherits from collections.OrderedDict and contains various keys with their corresponding values.\n",
        "- BatchFeature is a custom class defined in the transformers.image_processing_utils module of the Hugging Face Transformers library. It is specifically designed to represent batches of image features as dictionary-like objects.\n",
        "\n",
        "- In the BatchFeature class definition, you can see that it inherits from the Python collections.abc.Mapping class, which is an abstract base class that defines the interface for dictionary-like objects in Python. This means that BatchFeature is guaranteed to implement all the required methods of a dictionary-like object, including the keys() method, which is why you can call inputs.keys() on a BatchFeature object.\n",
        "\n",
        "- You can also inspect the source code of the BatchFeature class to see how it is implemented and verify that it meets the requirements of a dictionary-like object."
      ],
      "metadata": {
        "id": "zGqy08Q4CyK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs.keys()"
      ],
      "metadata": {
        "id": "iLi2JNic4lhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pixel_values = inputs[\"pixel_values\"]\n",
        "\n",
        "# # Inspect the tensor\n",
        "# print(pixel_values.shape)\n",
        "# print(pixel_values.dtype)\n",
        "# print(pixel_values)"
      ],
      "metadata": {
        "id": "nbVDBuPA4le_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing 'processor.tokenizer' module and researching its details lastly formed decoder_input_ids"
      ],
      "metadata": {
        "id": "nf-EqhYH_LrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX5WkNSX2RK7",
        "outputId": "40d21ba9-3300-4fd4-dffc-fb117cb9661a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(processor.tokenizer))"
      ],
      "metadata": {
        "id": "8T2FbfGoBTkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fc186a-a611-4e9c-c21b-44a2e1b752d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SPECIAL_TOKENS_ATTRIBUTES', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_tokens', '_additional_special_tokens', '_auto_class', '_batch_encode_plus', '_bos_token', '_call_one', '_cls_token', '_convert_encoding', '_convert_id_to_token', '_convert_token_to_id_with_added_voc', '_create_repo', '_decode', '_decode_use_source_tokenizer', '_encode_plus', '_eos_token', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_in_target_context_manager', '_mask_token', '_pad', '_pad_token', '_pad_token_type_id', '_processor_class', '_save_pretrained', '_sep_token', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_tokenizer', '_unk_token', '_upload_modified_files', 'add_special_tokens', 'add_tokens', 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'as_target_tokenizer', 'backend_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'build_inputs_with_special_tokens', 'can_save_slow_tokenizer', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'decoder', 'deprecation_warnings', 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_special_tokens_mask', 'get_vocab', 'init_inputs', 'init_kwargs', 'is_fast', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes', 'model_input_names', 'model_max_length', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token', 'sep_token_id', 'set_truncation_and_padding', 'slow_tokenizer_class', 'special_tokens_map', 'special_tokens_map_extended', 'tokenize', 'train_new_from_iterator', 'truncate_sequences', 'truncation_side', 'unk_token', 'unk_token_id', 'verbose', 'vocab', 'vocab_file', 'vocab_files_names', 'vocab_size']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.tokenizer.batch_decode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtKzFdh_23Xk",
        "outputId": "74260cf9-f050-4408-b693-05ef615643bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method PreTrainedTokenizerBase.batch_decode of XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=True)>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of parameter acceptance in processor.tokenizer method\n",
        "params_tok = inspect.signature(processor.tokenizer).parameters\n",
        "param_names_tok = [param for param in params_tok.keys()]\n",
        "\n",
        "param_names_tok"
      ],
      "metadata": {
        "id": "tIzajxDwgTjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9817c911-a2f3-4a54-b50e-1e5fc416fd96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'text_pair',\n",
              " 'text_target',\n",
              " 'text_pair_target',\n",
              " 'add_special_tokens',\n",
              " 'padding',\n",
              " 'truncation',\n",
              " 'max_length',\n",
              " 'stride',\n",
              " 'is_split_into_words',\n",
              " 'pad_to_multiple_of',\n",
              " 'return_tensors',\n",
              " 'return_token_type_ids',\n",
              " 'return_attention_mask',\n",
              " 'return_overflowing_tokens',\n",
              " 'return_special_tokens_mask',\n",
              " 'return_offsets_mapping',\n",
              " 'return_length',\n",
              " 'verbose',\n",
              " 'kwargs']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the dictionary of attribute\n",
        "vars(processor.tokenizer)"
      ],
      "metadata": {
        "id": "8zDO6rukSqY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58999873-a7a2-4a03-ed0f-80139b3121a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_tokenizer': <tokenizers.Tokenizer at 0x56f04d877a80>,\n",
              " '_decode_use_source_tokenizer': False,\n",
              " 'init_inputs': (),\n",
              " 'init_kwargs': {'bos_token': '<s>',\n",
              "  'eos_token': '</s>',\n",
              "  'sep_token': '</s>',\n",
              "  'cls_token': '<s>',\n",
              "  'unk_token': '<unk>',\n",
              "  'pad_token': '<pad>',\n",
              "  'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True),\n",
              "  'name_or_path': 'naver-clova-ix/donut-base',\n",
              "  'processor_class': 'DonutProcessor',\n",
              "  'sp_model_kwargs': {},\n",
              "  'special_tokens_map_file': None},\n",
              " 'name_or_path': 'naver-clova-ix/donut-base',\n",
              " '_processor_class': 'DonutProcessor',\n",
              " 'model_max_length': 1000000000000000019884624838656,\n",
              " 'padding_side': 'right',\n",
              " 'truncation_side': 'right',\n",
              " 'model_input_names': ['input_ids', 'attention_mask'],\n",
              " 'clean_up_tokenization_spaces': True,\n",
              " 'deprecation_warnings': {},\n",
              " '_in_target_context_manager': False,\n",
              " '_bos_token': '<s>',\n",
              " '_eos_token': '</s>',\n",
              " '_unk_token': '<unk>',\n",
              " '_sep_token': '</s>',\n",
              " '_pad_token': '<pad>',\n",
              " '_cls_token': '<s>',\n",
              " '_mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True),\n",
              " '_pad_token_type_id': 0,\n",
              " '_additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>'],\n",
              " 'verbose': True,\n",
              " 'vocab_file': '/root/.cache/huggingface/hub/models--naver-clova-ix--donut-base/snapshots/a959cf33c20e09215873e338299c900f57047c61/sentencepiece.bpe.model',\n",
              " 'can_save_slow_tokenizer': True}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## To get docstirng and it's details of the list of parameters and return value\n",
        "help(processor.tokenizer.batch_encode_plus)"
      ],
      "metadata": {
        "id": "9nb74apeYC0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86e9c23-a906-48c1-b802-d753156a856d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method batch_encode_plus in module transformers.tokenization_utils_base:\n",
            "\n",
            "batch_encode_plus(batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast instance\n",
            "    Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
            "    \n",
            "    <Tip warning={true}>\n",
            "    \n",
            "    This method is deprecated, `__call__` should be used instead.\n",
            "    \n",
            "    </Tip>\n",
            "    \n",
            "    Args:\n",
            "        batch_text_or_text_pairs (`List[str]`, `List[Tuple[str, str]]`, `List[List[str]]`, `List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also `List[List[int]]`, `List[Tuple[List[int], List[int]]]`):\n",
            "            Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
            "            string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
            "            details in `encode_plus`).\n",
            "    \n",
            "        add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to encode the sequences with the special tokens relative to their model.\n",
            "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            "            Activates and controls padding. Accepts the following values:\n",
            "    \n",
            "            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            "              sequence if provided).\n",
            "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            "              acceptable input length for the model if that argument is not provided.\n",
            "            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            "              lengths).\n",
            "        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            "            Activates and controls truncation. Accepts the following values:\n",
            "    \n",
            "            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            "              to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            "              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            "              sequences (or a batch of pairs) is provided.\n",
            "            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
            "              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            "            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
            "              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            "            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            "              greater than the model maximum admissible input size).\n",
            "        max_length (`int`, *optional*):\n",
            "            Controls the maximum length to use by one of the truncation/padding parameters.\n",
            "    \n",
            "            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            "            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            "            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            "        stride (`int`, *optional*, defaults to 0):\n",
            "            If set to a number along with `max_length`, the overflowing tokens returned when\n",
            "            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            "            returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            "            argument defines the number of overlapping tokens.\n",
            "        is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            "            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            "            which it will tokenize. This is useful for NER or token classification.\n",
            "        pad_to_multiple_of (`int`, *optional*):\n",
            "            If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            "            `>= 7.5` (Volta).\n",
            "        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            "            If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            "    \n",
            "            - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            "            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            "            - `'np'`: Return Numpy `np.ndarray` objects.\n",
            "    \n",
            "        return_token_type_ids (`bool`, *optional*):\n",
            "            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            "            the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            "    \n",
            "            [What are token type IDs?](../glossary#token-type-ids)\n",
            "        return_attention_mask (`bool`, *optional*):\n",
            "            Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            "            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            "    \n",
            "            [What are attention masks?](../glossary#attention-mask)\n",
            "        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            "            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            "            of returning overflowing tokens.\n",
            "        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return special tokens mask information.\n",
            "        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return `(char_start, char_end)` for each token.\n",
            "    \n",
            "            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            "            Python's tokenizer, this method will raise `NotImplementedError`.\n",
            "        return_length  (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the lengths of the encoded inputs.\n",
            "        verbose (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to print more information and warnings.\n",
            "        **kwargs: passed to the `self.tokenize()` method\n",
            "    \n",
            "    Return:\n",
            "        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            "    \n",
            "        - **input_ids** -- List of token ids to be fed to a model.\n",
            "    \n",
            "          [What are input IDs?](../glossary#input-ids)\n",
            "    \n",
            "        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            "          if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            "    \n",
            "          [What are token type IDs?](../glossary#token-type-ids)\n",
            "    \n",
            "        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            "          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            "    \n",
            "          [What are attention masks?](../glossary#attention-mask)\n",
            "    \n",
            "        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            "          `return_overflowing_tokens=True`).\n",
            "        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            "          `return_overflowing_tokens=True`).\n",
            "        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            "          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            "        - **length** -- The length of the inputs (when `return_length=True`)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(processor.tokenizer.add_tokens)"
      ],
      "metadata": {
        "id": "Hyt3bauLcRzz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c7c3a6-edb3-40d8-c6e0-6209186eded4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method add_tokens in module transformers.tokenization_utils_base:\n",
            "\n",
            "add_tokens(new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int method of transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast instance\n",
            "    Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
            "    it with indices starting from length of the current vocabulary and and will be isolated before the tokenization\n",
            "    algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n",
            "    not treated in the same way.\n",
            "    \n",
            "    Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\n",
            "    of the model so that its embedding matrix matches the tokenizer.\n",
            "    \n",
            "    In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
            "    \n",
            "    Args:\n",
            "        new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):\n",
            "            Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n",
            "            token to let you personalize its behavior: whether this token should only match against a single word,\n",
            "            whether this token should strip all potential whitespaces on the left side, whether this token should\n",
            "            strip all potential whitespaces on the right side, etc.\n",
            "        special_tokens (`bool`, *optional*, defaults to `False`):\n",
            "            Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
            "            (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
            "    \n",
            "            See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
            "    \n",
            "    Returns:\n",
            "        `int`: Number of tokens added to the vocabulary.\n",
            "    \n",
            "    Examples:\n",
            "    \n",
            "    ```python\n",
            "    # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
            "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
            "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
            "    \n",
            "    num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\n",
            "    print(\"We have added\", num_added_toks, \"tokens\")\n",
            "    # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
            "    model.resize_token_embeddings(len(tokenizer))\n",
            "    ```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.tokenizer.cls_token)\n",
        "print(processor.tokenizer.eos_token)\n",
        "print(processor.tokenizer.additional_special_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0bSsW_yf6CA",
        "outputId": "8fea0eb1-baac-4e49-bef6-17df12050971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n",
            "</s>\n",
            "['<s_iitcdip>', '<s_synthdog>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Input id creation and prompt creation example"
      ],
      "metadata": {
        "id": "F0lQEwQBcTDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare decoder inputs\n",
        "task_prompt = \"<s_question>{user_input}</s_question><s_answer>J. T. WINEBRENNER</s_answer>\"\n",
        "print(task_prompt, type(task_prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0GnQh091T48",
        "outputId": "fac83b10-6398-40ae-e00d-f70f9934d13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s_question>{user_input}</s_question><s_answer>J. T. WINEBRENNER</s_answer> <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the name of the person in the field?\"\n",
        "print(question, type(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psaOQO-S1Z0s",
        "outputId": "65514f03-556f-4fec-d526-0b1e70983a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the name of the person in the field? <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = task_prompt.replace(\"{user_input}\", question)\n",
        "print(prompt, type(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRJbBThR1Zx2",
        "outputId": "1cc3786a-e568-44c2-9cef-90a7b8c421b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s_question>What is the name of the person in the field?</s_question><s_answer>J. T. WINEBRENNER</s_answer> <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pro_tok = processor.tokenizer(prompt)\n",
        "print(pro_tok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JyFFx1pDdD0",
        "outputId": "dc24baa2-d120-44e9-9319-f3034de04dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [0, 41040, 46192, 41403, 49706, 8888, 34791, 43703, 52743, 48941, 55856, 2587, 48941, 37517, 25515, 48941, 41095, 36209, 40598, 46192, 41403, 49706, 8888, 34791, 57525, 52744, 39539, 38946, 39539, 46193, 34067, 45202, 36234, 43922, 40598, 46192, 41403, 355, 47680, 34791, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(pro_tok), pro_tok.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrBRILZkD9wC",
        "outputId": "324de7f1-b76b-4d21-ffd1-65d50ba1b352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(transformers.tokenization_utils_base.BatchEncoding,\n",
              " dict_keys(['input_ids', 'attention_mask']))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## list return\n",
        "decoder_input_ids = processor.tokenizer(prompt,max_length=100,padding=\"max_length\").input_ids\n",
        "print(decoder_input_ids)\n",
        "print(processor.decode(decoder_input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0HUACsvy4Li",
        "outputId": "3942306c-9c4a-4916-d19f-1c669a7a9a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 41040, 46192, 41403, 49706, 8888, 34791, 43703, 52743, 48941, 55856, 2587, 48941, 37517, 25515, 48941, 41095, 36209, 40598, 46192, 41403, 49706, 8888, 34791, 57525, 52744, 39539, 38946, 39539, 46193, 34067, 45202, 36234, 43922, 40598, 46192, 41403, 355, 47680, 34791, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "<s> <s_question>What is the name of the person in the field?</s_question><s_answer> J. T. WINEBRENNER</s_answer></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## tensor return\n",
        "decoder_input_ids_ten = processor.tokenizer(prompt, return_tensors=\"pt\",max_length=100,padding=\"max_length\").input_ids.squeeze(0)\n",
        "print(decoder_input_ids_ten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYprFFPKl5Fj",
        "outputId": "23898e51-5fda-4410-d6a8-cb8d5032c542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    0, 41040, 46192, 41403, 49706,  8888, 34791, 43703, 52743, 48941,\n",
            "        55856,  2587, 48941, 37517, 25515, 48941, 41095, 36209, 40598, 46192,\n",
            "        41403, 49706,  8888, 34791, 57525, 52744, 39539, 38946, 39539, 46193,\n",
            "        34067, 45202, 36234, 43922, 40598, 46192, 41403,   355, 47680, 34791,\n",
            "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tensor Clone for label\n",
        "decoder_input_ids_ten_clone = decoder_input_ids_ten.clone()\n",
        "print(decoder_input_ids_ten_clone)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNMPj0wAtxvA",
        "outputId": "2e294f2a-cb5e-4a1e-a99f-c6ab27832036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    0, 41040, 46192, 41403, 49706,  8888, 34791, 43703, 52743, 48941,\n",
            "        55856,  2587, 48941, 37517, 25515, 48941, 41095, 36209, 40598, 46192,\n",
            "        41403, 49706,  8888, 34791, 57525, 52744, 39539, 38946, 39539, 46193,\n",
            "        34067, 45202, 36234, 43922, 40598, 46192, 41403,   355, 47680, 34791,\n",
            "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.tokenizer.pad_token)\n",
        "print(processor.tokenizer.pad_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0qC0kT7lqsj",
        "outputId": "ba2c741d-f0ed-4608-a8b5-f42151e264a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad>\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ignore_id = -100\n",
        "decoder_input_ids_ten_clone[decoder_input_ids_ten_clone == processor.tokenizer.pad_token_id] = ignore_id\n",
        "print(decoder_input_ids_ten_clone)\n",
        "print('***********************************************************************')\n",
        "decoder_input_ids_ten_clone[ : torch.nonzero(decoder_input_ids_ten_clone == torch.tensor(prompt_end_token_id)).sum() + 1] = ignore_id\n",
        "print(decoder_input_ids_ten_clone)"
      ],
      "metadata": {
        "id": "WJRQKcPNc87P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a135c13d-704c-4e67-c733-8d7af54424bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    0, 41040, 46192, 41403, 49706,  8888, 34791, 43703, 52743, 48941,\n",
            "        55856,  2587, 48941, 37517, 25515, 48941, 41095, 36209, 40598, 46192,\n",
            "        41403, 49706,  8888, 34791, 57525, 52744, 39539, 38946, 39539, 46193,\n",
            "        34067, 45202, 36234, 43922, 40598, 46192, 41403,   355, 47680, 34791,\n",
            "            2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n",
            "***********************************************************************\n",
            "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100, 52744, 39539, 38946, 39539, 46193,\n",
            "        34067, 45202, 36234, 43922, 40598, 46192, 41403,   355, 47680, 34791,\n",
            "            2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detail experiment of huggingface datasets class"
      ],
      "metadata": {
        "id": "x4VXkpMJLAmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "8a6Q28JYEqeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(datasets))"
      ],
      "metadata": {
        "id": "01IiC5dhEqcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9023a6c6-439d-40c5-dd0d-cc95a42716d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Array2D', 'Array3D', 'Array4D', 'Array5D', 'ArrowBasedBuilder', 'Audio', 'AudioClassification', 'AutomaticSpeechRecognition', 'BeamBasedBuilder', 'BuilderConfig', 'ClassLabel', 'Dataset', 'DatasetBuilder', 'DatasetDict', 'DatasetInfo', 'DownloadConfig', 'DownloadManager', 'DownloadMode', 'Features', 'GeneratorBasedBuilder', 'Image', 'ImageClassification', 'IterableDataset', 'IterableDatasetDict', 'LanguageModeling', 'Metric', 'MetricInfo', 'NamedSplit', 'NamedSplitAll', 'QuestionAnsweringExtractive', 'ReadInstruction', 'Sequence', 'Split', 'SplitBase', 'SplitDict', 'SplitGenerator', 'SplitInfo', 'StreamingDownloadManager', 'SubSplitInfo', 'Summarization', 'TaskTemplate', 'TextClassification', 'Translation', 'TranslationVariableLanguages', 'Value', 'VerificationMode', 'Version', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'arrow_dataset', 'arrow_reader', 'arrow_writer', 'builder', 'combine', 'concatenate_datasets', 'config', 'data_files', 'dataset_dict', 'disable_caching', 'disable_progress_bar', 'download', 'enable_caching', 'enable_progress_bar', 'experimental', 'features', 'filesystems', 'fingerprint', 'formatting', 'get_dataset_config_info', 'get_dataset_config_names', 'get_dataset_infos', 'get_dataset_split_names', 'info', 'inspect', 'inspect_dataset', 'inspect_metric', 'interleave_datasets', 'is_caching_enabled', 'is_progress_bar_enabled', 'iterable_dataset', 'keyhash', 'list_datasets', 'list_metrics', 'load', 'load_dataset', 'load_dataset_builder', 'load_from_disk', 'load_metric', 'logging', 'metric', 'naming', 'packaged_modules', 'parallel', 'percent', 'search', 'set_caching_enabled', 'splits', 'streaming', 'table', 'tasks', 'utils']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(datasets.Dataset))"
      ],
      "metadata": {
        "id": "4Q9gEA3dEqZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76877d4e-a132-4879-cd8c-1f2204e3dbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['_TF_DATASET_REFS', '__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getitems__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_build_local_temp_path', '_check_index_is_initialized', '_estimate_nbytes', '_generate_tables_from_cache_file', '_generate_tables_from_shards', '_get_cache_file_path', '_get_output_signature', '_getitem', '_map_single', '_new_dataset_with_indices', '_push_parquet_shards_to_hub', '_save_to_disk_single', '_select_contiguous', '_select_with_indices_mapping', 'add_column', 'add_elasticsearch_index', 'add_faiss_index', 'add_faiss_index_from_external_arrays', 'add_item', 'align_labels_with_mapping', 'builder_name', 'cache_files', 'cast', 'cast_column', 'citation', 'class_encode_column', 'cleanup_cache_files', 'column_names', 'config_name', 'data', 'dataset_size', 'description', 'download_checksums', 'download_size', 'drop_index', 'export', 'features', 'filter', 'flatten', 'flatten_indices', 'format', 'formatted_as', 'from_buffer', 'from_csv', 'from_dict', 'from_file', 'from_generator', 'from_json', 'from_list', 'from_pandas', 'from_parquet', 'from_spark', 'from_sql', 'from_text', 'get_index', 'get_nearest_examples', 'get_nearest_examples_batch', 'homepage', 'info', 'is_index_initialized', 'iter', 'license', 'list_indexes', 'load_elasticsearch_index', 'load_faiss_index', 'load_from_disk', 'map', 'num_columns', 'num_rows', 'prepare_for_task', 'push_to_hub', 'remove_columns', 'rename_column', 'rename_columns', 'reset_format', 'save_faiss_index', 'save_to_disk', 'search', 'search_batch', 'select', 'select_columns', 'set_format', 'set_transform', 'shape', 'shard', 'shuffle', 'size_in_bytes', 'sort', 'split', 'supervised_keys', 'task_templates', 'to_csv', 'to_dict', 'to_iterable_dataset', 'to_json', 'to_list', 'to_pandas', 'to_parquet', 'to_sql', 'to_tf_dataset', 'train_test_split', 'unique', 'version', 'with_format', 'with_transform']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note: 'Dataset'\n",
        "- Dataset is a class that represents a dataset object in the Hugging Face library. It contains the data and associated metadata, such as the dataset name, size, feature names, and description"
      ],
      "metadata": {
        "id": "W2ptOYOWRvNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hfdataset_sub"
      ],
      "metadata": {
        "id": "5KE0DXcwN9Y0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2857f87e-5e8a-4a77-cc90-6d063942302f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['questionId', 'question', 'image', 'docId', 'ucsf_document_id', 'ucsf_document_page_no', 'answers', 'data_split', 'full_path_image'],\n",
              "    num_rows: 200\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(datasets.load_dataset))"
      ],
      "metadata": {
        "id": "54YgWLNGEqXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742ef3b5-d2a8-4bb0-e43a-120babbea0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__annotations__', '__builtins__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note: 'load_dataset'\n",
        "- A utility function is a small, specialized function that is designed to perform a specific task or solve a particular problem\n",
        "- Utility functions are often created to simplify code and make it more readable, maintainable, and reusable\n",
        "- 'load_dataset' is a utility function that allows you to download and load a pre-built dataset from the Hugging Face library.\n",
        "- *** It returns a DatasetDict object that contains one or more Dataset objects representing the training, validation, and test splits of the dataset, along with other metadata."
      ],
      "metadata": {
        "id": "tjVCNo1LUETv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q. Difference between HF 'Dataset' and 'load_dataset' also when to use which?\n",
        "- Ans_1 : Main difference between 'datasets.Dataset' and 'datasets.load_dataset' is that 'datasets.Dataset' is used to create a dataset object from a custom data source, while 'datasets.load_dataset' is used to load a pre-built dataset from the Hugging Face library\n",
        "- Ans_2 : You should use datasets.Dataset when you have your own data and want to create a Dataset object to work with in the Hugging Face library. On the other hand, you should use datasets.load_dataset when you want to use a pre-built dataset from the Hugging Face library for your NLP task. This allows you to save time and avoid the hassle of manually downloading and preprocessing the data yourself."
      ],
      "metadata": {
        "id": "evz6pO_eX-oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(hfdataset_sub)"
      ],
      "metadata": {
        "id": "EyVuGwN_eueR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4179c29-bbbf-4b28-8fb3-f599b18b942f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hfdataset_sub.features"
      ],
      "metadata": {
        "id": "7EEL3wQoe_R5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc0a170-7282-4606-b65e-364b63c1132d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'questionId': Value(dtype='int64', id=None),\n",
              " 'question': Value(dtype='string', id=None),\n",
              " 'image': Value(dtype='string', id=None),\n",
              " 'docId': Value(dtype='int64', id=None),\n",
              " 'ucsf_document_id': Value(dtype='string', id=None),\n",
              " 'ucsf_document_page_no': Value(dtype='string', id=None),\n",
              " 'answers': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
              " 'data_split': Value(dtype='string', id=None),\n",
              " 'full_path_image': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hfdataset_sub.num_rows"
      ],
      "metadata": {
        "id": "o9fOTH9AfKr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88c14d0-591d-4764-eba5-d1e8373fef21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = hfdataset_sub[0]\n",
        "example"
      ],
      "metadata": {
        "id": "Jc4RDdUjeYdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1527f091-7812-4240-941b-2f2d4f479fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'questionId': 337,\n",
              " 'question': 'what is the date mentioned in this letter?',\n",
              " 'image': 'documents/xnbl0037_1.png',\n",
              " 'docId': 279,\n",
              " 'ucsf_document_id': 'xnbl0037',\n",
              " 'ucsf_document_page_no': '1',\n",
              " 'answers': ['1/8/93'],\n",
              " 'data_split': 'train',\n",
              " 'full_path_image': '/content/drive/MyDrive/docVqa_dataset/train/documents/xnbl0037_1.png'}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(example)"
      ],
      "metadata": {
        "id": "5fBlYwNgfjig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0ef769-bee9-4e36-cf64-c64e39451b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example.keys()"
      ],
      "metadata": {
        "id": "ehZGXeaZgCln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6721a8c-51e6-4cd1-f232-116a7d17b27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['questionId', 'question', 'image', 'docId', 'ucsf_document_id', 'ucsf_document_page_no', 'answers', 'data_split', 'full_path_image'])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image_path = example['full_path_image']\n",
        "sample_image_path"
      ],
      "metadata": {
        "id": "DCESzL0IhMbH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cb84ddc7-e3bd-4c2d-842f-427301093160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/docVqa_dataset/train/documents/xnbl0037_1.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## viewing sample example image from hfdataset object\n",
        "sam_image = Image.open(sample_image_path)\n",
        "sam_image"
      ],
      "metadata": {
        "id": "sNw3xKdrEqSe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f95b7e8-695f-49e8-b4a1-1ab20401412f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=1695x2025>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(sam_image)"
      ],
      "metadata": {
        "id": "xXfuJgBDCBUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39f9c3f-bb7c-48a1-8765-522789eff6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PIL.PngImagePlugin.PngImageFile"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the PIL image to a NumPy array\n",
        "sam_np_array = np.array(sam_image)\n",
        "\n",
        "# Print the shape of the NumPy array\n",
        "print(sam_np_array.shape)"
      ],
      "metadata": {
        "id": "8e9MoU3bEbHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641b369e-3a1a-4961-ddc2-6f00d88e65d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2025, 1695)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Corresponding Question in this sample example\n",
        "example['question']"
      ],
      "metadata": {
        "id": "uHh_FbB6EqQI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b011b023-5169-4902-ffa4-e1fe2be4ca72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what is the date mentioned in this letter?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Corresponding Answer in this sample example\n",
        "example['answers']"
      ],
      "metadata": {
        "id": "U_71RT--EqKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18929f3-5767-4c5c-e3c5-2aa805b6a4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1/8/93']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add ground_truth for Donut:\n",
        "Donut requires you to add a column with some ground truth JSON/txt/JSON lines/whatever that you'd like the model to learn to generate. Below, we define a function that we'll apply on the entire dataset to add this column."
      ],
      "metadata": {
        "id": "PDBtCC0c7jjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "nOQIBtRZOISM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images = sample_image\n",
        "sam_question = example['question']\n",
        "sam_answers = example['answers']"
      ],
      "metadata": {
        "id": "gku0jDDBEqFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam_ground_truths = []"
      ],
      "metadata": {
        "id": "pwG9yKf0EqCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam_question = sam_question.replace('\\\\\"', \"\")\n",
        "sam_question = re.sub(' +', ' ', sam_question)\n",
        "sam_question = sam_question.replace('\"', '\\\"')"
      ],
      "metadata": {
        "id": "7aYFPwghEp9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam_question, type(sam_question)"
      ],
      "metadata": {
        "id": "65bBe4RlOWtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb55a7e7-ae85-4acd-d460-175e9484dd68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('what is the date mentioned in this letter?', str)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sam_answers, len(sam_answers)"
      ],
      "metadata": {
        "id": "VG3ptVJiSGkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ecdeca-8f45-44a9-9103-24aa2eb3600b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['1/8/93'], 1)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sam_ground_truth_example = '{\"gt_parses\": ['\n",
        "sam_ground_truth_example, type(sam_ground_truth_example)"
      ],
      "metadata": {
        "id": "sAVPWiCoEp6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d61566-f9fd-480b-dd6c-42bb1919c09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('{\"gt_parses\": [', str)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, answer in enumerate(sam_answers):\n",
        "  print(f\"Respective Index and Answer is : {idx} : {answer}\")\n",
        "  ans = answer.replace('\"', '\\\"')\n",
        "  print(f'Answer is : {ans}')\n",
        "  sam_ground_truth_example += '{\"question\" : \"' + sam_question + '\", \"answer\" : \"' + ans + '\"}'\n",
        "  print(sam_ground_truth_example)\n",
        "  print('---------*****-------------')\n",
        "sam_ground_truth_example += ']}'\n",
        "print(sam_ground_truth_example)\n",
        "print('----------------------')\n",
        "sam_ground_truths.append(sam_ground_truth_example)\n",
        "print(sam_ground_truths)"
      ],
      "metadata": {
        "id": "ChpF3ve5Ep38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e104fbf9-87de-4747-f9db-d1bd081d7bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respective Index and Answer is : 0 : 1/8/93\n",
            "Answer is : 1/8/93\n",
            "{\"gt_parses\": [{\"question\" : \"what is the date mentioned in this letter?\", \"answer\" : \"1/8/93\"}\n",
            "---------*****-------------\n",
            "{\"gt_parses\": [{\"question\" : \"what is the date mentioned in this letter?\", \"answer\" : \"1/8/93\"}]}\n",
            "----------------------\n",
            "['{\"gt_parses\": [{\"question\" : \"what is the date mentioned in this letter?\", \"answer\" : \"1/8/93\"}]}']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing gt_parse for whole dataset"
      ],
      "metadata": {
        "id": "2kjO6g4lsdjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating custom function for DOCVQA task\n",
        "def add_ground_truth(hf_examples):\n",
        "  images = hf_examples['full_path_image']\n",
        "  questions = hf_examples['question']\n",
        "  answers = hf_examples['answers']\n",
        "\n",
        "  ground_truths = []    # Creating empty list\n",
        "  for image,question, answer in zip(images,questions, answers):\n",
        "    # we need to escape \" characters appearing in the query and/or answer\n",
        "    question = question.replace(\"\\\\\", \"\") ## this was just one corrupt example (index 91 of training set)\n",
        "    question = re.sub(' +', ' ', question)\n",
        "    question = question.replace('\"', '\\\\\"')   # replacement of \" from question\n",
        "    # let's create the ground truth string\n",
        "    ground_truth_example = '{\"gt_parses\": ['\n",
        "    for idx, answ in enumerate(answer):\n",
        "      # ans = answ.replace('\"', '\\\"')   # replacement of \" from answer\n",
        "      answ = answ.replace(\"\\\\\", \"\")\n",
        "      ans = answ.replace('\"', '\\\\\"')\n",
        "      ground_truth_example += '{\"question\" : \"' + question + '\", \"answer\" : \"' + ans + '\"}'\n",
        "\n",
        "      # add comma for more than one element present in the answer list\n",
        "      if idx != len(answer) - 1:   # when current index is not equal to last index\n",
        "        ground_truth_example += ', '\n",
        "    ground_truth_example += ']}'\n",
        "    ground_truths.append(ground_truth_example)    ## appending ground_truths list for every row\n",
        "\n",
        "  hf_examples['ground_truth'] = ground_truths\n",
        "\n",
        "  return hf_examples"
      ],
      "metadata": {
        "id": "x4PgA0CPsdCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hfdataset_sub_gt = hfdataset_sub.map(add_ground_truth, batched=True)"
      ],
      "metadata": {
        "id": "racJup8Lsc_u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b59886a7bbd44b31aab08558b3d68a39",
            "e173f8a809504c38941bd99de8fe9dff",
            "a95eddf1d1d743a993ad546e7fc91201",
            "38adf038c15443c7b2b6bfbf7c0a1e7f",
            "9dc559b1a9984a2fb2d4210835ceb84f",
            "4bafea91fb0f486aaf36acc449ea9bbf",
            "fe6be0e8e2e44128845849e25a65736a",
            "4b64b5f661f54e75aa2c3309f90ddcb6",
            "6c7351e30d9844cf864f6d3525d1302b",
            "e4f070e43f514dbe8482d0f79e280fdf",
            "1299a5cda6294361a273fa5f8a2f4b8e"
          ]
        },
        "outputId": "07037add-13dc-491c-ae8e-78cb3a0def65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b59886a7bbd44b31aab08558b3d68a39"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hfdataset_sub_gt"
      ],
      "metadata": {
        "id": "UKv0YdXfsc61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53414a95-f5b5-420e-b7cc-cee7cd80a7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['questionId', 'question', 'image', 'docId', 'ucsf_document_id', 'ucsf_document_page_no', 'answers', 'data_split', 'full_path_image', 'ground_truth'],\n",
              "    num_rows: 200\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking whether GT_Parse working fine or not.\n",
        "- Checking the whole with three example"
      ],
      "metadata": {
        "id": "mphMrJIIJ9B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gtCheck(indx):    # Put the index value\n",
        "\n",
        "  example_indx = hfdataset_sub_gt[indx]\n",
        "  example_indx_gt_dict = json.loads(example_indx['ground_truth'])\n",
        "  return example_indx_gt_dict"
      ],
      "metadata": {
        "id": "DJwcWz5hKgyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub.iloc[135]['answers']"
      ],
      "metadata": {
        "id": "PCR0mBm6J5ei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbba688-3de6-4602-b97c-09a77bcb1d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Atlanta, georgia', 'Atlanta, Georgia']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## sample example 135\n",
        "example_135_gt_dict = gtCheck(135)\n",
        "example_135_gt_dict"
      ],
      "metadata": {
        "id": "ymApAdVWL-xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abca353-2a64-4051-ece6-2ac13d64de4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gt_parses': [{'question': 'Where is One Coca-Cola Plaza located?',\n",
              "   'answer': 'Atlanta, georgia'},\n",
              "  {'question': 'Where is One Coca-Cola Plaza located?',\n",
              "   'answer': 'Atlanta, Georgia'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_135 = hfdataset_sub_gt[135]\n",
        "example_135\n",
        "example_135['ground_truth']\n",
        "type(example_135['ground_truth'])\n",
        "\n",
        "example_135_gt_dict = json.loads(example_135['ground_truth'])   ## Convert it from str to json dict\n",
        "example_135_gt_dict\n",
        "\n",
        "example_135_gt_dict"
      ],
      "metadata": {
        "id": "QCzQz3JXsc3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79bc8047-d89b-4b1e-85e9-8d4b563004bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gt_parses': [{'question': 'Where is One Coca-Cola Plaza located?',\n",
              "   'answer': 'Atlanta, georgia'},\n",
              "  {'question': 'Where is One Coca-Cola Plaza located?',\n",
              "   'answer': 'Atlanta, Georgia'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample example 10\n",
        "example_10_gt_dict = gtCheck(10)\n",
        "example_10_gt_dict"
      ],
      "metadata": {
        "id": "9bP7Agr-Ep1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa51b1e-2b48-4aa9-fb9b-43be4465b536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gt_parses': [{'question': 'Which part of Virginia is this letter sent from',\n",
              "   'answer': 'Richmond'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample example 100\n",
        "example_100_gt_dict = gtCheck(100)\n",
        "example_100_gt_dict"
      ],
      "metadata": {
        "id": "v8kVuS5JEpwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d02374-60e8-4c77-ea0c-0bfe0016c332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gt_parses': [{'question': 'To whom is this letter addressed?',\n",
              "   'answer': 'R.Ferguson'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution and finding details of VisionEncoderDecoder Model and config"
      ],
      "metadata": {
        "id": "D_BSrubiYnhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderConfig, VisionEncoderDecoderModel, BartConfig"
      ],
      "metadata": {
        "id": "hmboBDNwYnRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(transformers))"
      ],
      "metadata": {
        "id": "L-WboObpYnK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ac7b0b-b072-4919-b9d0-883a0216d70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ALIGN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALIGN_PRETRAINED_MODEL_ARCHIVE_LIST', 'ALL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALTCLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ALTCLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'ASTConfig', 'ASTFeatureExtractor', 'ASTForAudioClassification', 'ASTModel', 'ASTPreTrainedModel', 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'AUDIO_SPECTROGRAM_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'AUTOFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'AUTOFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'Adafactor', 'AdamW', 'AdamWeightDecay', 'AdaptiveEmbedding', 'AddedToken', 'Agent', 'AlbertConfig', 'AlbertForMaskedLM', 'AlbertForMultipleChoice', 'AlbertForPreTraining', 'AlbertForQuestionAnswering', 'AlbertForSequenceClassification', 'AlbertForTokenClassification', 'AlbertModel', 'AlbertPreTrainedModel', 'AlbertTokenizer', 'AlbertTokenizerFast', 'AlignConfig', 'AlignModel', 'AlignPreTrainedModel', 'AlignProcessor', 'AlignTextConfig', 'AlignTextModel', 'AlignVisionConfig', 'AlignVisionModel', 'AltCLIPConfig', 'AltCLIPModel', 'AltCLIPPreTrainedModel', 'AltCLIPProcessor', 'AltCLIPTextConfig', 'AltCLIPTextModel', 'AltCLIPVisionConfig', 'AltCLIPVisionModel', 'AudioClassificationPipeline', 'AutoBackbone', 'AutoConfig', 'AutoFeatureExtractor', 'AutoImageProcessor', 'AutoModel', 'AutoModelForAudioClassification', 'AutoModelForAudioFrameClassification', 'AutoModelForAudioXVector', 'AutoModelForCTC', 'AutoModelForCausalLM', 'AutoModelForDepthEstimation', 'AutoModelForDocumentQuestionAnswering', 'AutoModelForImageClassification', 'AutoModelForImageSegmentation', 'AutoModelForInstanceSegmentation', 'AutoModelForMaskGeneration', 'AutoModelForMaskedImageModeling', 'AutoModelForMaskedLM', 'AutoModelForMultipleChoice', 'AutoModelForNextSentencePrediction', 'AutoModelForObjectDetection', 'AutoModelForPreTraining', 'AutoModelForQuestionAnswering', 'AutoModelForSemanticSegmentation', 'AutoModelForSeq2SeqLM', 'AutoModelForSequenceClassification', 'AutoModelForSpeechSeq2Seq', 'AutoModelForTableQuestionAnswering', 'AutoModelForTextEncoding', 'AutoModelForTokenClassification', 'AutoModelForUniversalSegmentation', 'AutoModelForVideoClassification', 'AutoModelForVision2Seq', 'AutoModelForVisualQuestionAnswering', 'AutoModelForZeroShotImageClassification', 'AutoModelForZeroShotObjectDetection', 'AutoModelWithLMHead', 'AutoProcessor', 'AutoTokenizer', 'AutoformerConfig', 'AutoformerForPrediction', 'AutoformerModel', 'AutoformerPreTrainedModel', 'AutomaticSpeechRecognitionPipeline', 'AzureOpenAiAgent', 'BARK_PRETRAINED_MODEL_ARCHIVE_LIST', 'BART_PRETRAINED_MODEL_ARCHIVE_LIST', 'BEIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIGBIRD_PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIGBIRD_PEGASUS_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIG_BIRD_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIG_BIRD_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIOGPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIOGPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLENDERBOT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLENDERBOT_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLENDERBOT_SMALL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLENDERBOT_SMALL_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLIP_2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLIP_2_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'BLOOM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BLOOM_PRETRAINED_MODEL_ARCHIVE_LIST', 'BRIDGETOWER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'BRIDGETOWER_PRETRAINED_MODEL_ARCHIVE_LIST', 'BarkCausalModel', 'BarkCoarseConfig', 'BarkCoarseModel', 'BarkConfig', 'BarkFineConfig', 'BarkFineModel', 'BarkModel', 'BarkPreTrainedModel', 'BarkProcessor', 'BarkSemanticConfig', 'BarkSemanticModel', 'BartConfig', 'BartForCausalLM', 'BartForConditionalGeneration', 'BartForQuestionAnswering', 'BartForSequenceClassification', 'BartModel', 'BartPretrainedModel', 'BartTokenizer', 'BartTokenizerFast', 'BarthezTokenizer', 'BarthezTokenizerFast', 'BartphoTokenizer', 'BasicTokenizer', 'BatchEncoding', 'BatchFeature', 'BeamScorer', 'BeamSearchScorer', 'BeitConfig', 'BeitFeatureExtractor', 'BeitForImageClassification', 'BeitForMaskedImageModeling', 'BeitForSemanticSegmentation', 'BeitImageProcessor', 'BeitModel', 'BeitPreTrainedModel', 'BertConfig', 'BertForMaskedLM', 'BertForMultipleChoice', 'BertForNextSentencePrediction', 'BertForPreTraining', 'BertForQuestionAnswering', 'BertForSequenceClassification', 'BertForTokenClassification', 'BertGenerationConfig', 'BertGenerationDecoder', 'BertGenerationEncoder', 'BertGenerationPreTrainedModel', 'BertGenerationTokenizer', 'BertJapaneseTokenizer', 'BertLMHeadModel', 'BertLayer', 'BertModel', 'BertPreTrainedModel', 'BertTokenizer', 'BertTokenizerFast', 'BertweetTokenizer', 'BigBirdConfig', 'BigBirdForCausalLM', 'BigBirdForMaskedLM', 'BigBirdForMultipleChoice', 'BigBirdForPreTraining', 'BigBirdForQuestionAnswering', 'BigBirdForSequenceClassification', 'BigBirdForTokenClassification', 'BigBirdLayer', 'BigBirdModel', 'BigBirdPegasusConfig', 'BigBirdPegasusForCausalLM', 'BigBirdPegasusForConditionalGeneration', 'BigBirdPegasusForQuestionAnswering', 'BigBirdPegasusForSequenceClassification', 'BigBirdPegasusModel', 'BigBirdPegasusPreTrainedModel', 'BigBirdPreTrainedModel', 'BigBirdTokenizer', 'BigBirdTokenizerFast', 'BioGptConfig', 'BioGptForCausalLM', 'BioGptForSequenceClassification', 'BioGptForTokenClassification', 'BioGptModel', 'BioGptPreTrainedModel', 'BioGptTokenizer', 'BitBackbone', 'BitConfig', 'BitForImageClassification', 'BitImageProcessor', 'BitModel', 'BitPreTrainedModel', 'BitsAndBytesConfig', 'BlenderbotConfig', 'BlenderbotForCausalLM', 'BlenderbotForConditionalGeneration', 'BlenderbotModel', 'BlenderbotPreTrainedModel', 'BlenderbotSmallConfig', 'BlenderbotSmallForCausalLM', 'BlenderbotSmallForConditionalGeneration', 'BlenderbotSmallModel', 'BlenderbotSmallPreTrainedModel', 'BlenderbotSmallTokenizer', 'BlenderbotSmallTokenizerFast', 'BlenderbotTokenizer', 'BlenderbotTokenizerFast', 'Blip2Config', 'Blip2ForConditionalGeneration', 'Blip2Model', 'Blip2PreTrainedModel', 'Blip2Processor', 'Blip2QFormerConfig', 'Blip2QFormerModel', 'Blip2VisionConfig', 'Blip2VisionModel', 'BlipConfig', 'BlipForConditionalGeneration', 'BlipForImageTextRetrieval', 'BlipForQuestionAnswering', 'BlipImageProcessor', 'BlipModel', 'BlipPreTrainedModel', 'BlipProcessor', 'BlipTextConfig', 'BlipTextModel', 'BlipVisionConfig', 'BlipVisionModel', 'BloomConfig', 'BloomForCausalLM', 'BloomForQuestionAnswering', 'BloomForSequenceClassification', 'BloomForTokenClassification', 'BloomModel', 'BloomPreTrainedModel', 'BloomTokenizerFast', 'BridgeTowerConfig', 'BridgeTowerForContrastiveLearning', 'BridgeTowerForImageAndTextRetrieval', 'BridgeTowerForMaskedLM', 'BridgeTowerImageProcessor', 'BridgeTowerModel', 'BridgeTowerPreTrainedModel', 'BridgeTowerProcessor', 'BridgeTowerTextConfig', 'BridgeTowerVisionConfig', 'ByT5Tokenizer', 'CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CANINE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CANINE_PRETRAINED_MODEL_ARCHIVE_LIST', 'CHINESE_CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CHINESE_CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'CLAP_PRETRAINED_MODEL_ARCHIVE_LIST', 'CLIPConfig', 'CLIPFeatureExtractor', 'CLIPImageProcessor', 'CLIPModel', 'CLIPPreTrainedModel', 'CLIPProcessor', 'CLIPSEG_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CLIPSEG_PRETRAINED_MODEL_ARCHIVE_LIST', 'CLIPSegConfig', 'CLIPSegForImageSegmentation', 'CLIPSegModel', 'CLIPSegPreTrainedModel', 'CLIPSegProcessor', 'CLIPSegTextConfig', 'CLIPSegTextModel', 'CLIPSegVisionConfig', 'CLIPSegVisionModel', 'CLIPTextConfig', 'CLIPTextModel', 'CLIPTextModelWithProjection', 'CLIPTokenizer', 'CLIPTokenizerFast', 'CLIPVisionConfig', 'CLIPVisionModel', 'CLIPVisionModelWithProjection', 'CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'CODEGEN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONDITIONAL_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONDITIONAL_DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONFIG_MAPPING', 'CONFIG_NAME', 'CONVBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONVBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONVNEXTV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONVNEXTV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'CONVNEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CONVNEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CPMANT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CPMANT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CTRLConfig', 'CTRLForSequenceClassification', 'CTRLLMHeadModel', 'CTRLModel', 'CTRLPreTrainedModel', 'CTRLTokenizer', 'CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CTRL_PRETRAINED_MODEL_ARCHIVE_LIST', 'CVT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'CVT_PRETRAINED_MODEL_ARCHIVE_LIST', 'CamembertConfig', 'CamembertForCausalLM', 'CamembertForMaskedLM', 'CamembertForMultipleChoice', 'CamembertForQuestionAnswering', 'CamembertForSequenceClassification', 'CamembertForTokenClassification', 'CamembertModel', 'CamembertPreTrainedModel', 'CamembertTokenizer', 'CamembertTokenizerFast', 'CanineConfig', 'CanineForMultipleChoice', 'CanineForQuestionAnswering', 'CanineForSequenceClassification', 'CanineForTokenClassification', 'CanineLayer', 'CanineModel', 'CaninePreTrainedModel', 'CanineTokenizer', 'CharSpan', 'CharacterTokenizer', 'ChineseCLIPConfig', 'ChineseCLIPFeatureExtractor', 'ChineseCLIPImageProcessor', 'ChineseCLIPModel', 'ChineseCLIPPreTrainedModel', 'ChineseCLIPProcessor', 'ChineseCLIPTextConfig', 'ChineseCLIPTextModel', 'ChineseCLIPVisionConfig', 'ChineseCLIPVisionModel', 'ClapAudioConfig', 'ClapAudioModel', 'ClapAudioModelWithProjection', 'ClapConfig', 'ClapFeatureExtractor', 'ClapModel', 'ClapPreTrainedModel', 'ClapProcessor', 'ClapTextConfig', 'ClapTextModel', 'ClapTextModelWithProjection', 'CodeGenConfig', 'CodeGenForCausalLM', 'CodeGenModel', 'CodeGenPreTrainedModel', 'CodeGenTokenizer', 'CodeGenTokenizerFast', 'ConditionalDetrConfig', 'ConditionalDetrFeatureExtractor', 'ConditionalDetrForObjectDetection', 'ConditionalDetrForSegmentation', 'ConditionalDetrImageProcessor', 'ConditionalDetrModel', 'ConditionalDetrPreTrainedModel', 'ConstrainedBeamSearchScorer', 'Constraint', 'ConstraintListState', 'Conv1D', 'ConvBertConfig', 'ConvBertForMaskedLM', 'ConvBertForMultipleChoice', 'ConvBertForQuestionAnswering', 'ConvBertForSequenceClassification', 'ConvBertForTokenClassification', 'ConvBertLayer', 'ConvBertModel', 'ConvBertPreTrainedModel', 'ConvBertTokenizer', 'ConvBertTokenizerFast', 'ConvNextBackbone', 'ConvNextConfig', 'ConvNextFeatureExtractor', 'ConvNextForImageClassification', 'ConvNextImageProcessor', 'ConvNextModel', 'ConvNextPreTrainedModel', 'ConvNextV2Backbone', 'ConvNextV2Config', 'ConvNextV2ForImageClassification', 'ConvNextV2Model', 'ConvNextV2PreTrainedModel', 'Conversation', 'ConversationalPipeline', 'CpmAntConfig', 'CpmAntForCausalLM', 'CpmAntModel', 'CpmAntPreTrainedModel', 'CpmAntTokenizer', 'CpmTokenizer', 'CpmTokenizerFast', 'CsvPipelineDataFormat', 'CvtConfig', 'CvtForImageClassification', 'CvtModel', 'CvtPreTrainedModel', 'DATA2VEC_AUDIO_PRETRAINED_MODEL_ARCHIVE_LIST', 'DATA2VEC_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DATA2VEC_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DATA2VEC_VISION_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DATA2VEC_VISION_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEBERTA_V2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'DECISION_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DECISION_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEFORMABLE_DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEFORMABLE_DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'DEIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DETA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DETA_PRETRAINED_MODEL_ARCHIVE_LIST', 'DETR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DETR_PRETRAINED_MODEL_ARCHIVE_LIST', 'DINAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DINAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'DONUT_SWIN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DONUT_SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPRConfig', 'DPRContextEncoder', 'DPRContextEncoderTokenizer', 'DPRContextEncoderTokenizerFast', 'DPRPreTrainedModel', 'DPRPretrainedContextEncoder', 'DPRPretrainedQuestionEncoder', 'DPRPretrainedReader', 'DPRQuestionEncoder', 'DPRQuestionEncoderTokenizer', 'DPRQuestionEncoderTokenizerFast', 'DPRReader', 'DPRReaderOutput', 'DPRReaderTokenizer', 'DPRReaderTokenizerFast', 'DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST', 'DPTConfig', 'DPTFeatureExtractor', 'DPTForDepthEstimation', 'DPTForSemanticSegmentation', 'DPTImageProcessor', 'DPTModel', 'DPTPreTrainedModel', 'DPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'DPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'Data2VecAudioConfig', 'Data2VecAudioForAudioFrameClassification', 'Data2VecAudioForCTC', 'Data2VecAudioForSequenceClassification', 'Data2VecAudioForXVector', 'Data2VecAudioModel', 'Data2VecAudioPreTrainedModel', 'Data2VecTextConfig', 'Data2VecTextForCausalLM', 'Data2VecTextForMaskedLM', 'Data2VecTextForMultipleChoice', 'Data2VecTextForQuestionAnswering', 'Data2VecTextForSequenceClassification', 'Data2VecTextForTokenClassification', 'Data2VecTextModel', 'Data2VecTextPreTrainedModel', 'Data2VecVisionConfig', 'Data2VecVisionForImageClassification', 'Data2VecVisionForSemanticSegmentation', 'Data2VecVisionModel', 'Data2VecVisionPreTrainedModel', 'DataCollator', 'DataCollatorForLanguageModeling', 'DataCollatorForPermutationLanguageModeling', 'DataCollatorForSOP', 'DataCollatorForSeq2Seq', 'DataCollatorForTokenClassification', 'DataCollatorForWholeWordMask', 'DataCollatorWithPadding', 'DataProcessor', 'DebertaConfig', 'DebertaForMaskedLM', 'DebertaForQuestionAnswering', 'DebertaForSequenceClassification', 'DebertaForTokenClassification', 'DebertaModel', 'DebertaPreTrainedModel', 'DebertaTokenizer', 'DebertaTokenizerFast', 'DebertaV2Config', 'DebertaV2ForMaskedLM', 'DebertaV2ForMultipleChoice', 'DebertaV2ForQuestionAnswering', 'DebertaV2ForSequenceClassification', 'DebertaV2ForTokenClassification', 'DebertaV2Model', 'DebertaV2PreTrainedModel', 'DebertaV2Tokenizer', 'DebertaV2TokenizerFast', 'DecisionTransformerConfig', 'DecisionTransformerGPT2Model', 'DecisionTransformerGPT2PreTrainedModel', 'DecisionTransformerModel', 'DecisionTransformerPreTrainedModel', 'DefaultDataCollator', 'DefaultFlowCallback', 'DeformableDetrConfig', 'DeformableDetrFeatureExtractor', 'DeformableDetrForObjectDetection', 'DeformableDetrImageProcessor', 'DeformableDetrModel', 'DeformableDetrPreTrainedModel', 'DeiTConfig', 'DeiTFeatureExtractor', 'DeiTForImageClassification', 'DeiTForImageClassificationWithTeacher', 'DeiTForMaskedImageModeling', 'DeiTImageProcessor', 'DeiTModel', 'DeiTPreTrainedModel', 'DepthEstimationPipeline', 'DetaConfig', 'DetaForObjectDetection', 'DetaImageProcessor', 'DetaModel', 'DetaPreTrainedModel', 'DetrConfig', 'DetrFeatureExtractor', 'DetrForObjectDetection', 'DetrForSegmentation', 'DetrImageProcessor', 'DetrModel', 'DetrPreTrainedModel', 'DinatBackbone', 'DinatConfig', 'DinatForImageClassification', 'DinatModel', 'DinatPreTrainedModel', 'DisjunctiveConstraint', 'DistilBertConfig', 'DistilBertForMaskedLM', 'DistilBertForMultipleChoice', 'DistilBertForQuestionAnswering', 'DistilBertForSequenceClassification', 'DistilBertForTokenClassification', 'DistilBertModel', 'DistilBertPreTrainedModel', 'DistilBertTokenizer', 'DistilBertTokenizerFast', 'DocumentQuestionAnsweringPipeline', 'DonutFeatureExtractor', 'DonutImageProcessor', 'DonutProcessor', 'DonutSwinConfig', 'DonutSwinModel', 'DonutSwinPreTrainedModel', 'DummyObject', 'EFFICIENTFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'EFFICIENTFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'EFFICIENTNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'EFFICIENTNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ELECTRA_PRETRAINED_MODEL_ARCHIVE_LIST', 'ENCODEC_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ENCODEC_PRETRAINED_MODEL_ARCHIVE_LIST', 'ERNIE_M_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ERNIE_M_PRETRAINED_MODEL_ARCHIVE_LIST', 'ERNIE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ERNIE_PRETRAINED_MODEL_ARCHIVE_LIST', 'ESM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ESM_PRETRAINED_MODEL_ARCHIVE_LIST', 'EarlyStoppingCallback', 'EfficientFormerConfig', 'EfficientFormerForImageClassification', 'EfficientFormerForImageClassificationWithTeacher', 'EfficientFormerImageProcessor', 'EfficientFormerModel', 'EfficientFormerPreTrainedModel', 'EfficientNetConfig', 'EfficientNetForImageClassification', 'EfficientNetImageProcessor', 'EfficientNetModel', 'EfficientNetPreTrainedModel', 'ElectraConfig', 'ElectraForCausalLM', 'ElectraForMaskedLM', 'ElectraForMultipleChoice', 'ElectraForPreTraining', 'ElectraForQuestionAnswering', 'ElectraForSequenceClassification', 'ElectraForTokenClassification', 'ElectraModel', 'ElectraPreTrainedModel', 'ElectraTokenizer', 'ElectraTokenizerFast', 'EncodecConfig', 'EncodecFeatureExtractor', 'EncodecModel', 'EncodecPreTrainedModel', 'EncoderDecoderConfig', 'EncoderDecoderModel', 'ErnieConfig', 'ErnieForCausalLM', 'ErnieForMaskedLM', 'ErnieForMultipleChoice', 'ErnieForNextSentencePrediction', 'ErnieForPreTraining', 'ErnieForQuestionAnswering', 'ErnieForSequenceClassification', 'ErnieForTokenClassification', 'ErnieMConfig', 'ErnieMForInformationExtraction', 'ErnieMForMultipleChoice', 'ErnieMForQuestionAnswering', 'ErnieMForSequenceClassification', 'ErnieMForTokenClassification', 'ErnieMModel', 'ErnieMPreTrainedModel', 'ErnieMTokenizer', 'ErnieModel', 'ErniePreTrainedModel', 'EsmConfig', 'EsmFoldPreTrainedModel', 'EsmForMaskedLM', 'EsmForProteinFolding', 'EsmForSequenceClassification', 'EsmForTokenClassification', 'EsmModel', 'EsmPreTrainedModel', 'EsmTokenizer', 'EvalPrediction', 'FALCON_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FALCON_PRETRAINED_MODEL_ARCHIVE_LIST', 'FEATURE_EXTRACTOR_MAPPING', 'FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FLAUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'FLAVA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FLAVA_PRETRAINED_MODEL_ARCHIVE_LIST', 'FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_MASKED_LM_MAPPING', 'FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'FLAX_MODEL_FOR_PRETRAINING_MAPPING', 'FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING', 'FLAX_MODEL_MAPPING', 'FLAX_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'FNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'FNetConfig', 'FNetForMaskedLM', 'FNetForMultipleChoice', 'FNetForNextSentencePrediction', 'FNetForPreTraining', 'FNetForQuestionAnswering', 'FNetForSequenceClassification', 'FNetForTokenClassification', 'FNetLayer', 'FNetModel', 'FNetPreTrainedModel', 'FNetTokenizer', 'FNetTokenizerFast', 'FOCALNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FOCALNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'FSMTConfig', 'FSMTForConditionalGeneration', 'FSMTModel', 'FSMTTokenizer', 'FSMT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FUNNEL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'FUNNEL_PRETRAINED_MODEL_ARCHIVE_LIST', 'FalconConfig', 'FalconForCausalLM', 'FalconForQuestionAnswering', 'FalconForSequenceClassification', 'FalconForTokenClassification', 'FalconModel', 'FalconPreTrainedModel', 'FeatureExtractionMixin', 'FeatureExtractionPipeline', 'FillMaskPipeline', 'FlaubertConfig', 'FlaubertForMultipleChoice', 'FlaubertForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FlaubertForSequenceClassification', 'FlaubertForTokenClassification', 'FlaubertModel', 'FlaubertPreTrainedModel', 'FlaubertTokenizer', 'FlaubertWithLMHeadModel', 'FlavaConfig', 'FlavaFeatureExtractor', 'FlavaForPreTraining', 'FlavaImageCodebook', 'FlavaImageCodebookConfig', 'FlavaImageConfig', 'FlavaImageModel', 'FlavaImageProcessor', 'FlavaModel', 'FlavaMultimodalConfig', 'FlavaMultimodalModel', 'FlavaPreTrainedModel', 'FlavaProcessor', 'FlavaTextConfig', 'FlavaTextModel', 'FlaxAlbertForMaskedLM', 'FlaxAlbertForMultipleChoice', 'FlaxAlbertForPreTraining', 'FlaxAlbertForQuestionAnswering', 'FlaxAlbertForSequenceClassification', 'FlaxAlbertForTokenClassification', 'FlaxAlbertModel', 'FlaxAlbertPreTrainedModel', 'FlaxAutoModel', 'FlaxAutoModelForCausalLM', 'FlaxAutoModelForImageClassification', 'FlaxAutoModelForMaskedLM', 'FlaxAutoModelForMultipleChoice', 'FlaxAutoModelForNextSentencePrediction', 'FlaxAutoModelForPreTraining', 'FlaxAutoModelForQuestionAnswering', 'FlaxAutoModelForSeq2SeqLM', 'FlaxAutoModelForSequenceClassification', 'FlaxAutoModelForSpeechSeq2Seq', 'FlaxAutoModelForTokenClassification', 'FlaxAutoModelForVision2Seq', 'FlaxBartDecoderPreTrainedModel', 'FlaxBartForCausalLM', 'FlaxBartForConditionalGeneration', 'FlaxBartForQuestionAnswering', 'FlaxBartForSequenceClassification', 'FlaxBartModel', 'FlaxBartPreTrainedModel', 'FlaxBeitForImageClassification', 'FlaxBeitForMaskedImageModeling', 'FlaxBeitModel', 'FlaxBeitPreTrainedModel', 'FlaxBertForCausalLM', 'FlaxBertForMaskedLM', 'FlaxBertForMultipleChoice', 'FlaxBertForNextSentencePrediction', 'FlaxBertForPreTraining', 'FlaxBertForQuestionAnswering', 'FlaxBertForSequenceClassification', 'FlaxBertForTokenClassification', 'FlaxBertModel', 'FlaxBertPreTrainedModel', 'FlaxBigBirdForCausalLM', 'FlaxBigBirdForMaskedLM', 'FlaxBigBirdForMultipleChoice', 'FlaxBigBirdForPreTraining', 'FlaxBigBirdForQuestionAnswering', 'FlaxBigBirdForSequenceClassification', 'FlaxBigBirdForTokenClassification', 'FlaxBigBirdModel', 'FlaxBigBirdPreTrainedModel', 'FlaxBlenderbotForConditionalGeneration', 'FlaxBlenderbotModel', 'FlaxBlenderbotPreTrainedModel', 'FlaxBlenderbotSmallForConditionalGeneration', 'FlaxBlenderbotSmallModel', 'FlaxBlenderbotSmallPreTrainedModel', 'FlaxCLIPModel', 'FlaxCLIPPreTrainedModel', 'FlaxCLIPTextModel', 'FlaxCLIPTextPreTrainedModel', 'FlaxCLIPVisionModel', 'FlaxCLIPVisionPreTrainedModel', 'FlaxDistilBertForMaskedLM', 'FlaxDistilBertForMultipleChoice', 'FlaxDistilBertForQuestionAnswering', 'FlaxDistilBertForSequenceClassification', 'FlaxDistilBertForTokenClassification', 'FlaxDistilBertModel', 'FlaxDistilBertPreTrainedModel', 'FlaxElectraForCausalLM', 'FlaxElectraForMaskedLM', 'FlaxElectraForMultipleChoice', 'FlaxElectraForPreTraining', 'FlaxElectraForQuestionAnswering', 'FlaxElectraForSequenceClassification', 'FlaxElectraForTokenClassification', 'FlaxElectraModel', 'FlaxElectraPreTrainedModel', 'FlaxEncoderDecoderModel', 'FlaxForcedBOSTokenLogitsProcessor', 'FlaxForcedEOSTokenLogitsProcessor', 'FlaxGPT2LMHeadModel', 'FlaxGPT2Model', 'FlaxGPT2PreTrainedModel', 'FlaxGPTJForCausalLM', 'FlaxGPTJModel', 'FlaxGPTJPreTrainedModel', 'FlaxGPTNeoForCausalLM', 'FlaxGPTNeoModel', 'FlaxGPTNeoPreTrainedModel', 'FlaxGenerationMixin', 'FlaxLogitsProcessor', 'FlaxLogitsProcessorList', 'FlaxLogitsWarper', 'FlaxLongT5ForConditionalGeneration', 'FlaxLongT5Model', 'FlaxLongT5PreTrainedModel', 'FlaxMBartForConditionalGeneration', 'FlaxMBartForQuestionAnswering', 'FlaxMBartForSequenceClassification', 'FlaxMBartModel', 'FlaxMBartPreTrainedModel', 'FlaxMT5EncoderModel', 'FlaxMT5ForConditionalGeneration', 'FlaxMT5Model', 'FlaxMarianMTModel', 'FlaxMarianModel', 'FlaxMarianPreTrainedModel', 'FlaxMinLengthLogitsProcessor', 'FlaxOPTForCausalLM', 'FlaxOPTModel', 'FlaxOPTPreTrainedModel', 'FlaxPegasusForConditionalGeneration', 'FlaxPegasusModel', 'FlaxPegasusPreTrainedModel', 'FlaxPreTrainedModel', 'FlaxRegNetForImageClassification', 'FlaxRegNetModel', 'FlaxRegNetPreTrainedModel', 'FlaxResNetForImageClassification', 'FlaxResNetModel', 'FlaxResNetPreTrainedModel', 'FlaxRoFormerForMaskedLM', 'FlaxRoFormerForMultipleChoice', 'FlaxRoFormerForQuestionAnswering', 'FlaxRoFormerForSequenceClassification', 'FlaxRoFormerForTokenClassification', 'FlaxRoFormerModel', 'FlaxRoFormerPreTrainedModel', 'FlaxRobertaForCausalLM', 'FlaxRobertaForMaskedLM', 'FlaxRobertaForMultipleChoice', 'FlaxRobertaForQuestionAnswering', 'FlaxRobertaForSequenceClassification', 'FlaxRobertaForTokenClassification', 'FlaxRobertaModel', 'FlaxRobertaPreLayerNormForCausalLM', 'FlaxRobertaPreLayerNormForMaskedLM', 'FlaxRobertaPreLayerNormForMultipleChoice', 'FlaxRobertaPreLayerNormForQuestionAnswering', 'FlaxRobertaPreLayerNormForSequenceClassification', 'FlaxRobertaPreLayerNormForTokenClassification', 'FlaxRobertaPreLayerNormModel', 'FlaxRobertaPreLayerNormPreTrainedModel', 'FlaxRobertaPreTrainedModel', 'FlaxSpeechEncoderDecoderModel', 'FlaxT5EncoderModel', 'FlaxT5ForConditionalGeneration', 'FlaxT5Model', 'FlaxT5PreTrainedModel', 'FlaxTemperatureLogitsWarper', 'FlaxTopKLogitsWarper', 'FlaxTopPLogitsWarper', 'FlaxViTForImageClassification', 'FlaxViTModel', 'FlaxViTPreTrainedModel', 'FlaxVisionEncoderDecoderModel', 'FlaxVisionTextDualEncoderModel', 'FlaxWav2Vec2ForCTC', 'FlaxWav2Vec2ForPreTraining', 'FlaxWav2Vec2Model', 'FlaxWav2Vec2PreTrainedModel', 'FlaxWhisperForAudioClassification', 'FlaxWhisperForConditionalGeneration', 'FlaxWhisperModel', 'FlaxWhisperPreTrainedModel', 'FlaxXGLMForCausalLM', 'FlaxXGLMModel', 'FlaxXGLMPreTrainedModel', 'FlaxXLMRobertaForCausalLM', 'FlaxXLMRobertaForMaskedLM', 'FlaxXLMRobertaForMultipleChoice', 'FlaxXLMRobertaForQuestionAnswering', 'FlaxXLMRobertaForSequenceClassification', 'FlaxXLMRobertaForTokenClassification', 'FlaxXLMRobertaModel', 'FlaxXLMRobertaPreTrainedModel', 'FocalNetBackbone', 'FocalNetConfig', 'FocalNetForImageClassification', 'FocalNetForMaskedImageModeling', 'FocalNetModel', 'FocalNetPreTrainedModel', 'ForcedBOSTokenLogitsProcessor', 'ForcedEOSTokenLogitsProcessor', 'FunnelBaseModel', 'FunnelConfig', 'FunnelForMaskedLM', 'FunnelForMultipleChoice', 'FunnelForPreTraining', 'FunnelForQuestionAnswering', 'FunnelForSequenceClassification', 'FunnelForTokenClassification', 'FunnelModel', 'FunnelPreTrainedModel', 'FunnelTokenizer', 'FunnelTokenizerFast', 'GIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'GLPNConfig', 'GLPNFeatureExtractor', 'GLPNForDepthEstimation', 'GLPNImageProcessor', 'GLPNModel', 'GLPNPreTrainedModel', 'GLPN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GLPN_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT2Config', 'GPT2DoubleHeadsModel', 'GPT2ForQuestionAnswering', 'GPT2ForSequenceClassification', 'GPT2ForTokenClassification', 'GPT2LMHeadModel', 'GPT2Model', 'GPT2PreTrainedModel', 'GPT2Tokenizer', 'GPT2TokenizerFast', 'GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT2_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPTBigCodeConfig', 'GPTBigCodeForCausalLM', 'GPTBigCodeForSequenceClassification', 'GPTBigCodeForTokenClassification', 'GPTBigCodeModel', 'GPTBigCodePreTrainedModel', 'GPTJConfig', 'GPTJForCausalLM', 'GPTJForQuestionAnswering', 'GPTJForSequenceClassification', 'GPTJModel', 'GPTJPreTrainedModel', 'GPTJ_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPTJ_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPTNeoConfig', 'GPTNeoForCausalLM', 'GPTNeoForQuestionAnswering', 'GPTNeoForSequenceClassification', 'GPTNeoForTokenClassification', 'GPTNeoModel', 'GPTNeoPreTrainedModel', 'GPTNeoXConfig', 'GPTNeoXForCausalLM', 'GPTNeoXForQuestionAnswering', 'GPTNeoXForSequenceClassification', 'GPTNeoXForTokenClassification', 'GPTNeoXJapaneseConfig', 'GPTNeoXJapaneseForCausalLM', 'GPTNeoXJapaneseLayer', 'GPTNeoXJapaneseModel', 'GPTNeoXJapanesePreTrainedModel', 'GPTNeoXJapaneseTokenizer', 'GPTNeoXLayer', 'GPTNeoXModel', 'GPTNeoXPreTrainedModel', 'GPTNeoXTokenizerFast', 'GPTSAN_JAPANESE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPTSAN_JAPANESE_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPTSanJapaneseConfig', 'GPTSanJapaneseForConditionalGeneration', 'GPTSanJapaneseModel', 'GPTSanJapanesePreTrainedModel', 'GPTSanJapaneseTokenizer', 'GPTSw3Tokenizer', 'GPT_BIGCODE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_BIGCODE_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT_NEOX_JAPANESE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEOX_JAPANESE_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT_NEOX_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEOX_PRETRAINED_MODEL_ARCHIVE_LIST', 'GPT_NEO_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GPT_NEO_PRETRAINED_MODEL_ARCHIVE_LIST', 'GRAPHORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GRAPHORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'GROUPVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'GenerationConfig', 'GenerationMixin', 'GitConfig', 'GitForCausalLM', 'GitModel', 'GitPreTrainedModel', 'GitProcessor', 'GitVisionConfig', 'GitVisionModel', 'GlueDataTrainingArguments', 'GlueDataset', 'GradientAccumulator', 'GraphormerConfig', 'GraphormerForGraphClassification', 'GraphormerModel', 'GraphormerPreTrainedModel', 'GroupViTConfig', 'GroupViTModel', 'GroupViTPreTrainedModel', 'GroupViTTextConfig', 'GroupViTTextModel', 'GroupViTVisionConfig', 'GroupViTVisionModel', 'HUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'HUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'HammingDiversityLogitsProcessor', 'HerbertTokenizer', 'HerbertTokenizerFast', 'HfAgent', 'HfArgumentParser', 'HubertConfig', 'HubertForCTC', 'HubertForSequenceClassification', 'HubertModel', 'HubertPreTrainedModel', 'IBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'IBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'IBertConfig', 'IBertForMaskedLM', 'IBertForMultipleChoice', 'IBertForQuestionAnswering', 'IBertForSequenceClassification', 'IBertForTokenClassification', 'IBertModel', 'IBertPreTrainedModel', 'IMAGEGPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'IMAGEGPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'IMAGE_PROCESSOR_MAPPING', 'INFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'INFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'INSTRUCTBLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'INSTRUCTBLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'ImageClassificationPipeline', 'ImageFeatureExtractionMixin', 'ImageGPTConfig', 'ImageGPTFeatureExtractor', 'ImageGPTForCausalImageModeling', 'ImageGPTForImageClassification', 'ImageGPTImageProcessor', 'ImageGPTModel', 'ImageGPTPreTrainedModel', 'ImageProcessingMixin', 'ImageSegmentationPipeline', 'ImageToTextPipeline', 'InfNanRemoveLogitsProcessor', 'InformerConfig', 'InformerForPrediction', 'InformerModel', 'InformerPreTrainedModel', 'InputExample', 'InputFeatures', 'InstructBlipConfig', 'InstructBlipForConditionalGeneration', 'InstructBlipPreTrainedModel', 'InstructBlipProcessor', 'InstructBlipQFormerConfig', 'InstructBlipQFormerModel', 'InstructBlipVisionConfig', 'InstructBlipVisionModel', 'IntervalStrategy', 'JUKEBOX_PRETRAINED_CONFIG_ARCHIVE_MAP', 'JUKEBOX_PRETRAINED_MODEL_ARCHIVE_LIST', 'JsonPipelineDataFormat', 'JukeboxConfig', 'JukeboxModel', 'JukeboxPreTrainedModel', 'JukeboxPrior', 'JukeboxPriorConfig', 'JukeboxTokenizer', 'JukeboxVQVAE', 'JukeboxVQVAEConfig', 'KerasMetricCallback', 'LAYOUTLMV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLMV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'LAYOUTLMV3_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLMV3_PRETRAINED_MODEL_ARCHIVE_LIST', 'LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'LEDConfig', 'LEDForConditionalGeneration', 'LEDForQuestionAnswering', 'LEDForSequenceClassification', 'LEDModel', 'LEDPreTrainedModel', 'LEDTokenizer', 'LEDTokenizerFast', 'LED_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LED_PRETRAINED_MODEL_ARCHIVE_LIST', 'LEVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'LILT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LILT_PRETRAINED_MODEL_ARCHIVE_LIST', 'LLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LONGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LONGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'LONGT5_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LONGT5_PRETRAINED_MODEL_ARCHIVE_LIST', 'LUKE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LUKE_PRETRAINED_MODEL_ARCHIVE_LIST', 'LXMERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'LayoutLMConfig', 'LayoutLMForMaskedLM', 'LayoutLMForQuestionAnswering', 'LayoutLMForSequenceClassification', 'LayoutLMForTokenClassification', 'LayoutLMModel', 'LayoutLMPreTrainedModel', 'LayoutLMTokenizer', 'LayoutLMTokenizerFast', 'LayoutLMv2Config', 'LayoutLMv2FeatureExtractor', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv2ImageProcessor', 'LayoutLMv2Model', 'LayoutLMv2PreTrainedModel', 'LayoutLMv2Processor', 'LayoutLMv2Tokenizer', 'LayoutLMv2TokenizerFast', 'LayoutLMv3Config', 'LayoutLMv3FeatureExtractor', 'LayoutLMv3ForQuestionAnswering', 'LayoutLMv3ForSequenceClassification', 'LayoutLMv3ForTokenClassification', 'LayoutLMv3ImageProcessor', 'LayoutLMv3Model', 'LayoutLMv3PreTrainedModel', 'LayoutLMv3Processor', 'LayoutLMv3Tokenizer', 'LayoutLMv3TokenizerFast', 'LayoutXLMProcessor', 'LayoutXLMTokenizer', 'LayoutXLMTokenizerFast', 'LevitConfig', 'LevitFeatureExtractor', 'LevitForImageClassification', 'LevitForImageClassificationWithTeacher', 'LevitImageProcessor', 'LevitModel', 'LevitPreTrainedModel', 'LiltConfig', 'LiltForQuestionAnswering', 'LiltForSequenceClassification', 'LiltForTokenClassification', 'LiltModel', 'LiltPreTrainedModel', 'LineByLineTextDataset', 'LineByLineWithRefDataset', 'LineByLineWithSOPTextDataset', 'LlamaConfig', 'LlamaForCausalLM', 'LlamaForSequenceClassification', 'LlamaModel', 'LlamaPreTrainedModel', 'LlamaTokenizer', 'LlamaTokenizerFast', 'LocalAgent', 'LogitsProcessor', 'LogitsProcessorList', 'LogitsWarper', 'LongT5Config', 'LongT5EncoderModel', 'LongT5ForConditionalGeneration', 'LongT5Model', 'LongT5PreTrainedModel', 'LongformerConfig', 'LongformerForMaskedLM', 'LongformerForMultipleChoice', 'LongformerForQuestionAnswering', 'LongformerForSequenceClassification', 'LongformerForTokenClassification', 'LongformerModel', 'LongformerPreTrainedModel', 'LongformerSelfAttention', 'LongformerTokenizer', 'LongformerTokenizerFast', 'LukeConfig', 'LukeForEntityClassification', 'LukeForEntityPairClassification', 'LukeForEntitySpanClassification', 'LukeForMaskedLM', 'LukeForMultipleChoice', 'LukeForQuestionAnswering', 'LukeForSequenceClassification', 'LukeForTokenClassification', 'LukeModel', 'LukePreTrainedModel', 'LukeTokenizer', 'LxmertConfig', 'LxmertEncoder', 'LxmertForPreTraining', 'LxmertForQuestionAnswering', 'LxmertModel', 'LxmertPreTrainedModel', 'LxmertTokenizer', 'LxmertTokenizerFast', 'LxmertVisualFeatureEncoder', 'LxmertXLayer', 'M2M100Config', 'M2M100ForConditionalGeneration', 'M2M100Model', 'M2M100PreTrainedModel', 'M2M100Tokenizer', 'M2M_100_PRETRAINED_CONFIG_ARCHIVE_MAP', 'M2M_100_PRETRAINED_MODEL_ARCHIVE_LIST', 'MARKUPLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MARKUPLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'MASK2FORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MASK2FORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'MASKFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MASKFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'MBart50Tokenizer', 'MBart50TokenizerFast', 'MBartConfig', 'MBartForCausalLM', 'MBartForConditionalGeneration', 'MBartForQuestionAnswering', 'MBartForSequenceClassification', 'MBartModel', 'MBartPreTrainedModel', 'MBartTokenizer', 'MBartTokenizerFast', 'MCTCTConfig', 'MCTCTFeatureExtractor', 'MCTCTForCTC', 'MCTCTModel', 'MCTCTPreTrainedModel', 'MCTCTProcessor', 'MCTCT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MCTCT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MEGATRON_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MEGATRON_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MEGA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MEGA_PRETRAINED_MODEL_ARCHIVE_LIST', 'MGP_STR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MGP_STR_PRETRAINED_MODEL_ARCHIVE_LIST', 'MLukeTokenizer', 'MMBTConfig', 'MMBTForClassification', 'MMBTModel', 'MOBILEBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILENET_V1_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILENET_V1_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILENET_V2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILENET_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILEVITV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILEVITV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'MOBILEVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MOBILEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'MODEL_CARD_NAME', 'MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING', 'MODEL_FOR_AUDIO_XVECTOR_MAPPING', 'MODEL_FOR_BACKBONE_MAPPING', 'MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING', 'MODEL_FOR_CAUSAL_LM_MAPPING', 'MODEL_FOR_CTC_MAPPING', 'MODEL_FOR_DEPTH_ESTIMATION_MAPPING', 'MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'MODEL_FOR_IMAGE_SEGMENTATION_MAPPING', 'MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING', 'MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING', 'MODEL_FOR_MASKED_LM_MAPPING', 'MODEL_FOR_MASK_GENERATION_MAPPING', 'MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'MODEL_FOR_OBJECT_DETECTION_MAPPING', 'MODEL_FOR_PRETRAINING_MAPPING', 'MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING', 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_TEXT_ENCODING_MAPPING', 'MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING', 'MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING', 'MODEL_FOR_VISION_2_SEQ_MAPPING', 'MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING', 'MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING', 'MODEL_MAPPING', 'MODEL_NAMES_MAPPING', 'MODEL_WITH_LM_HEAD_MAPPING', 'MPNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MPNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'MPNetConfig', 'MPNetForMaskedLM', 'MPNetForMultipleChoice', 'MPNetForQuestionAnswering', 'MPNetForSequenceClassification', 'MPNetForTokenClassification', 'MPNetLayer', 'MPNetModel', 'MPNetPreTrainedModel', 'MPNetTokenizer', 'MPNetTokenizerFast', 'MRA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MRA_PRETRAINED_MODEL_ARCHIVE_LIST', 'MT5Config', 'MT5EncoderModel', 'MT5ForConditionalGeneration', 'MT5ForQuestionAnswering', 'MT5Model', 'MT5PreTrainedModel', 'MT5Tokenizer', 'MT5TokenizerFast', 'MUSICGEN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'MUSICGEN_PRETRAINED_MODEL_ARCHIVE_LIST', 'MVP_PRETRAINED_MODEL_ARCHIVE_LIST', 'MarianConfig', 'MarianForCausalLM', 'MarianMTModel', 'MarianModel', 'MarianTokenizer', 'MarkupLMConfig', 'MarkupLMFeatureExtractor', 'MarkupLMForQuestionAnswering', 'MarkupLMForSequenceClassification', 'MarkupLMForTokenClassification', 'MarkupLMModel', 'MarkupLMPreTrainedModel', 'MarkupLMProcessor', 'MarkupLMTokenizer', 'MarkupLMTokenizerFast', 'Mask2FormerConfig', 'Mask2FormerForUniversalSegmentation', 'Mask2FormerImageProcessor', 'Mask2FormerModel', 'Mask2FormerPreTrainedModel', 'MaskFormerConfig', 'MaskFormerFeatureExtractor', 'MaskFormerForInstanceSegmentation', 'MaskFormerImageProcessor', 'MaskFormerModel', 'MaskFormerPreTrainedModel', 'MaskFormerSwinBackbone', 'MaskFormerSwinConfig', 'MaxLengthCriteria', 'MaxTimeCriteria', 'MecabTokenizer', 'MegaConfig', 'MegaForCausalLM', 'MegaForMaskedLM', 'MegaForMultipleChoice', 'MegaForQuestionAnswering', 'MegaForSequenceClassification', 'MegaForTokenClassification', 'MegaModel', 'MegaPreTrainedModel', 'MegatronBertConfig', 'MegatronBertForCausalLM', 'MegatronBertForMaskedLM', 'MegatronBertForMultipleChoice', 'MegatronBertForNextSentencePrediction', 'MegatronBertForPreTraining', 'MegatronBertForQuestionAnswering', 'MegatronBertForSequenceClassification', 'MegatronBertForTokenClassification', 'MegatronBertModel', 'MegatronBertPreTrainedModel', 'MgpstrConfig', 'MgpstrForSceneTextRecognition', 'MgpstrModel', 'MgpstrPreTrainedModel', 'MgpstrProcessor', 'MgpstrTokenizer', 'MinLengthLogitsProcessor', 'MinNewTokensLengthLogitsProcessor', 'MobileBertConfig', 'MobileBertForMaskedLM', 'MobileBertForMultipleChoice', 'MobileBertForNextSentencePrediction', 'MobileBertForPreTraining', 'MobileBertForQuestionAnswering', 'MobileBertForSequenceClassification', 'MobileBertForTokenClassification', 'MobileBertLayer', 'MobileBertModel', 'MobileBertPreTrainedModel', 'MobileBertTokenizer', 'MobileBertTokenizerFast', 'MobileNetV1Config', 'MobileNetV1FeatureExtractor', 'MobileNetV1ForImageClassification', 'MobileNetV1ImageProcessor', 'MobileNetV1Model', 'MobileNetV1PreTrainedModel', 'MobileNetV2Config', 'MobileNetV2FeatureExtractor', 'MobileNetV2ForImageClassification', 'MobileNetV2ForSemanticSegmentation', 'MobileNetV2ImageProcessor', 'MobileNetV2Model', 'MobileNetV2PreTrainedModel', 'MobileViTConfig', 'MobileViTFeatureExtractor', 'MobileViTForImageClassification', 'MobileViTForSemanticSegmentation', 'MobileViTImageProcessor', 'MobileViTModel', 'MobileViTPreTrainedModel', 'MobileViTV2Config', 'MobileViTV2ForImageClassification', 'MobileViTV2ForSemanticSegmentation', 'MobileViTV2Model', 'MobileViTV2PreTrainedModel', 'ModalEmbeddings', 'ModelCard', 'MraConfig', 'MraForMaskedLM', 'MraForMultipleChoice', 'MraForQuestionAnswering', 'MraForSequenceClassification', 'MraForTokenClassification', 'MraModel', 'MraPreTrainedModel', 'MusicgenConfig', 'MusicgenDecoderConfig', 'MusicgenForCausalLM', 'MusicgenForConditionalGeneration', 'MusicgenModel', 'MusicgenPreTrainedModel', 'MusicgenProcessor', 'MvpConfig', 'MvpForCausalLM', 'MvpForConditionalGeneration', 'MvpForQuestionAnswering', 'MvpForSequenceClassification', 'MvpModel', 'MvpPreTrainedModel', 'MvpTokenizer', 'MvpTokenizerFast', 'NAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'NEZHA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NEZHA_PRETRAINED_MODEL_ARCHIVE_LIST', 'NLLB_MOE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NLLB_MOE_PRETRAINED_MODEL_ARCHIVE_LIST', 'NYSTROMFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'NYSTROMFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'NatBackbone', 'NatConfig', 'NatForImageClassification', 'NatModel', 'NatPreTrainedModel', 'NerPipeline', 'NezhaConfig', 'NezhaForMaskedLM', 'NezhaForMultipleChoice', 'NezhaForNextSentencePrediction', 'NezhaForPreTraining', 'NezhaForQuestionAnswering', 'NezhaForSequenceClassification', 'NezhaForTokenClassification', 'NezhaModel', 'NezhaPreTrainedModel', 'NllbMoeConfig', 'NllbMoeForConditionalGeneration', 'NllbMoeModel', 'NllbMoePreTrainedModel', 'NllbMoeSparseMLP', 'NllbMoeTop2Router', 'NllbTokenizer', 'NllbTokenizerFast', 'NoBadWordsLogitsProcessor', 'NoRepeatNGramLogitsProcessor', 'NystromformerConfig', 'NystromformerForMaskedLM', 'NystromformerForMultipleChoice', 'NystromformerForQuestionAnswering', 'NystromformerForSequenceClassification', 'NystromformerForTokenClassification', 'NystromformerLayer', 'NystromformerModel', 'NystromformerPreTrainedModel', 'ONEFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ONEFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'OPEN_LLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'OPTConfig', 'OPTForCausalLM', 'OPTForQuestionAnswering', 'OPTForSequenceClassification', 'OPTModel', 'OPTPreTrainedModel', 'OPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'OWLVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'OWLVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ObjectDetectionPipeline', 'OneFormerConfig', 'OneFormerForUniversalSegmentation', 'OneFormerImageProcessor', 'OneFormerModel', 'OneFormerPreTrainedModel', 'OneFormerProcessor', 'OpenAIGPTConfig', 'OpenAIGPTDoubleHeadsModel', 'OpenAIGPTForSequenceClassification', 'OpenAIGPTLMHeadModel', 'OpenAIGPTModel', 'OpenAIGPTPreTrainedModel', 'OpenAIGPTTokenizer', 'OpenAIGPTTokenizerFast', 'OpenAiAgent', 'OpenLlamaConfig', 'OpenLlamaForCausalLM', 'OpenLlamaForSequenceClassification', 'OpenLlamaModel', 'OpenLlamaPreTrainedModel', 'OwlViTConfig', 'OwlViTFeatureExtractor', 'OwlViTForObjectDetection', 'OwlViTImageProcessor', 'OwlViTModel', 'OwlViTPreTrainedModel', 'OwlViTProcessor', 'OwlViTTextConfig', 'OwlViTTextModel', 'OwlViTVisionConfig', 'OwlViTVisionModel', 'PEGASUS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PEGASUS_X_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PEGASUS_X_PRETRAINED_MODEL_ARCHIVE_LIST', 'PERCEIVER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PERCEIVER_PRETRAINED_MODEL_ARCHIVE_LIST', 'PIX2STRUCT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PIX2STRUCT_PRETRAINED_MODEL_ARCHIVE_LIST', 'PLBART_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PLBART_PRETRAINED_MODEL_ARCHIVE_LIST', 'PLBartConfig', 'PLBartForCausalLM', 'PLBartForConditionalGeneration', 'PLBartForSequenceClassification', 'PLBartModel', 'PLBartPreTrainedModel', 'PLBartTokenizer', 'POOLFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'POOLFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'PROCESSOR_MAPPING', 'PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'PROPHETNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'PYTORCH_PRETRAINED_BERT_CACHE', 'PYTORCH_TRANSFORMERS_CACHE', 'PegasusConfig', 'PegasusForCausalLM', 'PegasusForConditionalGeneration', 'PegasusModel', 'PegasusPreTrainedModel', 'PegasusTokenizer', 'PegasusTokenizerFast', 'PegasusXConfig', 'PegasusXForConditionalGeneration', 'PegasusXModel', 'PegasusXPreTrainedModel', 'PerceiverConfig', 'PerceiverFeatureExtractor', 'PerceiverForImageClassificationConvProcessing', 'PerceiverForImageClassificationFourier', 'PerceiverForImageClassificationLearned', 'PerceiverForMaskedLM', 'PerceiverForMultimodalAutoencoding', 'PerceiverForOpticalFlow', 'PerceiverForSequenceClassification', 'PerceiverImageProcessor', 'PerceiverLayer', 'PerceiverModel', 'PerceiverPreTrainedModel', 'PerceiverTokenizer', 'PhobertTokenizer', 'PhrasalConstraint', 'PipedPipelineDataFormat', 'Pipeline', 'PipelineDataFormat', 'PipelineTool', 'Pix2StructConfig', 'Pix2StructForConditionalGeneration', 'Pix2StructImageProcessor', 'Pix2StructPreTrainedModel', 'Pix2StructProcessor', 'Pix2StructTextConfig', 'Pix2StructTextModel', 'Pix2StructVisionConfig', 'Pix2StructVisionModel', 'PoolFormerConfig', 'PoolFormerFeatureExtractor', 'PoolFormerForImageClassification', 'PoolFormerImageProcessor', 'PoolFormerModel', 'PoolFormerPreTrainedModel', 'PreTrainedModel', 'PreTrainedTokenizer', 'PreTrainedTokenizerBase', 'PreTrainedTokenizerFast', 'PrefixConstrainedLogitsProcessor', 'PretrainedBartModel', 'PretrainedConfig', 'PretrainedFSMTModel', 'PrinterCallback', 'ProcessorMixin', 'ProgressCallback', 'ProphetNetConfig', 'ProphetNetDecoder', 'ProphetNetEncoder', 'ProphetNetForCausalLM', 'ProphetNetForConditionalGeneration', 'ProphetNetModel', 'ProphetNetPreTrainedModel', 'ProphetNetTokenizer', 'PushToHubCallback', 'PyTorchBenchmark', 'PyTorchBenchmarkArguments', 'QDQBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'QDQBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'QDQBertConfig', 'QDQBertForMaskedLM', 'QDQBertForMultipleChoice', 'QDQBertForNextSentencePrediction', 'QDQBertForQuestionAnswering', 'QDQBertForSequenceClassification', 'QDQBertForTokenClassification', 'QDQBertLMHeadModel', 'QDQBertLayer', 'QDQBertModel', 'QDQBertPreTrainedModel', 'QuestionAnsweringPipeline', 'REALM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REALM_PRETRAINED_MODEL_ARCHIVE_LIST', 'REFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'REGNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REGNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'REMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'REMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'RESNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'RESNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'RETRIBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'RETRIBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROBERTA_PRELAYERNORM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROBERTA_PRELAYERNORM_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROC_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROC_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'ROFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'ROFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'RWKV_PRETRAINED_CONFIG_ARCHIVE_MAP', 'RWKV_PRETRAINED_MODEL_ARCHIVE_LIST', 'RagConfig', 'RagModel', 'RagPreTrainedModel', 'RagRetriever', 'RagSequenceForGeneration', 'RagTokenForGeneration', 'RagTokenizer', 'RealmConfig', 'RealmEmbedder', 'RealmForOpenQA', 'RealmKnowledgeAugEncoder', 'RealmPreTrainedModel', 'RealmReader', 'RealmRetriever', 'RealmScorer', 'RealmTokenizer', 'RealmTokenizerFast', 'ReformerAttention', 'ReformerConfig', 'ReformerForMaskedLM', 'ReformerForQuestionAnswering', 'ReformerForSequenceClassification', 'ReformerLayer', 'ReformerModel', 'ReformerModelWithLMHead', 'ReformerPreTrainedModel', 'ReformerTokenizer', 'ReformerTokenizerFast', 'RegNetConfig', 'RegNetForImageClassification', 'RegNetModel', 'RegNetPreTrainedModel', 'RemBertConfig', 'RemBertForCausalLM', 'RemBertForMaskedLM', 'RemBertForMultipleChoice', 'RemBertForQuestionAnswering', 'RemBertForSequenceClassification', 'RemBertForTokenClassification', 'RemBertLayer', 'RemBertModel', 'RemBertPreTrainedModel', 'RemBertTokenizer', 'RemBertTokenizerFast', 'RemoteTool', 'RepetitionPenaltyLogitsProcessor', 'ResNetBackbone', 'ResNetConfig', 'ResNetForImageClassification', 'ResNetModel', 'ResNetPreTrainedModel', 'RetriBertConfig', 'RetriBertModel', 'RetriBertPreTrainedModel', 'RetriBertTokenizer', 'RetriBertTokenizerFast', 'RoCBertConfig', 'RoCBertForCausalLM', 'RoCBertForMaskedLM', 'RoCBertForMultipleChoice', 'RoCBertForPreTraining', 'RoCBertForQuestionAnswering', 'RoCBertForSequenceClassification', 'RoCBertForTokenClassification', 'RoCBertLayer', 'RoCBertModel', 'RoCBertPreTrainedModel', 'RoCBertTokenizer', 'RoFormerConfig', 'RoFormerForCausalLM', 'RoFormerForMaskedLM', 'RoFormerForMultipleChoice', 'RoFormerForQuestionAnswering', 'RoFormerForSequenceClassification', 'RoFormerForTokenClassification', 'RoFormerLayer', 'RoFormerModel', 'RoFormerPreTrainedModel', 'RoFormerTokenizer', 'RoFormerTokenizerFast', 'RobertaConfig', 'RobertaForCausalLM', 'RobertaForMaskedLM', 'RobertaForMultipleChoice', 'RobertaForQuestionAnswering', 'RobertaForSequenceClassification', 'RobertaForTokenClassification', 'RobertaModel', 'RobertaPreLayerNormConfig', 'RobertaPreLayerNormForCausalLM', 'RobertaPreLayerNormForMaskedLM', 'RobertaPreLayerNormForMultipleChoice', 'RobertaPreLayerNormForQuestionAnswering', 'RobertaPreLayerNormForSequenceClassification', 'RobertaPreLayerNormForTokenClassification', 'RobertaPreLayerNormModel', 'RobertaPreLayerNormPreTrainedModel', 'RobertaPreTrainedModel', 'RobertaTokenizer', 'RobertaTokenizerFast', 'RwkvConfig', 'RwkvForCausalLM', 'RwkvModel', 'RwkvPreTrainedModel', 'SAM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SAM_PRETRAINED_MODEL_ARCHIVE_LIST', 'SEGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'SEWConfig', 'SEWDConfig', 'SEWDForCTC', 'SEWDForSequenceClassification', 'SEWDModel', 'SEWDPreTrainedModel', 'SEWForCTC', 'SEWForSequenceClassification', 'SEWModel', 'SEWPreTrainedModel', 'SEW_D_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEW_D_PRETRAINED_MODEL_ARCHIVE_LIST', 'SEW_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SEW_PRETRAINED_MODEL_ARCHIVE_LIST', 'SLOW_TO_FAST_CONVERTERS', 'SPEECHT5_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPEECHT5_PRETRAINED_HIFIGAN_CONFIG_ARCHIVE_MAP', 'SPEECHT5_PRETRAINED_MODEL_ARCHIVE_LIST', 'SPEECH_TO_TEXT_2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPEECH_TO_TEXT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPEECH_TO_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'SPIECE_UNDERLINE', 'SPLINTER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SPLINTER_PRETRAINED_MODEL_ARCHIVE_LIST', 'SQUEEZEBERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SQUEEZEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWIFTFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWIFTFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWIN2SR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWIN2SR_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWINV2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWINV2_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWIN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'SWITCH_TRANSFORMERS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'SWITCH_TRANSFORMERS_PRETRAINED_MODEL_ARCHIVE_LIST', 'SamConfig', 'SamImageProcessor', 'SamMaskDecoderConfig', 'SamModel', 'SamPreTrainedModel', 'SamProcessor', 'SamPromptEncoderConfig', 'SamVisionConfig', 'SchedulerType', 'SegformerConfig', 'SegformerDecodeHead', 'SegformerFeatureExtractor', 'SegformerForImageClassification', 'SegformerForSemanticSegmentation', 'SegformerImageProcessor', 'SegformerLayer', 'SegformerModel', 'SegformerPreTrainedModel', 'Seq2SeqTrainer', 'Seq2SeqTrainingArguments', 'SequenceBiasLogitsProcessor', 'SequenceFeatureExtractor', 'SingleSentenceClassificationProcessor', 'SpecialTokensMixin', 'Speech2Text2Config', 'Speech2Text2ForCausalLM', 'Speech2Text2PreTrainedModel', 'Speech2Text2Processor', 'Speech2Text2Tokenizer', 'Speech2TextConfig', 'Speech2TextFeatureExtractor', 'Speech2TextForConditionalGeneration', 'Speech2TextModel', 'Speech2TextPreTrainedModel', 'Speech2TextProcessor', 'Speech2TextTokenizer', 'SpeechEncoderDecoderConfig', 'SpeechEncoderDecoderModel', 'SpeechT5Config', 'SpeechT5FeatureExtractor', 'SpeechT5ForSpeechToSpeech', 'SpeechT5ForSpeechToText', 'SpeechT5ForTextToSpeech', 'SpeechT5HifiGan', 'SpeechT5HifiGanConfig', 'SpeechT5Model', 'SpeechT5PreTrainedModel', 'SpeechT5Processor', 'SpeechT5Tokenizer', 'SplinterConfig', 'SplinterForPreTraining', 'SplinterForQuestionAnswering', 'SplinterLayer', 'SplinterModel', 'SplinterPreTrainedModel', 'SplinterTokenizer', 'SplinterTokenizerFast', 'SquadDataTrainingArguments', 'SquadDataset', 'SquadExample', 'SquadFeatures', 'SquadV1Processor', 'SquadV2Processor', 'SqueezeBertConfig', 'SqueezeBertForMaskedLM', 'SqueezeBertForMultipleChoice', 'SqueezeBertForQuestionAnswering', 'SqueezeBertForSequenceClassification', 'SqueezeBertForTokenClassification', 'SqueezeBertModel', 'SqueezeBertModule', 'SqueezeBertPreTrainedModel', 'SqueezeBertTokenizer', 'SqueezeBertTokenizerFast', 'StoppingCriteria', 'StoppingCriteriaList', 'SummarizationPipeline', 'SwiftFormerConfig', 'SwiftFormerForImageClassification', 'SwiftFormerModel', 'SwiftFormerPreTrainedModel', 'Swin2SRConfig', 'Swin2SRForImageSuperResolution', 'Swin2SRImageProcessor', 'Swin2SRModel', 'Swin2SRPreTrainedModel', 'SwinBackbone', 'SwinConfig', 'SwinForImageClassification', 'SwinForMaskedImageModeling', 'SwinModel', 'SwinPreTrainedModel', 'Swinv2Config', 'Swinv2ForImageClassification', 'Swinv2ForMaskedImageModeling', 'Swinv2Model', 'Swinv2PreTrainedModel', 'SwitchTransformersConfig', 'SwitchTransformersEncoderModel', 'SwitchTransformersForConditionalGeneration', 'SwitchTransformersModel', 'SwitchTransformersPreTrainedModel', 'SwitchTransformersSparseMLP', 'SwitchTransformersTop1Router', 'T5Config', 'T5EncoderModel', 'T5ForConditionalGeneration', 'T5ForQuestionAnswering', 'T5Model', 'T5PreTrainedModel', 'T5Tokenizer', 'T5TokenizerFast', 'T5_PRETRAINED_CONFIG_ARCHIVE_MAP', 'T5_PRETRAINED_MODEL_ARCHIVE_LIST', 'TABLE_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TABLE_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TAPAS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TAPAS_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF2_WEIGHTS_NAME', 'TFAdaptiveEmbedding', 'TFAlbertForMaskedLM', 'TFAlbertForMultipleChoice', 'TFAlbertForPreTraining', 'TFAlbertForQuestionAnswering', 'TFAlbertForSequenceClassification', 'TFAlbertForTokenClassification', 'TFAlbertMainLayer', 'TFAlbertModel', 'TFAlbertPreTrainedModel', 'TFAutoModel', 'TFAutoModelForAudioClassification', 'TFAutoModelForCausalLM', 'TFAutoModelForDocumentQuestionAnswering', 'TFAutoModelForImageClassification', 'TFAutoModelForMaskGeneration', 'TFAutoModelForMaskedImageModeling', 'TFAutoModelForMaskedLM', 'TFAutoModelForMultipleChoice', 'TFAutoModelForNextSentencePrediction', 'TFAutoModelForPreTraining', 'TFAutoModelForQuestionAnswering', 'TFAutoModelForSemanticSegmentation', 'TFAutoModelForSeq2SeqLM', 'TFAutoModelForSequenceClassification', 'TFAutoModelForSpeechSeq2Seq', 'TFAutoModelForTableQuestionAnswering', 'TFAutoModelForTextEncoding', 'TFAutoModelForTokenClassification', 'TFAutoModelForVision2Seq', 'TFAutoModelForZeroShotImageClassification', 'TFAutoModelWithLMHead', 'TFBartForConditionalGeneration', 'TFBartForSequenceClassification', 'TFBartModel', 'TFBartPretrainedModel', 'TFBertEmbeddings', 'TFBertForMaskedLM', 'TFBertForMultipleChoice', 'TFBertForNextSentencePrediction', 'TFBertForPreTraining', 'TFBertForQuestionAnswering', 'TFBertForSequenceClassification', 'TFBertForTokenClassification', 'TFBertLMHeadModel', 'TFBertMainLayer', 'TFBertModel', 'TFBertPreTrainedModel', 'TFBertTokenizer', 'TFBlenderbotForConditionalGeneration', 'TFBlenderbotModel', 'TFBlenderbotPreTrainedModel', 'TFBlenderbotSmallForConditionalGeneration', 'TFBlenderbotSmallModel', 'TFBlenderbotSmallPreTrainedModel', 'TFBlipForConditionalGeneration', 'TFBlipForImageTextRetrieval', 'TFBlipForQuestionAnswering', 'TFBlipModel', 'TFBlipPreTrainedModel', 'TFBlipTextModel', 'TFBlipVisionModel', 'TFCLIPModel', 'TFCLIPPreTrainedModel', 'TFCLIPTextModel', 'TFCLIPVisionModel', 'TFCTRLForSequenceClassification', 'TFCTRLLMHeadModel', 'TFCTRLModel', 'TFCTRLPreTrainedModel', 'TFCamembertForCausalLM', 'TFCamembertForMaskedLM', 'TFCamembertForMultipleChoice', 'TFCamembertForQuestionAnswering', 'TFCamembertForSequenceClassification', 'TFCamembertForTokenClassification', 'TFCamembertModel', 'TFCamembertPreTrainedModel', 'TFConvBertForMaskedLM', 'TFConvBertForMultipleChoice', 'TFConvBertForQuestionAnswering', 'TFConvBertForSequenceClassification', 'TFConvBertForTokenClassification', 'TFConvBertLayer', 'TFConvBertModel', 'TFConvBertPreTrainedModel', 'TFConvNextForImageClassification', 'TFConvNextModel', 'TFConvNextPreTrainedModel', 'TFCvtForImageClassification', 'TFCvtModel', 'TFCvtPreTrainedModel', 'TFDPRContextEncoder', 'TFDPRPretrainedContextEncoder', 'TFDPRPretrainedQuestionEncoder', 'TFDPRPretrainedReader', 'TFDPRQuestionEncoder', 'TFDPRReader', 'TFData2VecVisionForImageClassification', 'TFData2VecVisionForSemanticSegmentation', 'TFData2VecVisionModel', 'TFData2VecVisionPreTrainedModel', 'TFDebertaForMaskedLM', 'TFDebertaForQuestionAnswering', 'TFDebertaForSequenceClassification', 'TFDebertaForTokenClassification', 'TFDebertaModel', 'TFDebertaPreTrainedModel', 'TFDebertaV2ForMaskedLM', 'TFDebertaV2ForQuestionAnswering', 'TFDebertaV2ForSequenceClassification', 'TFDebertaV2ForTokenClassification', 'TFDebertaV2Model', 'TFDebertaV2PreTrainedModel', 'TFDeiTForImageClassification', 'TFDeiTForImageClassificationWithTeacher', 'TFDeiTForMaskedImageModeling', 'TFDeiTModel', 'TFDeiTPreTrainedModel', 'TFDistilBertForMaskedLM', 'TFDistilBertForMultipleChoice', 'TFDistilBertForQuestionAnswering', 'TFDistilBertForSequenceClassification', 'TFDistilBertForTokenClassification', 'TFDistilBertMainLayer', 'TFDistilBertModel', 'TFDistilBertPreTrainedModel', 'TFEfficientFormerForImageClassification', 'TFEfficientFormerForImageClassificationWithTeacher', 'TFEfficientFormerModel', 'TFEfficientFormerPreTrainedModel', 'TFElectraForMaskedLM', 'TFElectraForMultipleChoice', 'TFElectraForPreTraining', 'TFElectraForQuestionAnswering', 'TFElectraForSequenceClassification', 'TFElectraForTokenClassification', 'TFElectraModel', 'TFElectraPreTrainedModel', 'TFEncoderDecoderModel', 'TFEsmForMaskedLM', 'TFEsmForSequenceClassification', 'TFEsmForTokenClassification', 'TFEsmModel', 'TFEsmPreTrainedModel', 'TFFlaubertForMultipleChoice', 'TFFlaubertForQuestionAnsweringSimple', 'TFFlaubertForSequenceClassification', 'TFFlaubertForTokenClassification', 'TFFlaubertModel', 'TFFlaubertPreTrainedModel', 'TFFlaubertWithLMHeadModel', 'TFForcedBOSTokenLogitsProcessor', 'TFForcedEOSTokenLogitsProcessor', 'TFFunnelBaseModel', 'TFFunnelForMaskedLM', 'TFFunnelForMultipleChoice', 'TFFunnelForPreTraining', 'TFFunnelForQuestionAnswering', 'TFFunnelForSequenceClassification', 'TFFunnelForTokenClassification', 'TFFunnelModel', 'TFFunnelPreTrainedModel', 'TFGPT2DoubleHeadsModel', 'TFGPT2ForSequenceClassification', 'TFGPT2LMHeadModel', 'TFGPT2MainLayer', 'TFGPT2Model', 'TFGPT2PreTrainedModel', 'TFGPT2Tokenizer', 'TFGPTJForCausalLM', 'TFGPTJForQuestionAnswering', 'TFGPTJForSequenceClassification', 'TFGPTJModel', 'TFGPTJPreTrainedModel', 'TFGenerationMixin', 'TFGroupViTModel', 'TFGroupViTPreTrainedModel', 'TFGroupViTTextModel', 'TFGroupViTVisionModel', 'TFHubertForCTC', 'TFHubertModel', 'TFHubertPreTrainedModel', 'TFLEDForConditionalGeneration', 'TFLEDModel', 'TFLEDPreTrainedModel', 'TFLayoutLMForMaskedLM', 'TFLayoutLMForQuestionAnswering', 'TFLayoutLMForSequenceClassification', 'TFLayoutLMForTokenClassification', 'TFLayoutLMMainLayer', 'TFLayoutLMModel', 'TFLayoutLMPreTrainedModel', 'TFLayoutLMv3ForQuestionAnswering', 'TFLayoutLMv3ForSequenceClassification', 'TFLayoutLMv3ForTokenClassification', 'TFLayoutLMv3Model', 'TFLayoutLMv3PreTrainedModel', 'TFLogitsProcessor', 'TFLogitsProcessorList', 'TFLogitsWarper', 'TFLongformerForMaskedLM', 'TFLongformerForMultipleChoice', 'TFLongformerForQuestionAnswering', 'TFLongformerForSequenceClassification', 'TFLongformerForTokenClassification', 'TFLongformerModel', 'TFLongformerPreTrainedModel', 'TFLongformerSelfAttention', 'TFLxmertForPreTraining', 'TFLxmertMainLayer', 'TFLxmertModel', 'TFLxmertPreTrainedModel', 'TFLxmertVisualFeatureEncoder', 'TFMBartForConditionalGeneration', 'TFMBartModel', 'TFMBartPreTrainedModel', 'TFMPNetForMaskedLM', 'TFMPNetForMultipleChoice', 'TFMPNetForQuestionAnswering', 'TFMPNetForSequenceClassification', 'TFMPNetForTokenClassification', 'TFMPNetMainLayer', 'TFMPNetModel', 'TFMPNetPreTrainedModel', 'TFMT5EncoderModel', 'TFMT5ForConditionalGeneration', 'TFMT5Model', 'TFMarianMTModel', 'TFMarianModel', 'TFMarianPreTrainedModel', 'TFMinLengthLogitsProcessor', 'TFMobileBertForMaskedLM', 'TFMobileBertForMultipleChoice', 'TFMobileBertForNextSentencePrediction', 'TFMobileBertForPreTraining', 'TFMobileBertForQuestionAnswering', 'TFMobileBertForSequenceClassification', 'TFMobileBertForTokenClassification', 'TFMobileBertMainLayer', 'TFMobileBertModel', 'TFMobileBertPreTrainedModel', 'TFMobileViTForImageClassification', 'TFMobileViTForSemanticSegmentation', 'TFMobileViTModel', 'TFMobileViTPreTrainedModel', 'TFNoBadWordsLogitsProcessor', 'TFNoRepeatNGramLogitsProcessor', 'TFOPTForCausalLM', 'TFOPTModel', 'TFOPTPreTrainedModel', 'TFOpenAIGPTDoubleHeadsModel', 'TFOpenAIGPTForSequenceClassification', 'TFOpenAIGPTLMHeadModel', 'TFOpenAIGPTMainLayer', 'TFOpenAIGPTModel', 'TFOpenAIGPTPreTrainedModel', 'TFPegasusForConditionalGeneration', 'TFPegasusModel', 'TFPegasusPreTrainedModel', 'TFPreTrainedModel', 'TFRagModel', 'TFRagPreTrainedModel', 'TFRagSequenceForGeneration', 'TFRagTokenForGeneration', 'TFRegNetForImageClassification', 'TFRegNetModel', 'TFRegNetPreTrainedModel', 'TFRemBertForCausalLM', 'TFRemBertForMaskedLM', 'TFRemBertForMultipleChoice', 'TFRemBertForQuestionAnswering', 'TFRemBertForSequenceClassification', 'TFRemBertForTokenClassification', 'TFRemBertLayer', 'TFRemBertModel', 'TFRemBertPreTrainedModel', 'TFRepetitionPenaltyLogitsProcessor', 'TFResNetForImageClassification', 'TFResNetModel', 'TFResNetPreTrainedModel', 'TFRoFormerForCausalLM', 'TFRoFormerForMaskedLM', 'TFRoFormerForMultipleChoice', 'TFRoFormerForQuestionAnswering', 'TFRoFormerForSequenceClassification', 'TFRoFormerForTokenClassification', 'TFRoFormerLayer', 'TFRoFormerModel', 'TFRoFormerPreTrainedModel', 'TFRobertaForCausalLM', 'TFRobertaForMaskedLM', 'TFRobertaForMultipleChoice', 'TFRobertaForQuestionAnswering', 'TFRobertaForSequenceClassification', 'TFRobertaForTokenClassification', 'TFRobertaMainLayer', 'TFRobertaModel', 'TFRobertaPreLayerNormForCausalLM', 'TFRobertaPreLayerNormForMaskedLM', 'TFRobertaPreLayerNormForMultipleChoice', 'TFRobertaPreLayerNormForQuestionAnswering', 'TFRobertaPreLayerNormForSequenceClassification', 'TFRobertaPreLayerNormForTokenClassification', 'TFRobertaPreLayerNormMainLayer', 'TFRobertaPreLayerNormModel', 'TFRobertaPreLayerNormPreTrainedModel', 'TFRobertaPreTrainedModel', 'TFSamModel', 'TFSamPreTrainedModel', 'TFSegformerDecodeHead', 'TFSegformerForImageClassification', 'TFSegformerForSemanticSegmentation', 'TFSegformerModel', 'TFSegformerPreTrainedModel', 'TFSequenceSummary', 'TFSharedEmbeddings', 'TFSpeech2TextForConditionalGeneration', 'TFSpeech2TextModel', 'TFSpeech2TextPreTrainedModel', 'TFSwinForImageClassification', 'TFSwinForMaskedImageModeling', 'TFSwinModel', 'TFSwinPreTrainedModel', 'TFT5EncoderModel', 'TFT5ForConditionalGeneration', 'TFT5Model', 'TFT5PreTrainedModel', 'TFTapasForMaskedLM', 'TFTapasForQuestionAnswering', 'TFTapasForSequenceClassification', 'TFTapasModel', 'TFTapasPreTrainedModel', 'TFTemperatureLogitsWarper', 'TFTopKLogitsWarper', 'TFTopPLogitsWarper', 'TFTrainer', 'TFTrainingArguments', 'TFTransfoXLForSequenceClassification', 'TFTransfoXLLMHeadModel', 'TFTransfoXLMainLayer', 'TFTransfoXLModel', 'TFTransfoXLPreTrainedModel', 'TFViTForImageClassification', 'TFViTMAEForPreTraining', 'TFViTMAEModel', 'TFViTMAEPreTrainedModel', 'TFViTModel', 'TFViTPreTrainedModel', 'TFVisionEncoderDecoderModel', 'TFVisionTextDualEncoderModel', 'TFWav2Vec2ForCTC', 'TFWav2Vec2ForSequenceClassification', 'TFWav2Vec2Model', 'TFWav2Vec2PreTrainedModel', 'TFWhisperForConditionalGeneration', 'TFWhisperModel', 'TFWhisperPreTrainedModel', 'TFXGLMForCausalLM', 'TFXGLMModel', 'TFXGLMPreTrainedModel', 'TFXLMForMultipleChoice', 'TFXLMForQuestionAnsweringSimple', 'TFXLMForSequenceClassification', 'TFXLMForTokenClassification', 'TFXLMMainLayer', 'TFXLMModel', 'TFXLMPreTrainedModel', 'TFXLMRobertaForCausalLM', 'TFXLMRobertaForMaskedLM', 'TFXLMRobertaForMultipleChoice', 'TFXLMRobertaForQuestionAnswering', 'TFXLMRobertaForSequenceClassification', 'TFXLMRobertaForTokenClassification', 'TFXLMRobertaModel', 'TFXLMRobertaPreTrainedModel', 'TFXLMWithLMHeadModel', 'TFXLNetForMultipleChoice', 'TFXLNetForQuestionAnsweringSimple', 'TFXLNetForSequenceClassification', 'TFXLNetForTokenClassification', 'TFXLNetLMHeadModel', 'TFXLNetMainLayer', 'TFXLNetModel', 'TFXLNetPreTrainedModel', 'TF_ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_BLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CONVBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CTRL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_CVT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEBERTA_V2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DEIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_CONTEXT_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_QUESTION_ENCODER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_DPR_READER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_EFFICIENTFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ELECTRA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_FLAUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_FUNNEL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_GPT2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_GROUPVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_HUBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LAYOUTLMV3_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LONGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_LXMERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MOBILEBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MOBILEVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_CAUSAL_LM_MAPPING', 'TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING', 'TF_MODEL_FOR_MASKED_LM_MAPPING', 'TF_MODEL_FOR_MASK_GENERATION_MAPPING', 'TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'TF_MODEL_FOR_PRETRAINING_MAPPING', 'TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING', 'TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_TEXT_ENCODING_MAPPING', 'TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_VISION_2_SEQ_MAPPING', 'TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING', 'TF_MODEL_MAPPING', 'TF_MODEL_WITH_LM_HEAD_MAPPING', 'TF_MPNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_REGNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_REMBERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_RESNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROBERTA_PRELAYERNORM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_ROFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SAM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SEGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SPEECH_TO_TEXT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_SWIN_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_T5_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_TAPAS_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_WAV_2_VEC_2_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_WEIGHTS_NAME', 'TF_WHISPER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XGLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'TF_XLNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'TIMESFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TIMESFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TIME_SERIES_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TIME_SERIES_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TOKENIZER_MAPPING', 'TRAJECTORY_TRANSFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TRAJECTORY_TRANSFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'TRANSFORMERS_CACHE', 'TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'TROCR_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TROCR_PRETRAINED_MODEL_ARCHIVE_LIST', 'TVLT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'TVLT_PRETRAINED_MODEL_ARCHIVE_LIST', 'TableQuestionAnsweringPipeline', 'TableTransformerConfig', 'TableTransformerForObjectDetection', 'TableTransformerModel', 'TableTransformerPreTrainedModel', 'TapasConfig', 'TapasForMaskedLM', 'TapasForQuestionAnswering', 'TapasForSequenceClassification', 'TapasModel', 'TapasPreTrainedModel', 'TapasTokenizer', 'TapexTokenizer', 'TemperatureLogitsWarper', 'TensorFlowBenchmark', 'TensorFlowBenchmarkArguments', 'TensorType', 'Text2TextGenerationPipeline', 'TextClassificationPipeline', 'TextDataset', 'TextDatasetForNextSentencePrediction', 'TextGenerationPipeline', 'TextIteratorStreamer', 'TextStreamer', 'TimeSeriesTransformerConfig', 'TimeSeriesTransformerForPrediction', 'TimeSeriesTransformerModel', 'TimeSeriesTransformerPreTrainedModel', 'TimesformerConfig', 'TimesformerForVideoClassification', 'TimesformerModel', 'TimesformerPreTrainedModel', 'TimmBackbone', 'TimmBackboneConfig', 'TokenClassificationPipeline', 'TokenSpan', 'Tool', 'TopKLogitsWarper', 'TopPLogitsWarper', 'TrOCRConfig', 'TrOCRForCausalLM', 'TrOCRPreTrainedModel', 'TrOCRProcessor', 'Trainer', 'TrainerCallback', 'TrainerControl', 'TrainerState', 'TrainingArguments', 'TrajectoryTransformerConfig', 'TrajectoryTransformerModel', 'TrajectoryTransformerPreTrainedModel', 'TransfoXLConfig', 'TransfoXLCorpus', 'TransfoXLForSequenceClassification', 'TransfoXLLMHeadModel', 'TransfoXLModel', 'TransfoXLPreTrainedModel', 'TransfoXLTokenizer', 'TranslationPipeline', 'TvltConfig', 'TvltFeatureExtractor', 'TvltForAudioVisualClassification', 'TvltForPreTraining', 'TvltImageProcessor', 'TvltModel', 'TvltPreTrainedModel', 'TvltProcessor', 'TypicalLogitsWarper', 'UMT5Config', 'UMT5EncoderModel', 'UMT5ForConditionalGeneration', 'UMT5ForQuestionAnswering', 'UMT5Model', 'UMT5PreTrainedModel', 'UNISPEECH_PRETRAINED_CONFIG_ARCHIVE_MAP', 'UNISPEECH_PRETRAINED_MODEL_ARCHIVE_LIST', 'UNISPEECH_SAT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'UNISPEECH_SAT_PRETRAINED_MODEL_ARCHIVE_LIST', 'UniSpeechConfig', 'UniSpeechForCTC', 'UniSpeechForPreTraining', 'UniSpeechForSequenceClassification', 'UniSpeechModel', 'UniSpeechPreTrainedModel', 'UniSpeechSatConfig', 'UniSpeechSatForAudioFrameClassification', 'UniSpeechSatForCTC', 'UniSpeechSatForPreTraining', 'UniSpeechSatForSequenceClassification', 'UniSpeechSatForXVector', 'UniSpeechSatModel', 'UniSpeechSatPreTrainedModel', 'UperNetConfig', 'UperNetForSemanticSegmentation', 'UperNetPreTrainedModel', 'VAN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VAN_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIDEOMAE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIDEOMAE_PRETRAINED_MODEL_ARCHIVE_LIST', 'VILT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VILT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VISUAL_BERT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VISUAL_BERT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_HYBRID_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_HYBRID_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_MAE_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_MAE_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_MSN_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_MSN_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VIVIT_PRETRAINED_CONFIG_ARCHIVE_MAP', 'VIVIT_PRETRAINED_MODEL_ARCHIVE_LIST', 'VanConfig', 'VanForImageClassification', 'VanModel', 'VanPreTrainedModel', 'ViTConfig', 'ViTFeatureExtractor', 'ViTForImageClassification', 'ViTForMaskedImageModeling', 'ViTHybridConfig', 'ViTHybridForImageClassification', 'ViTHybridImageProcessor', 'ViTHybridModel', 'ViTHybridPreTrainedModel', 'ViTImageProcessor', 'ViTMAEConfig', 'ViTMAEForPreTraining', 'ViTMAELayer', 'ViTMAEModel', 'ViTMAEPreTrainedModel', 'ViTMSNConfig', 'ViTMSNForImageClassification', 'ViTMSNModel', 'ViTMSNPreTrainedModel', 'ViTModel', 'ViTPreTrainedModel', 'VideoClassificationPipeline', 'VideoMAEConfig', 'VideoMAEFeatureExtractor', 'VideoMAEForPreTraining', 'VideoMAEForVideoClassification', 'VideoMAEImageProcessor', 'VideoMAEModel', 'VideoMAEPreTrainedModel', 'ViltConfig', 'ViltFeatureExtractor', 'ViltForImageAndTextRetrieval', 'ViltForImagesAndTextClassification', 'ViltForMaskedLM', 'ViltForQuestionAnswering', 'ViltForTokenClassification', 'ViltImageProcessor', 'ViltLayer', 'ViltModel', 'ViltPreTrainedModel', 'ViltProcessor', 'VisionEncoderDecoderConfig', 'VisionEncoderDecoderModel', 'VisionTextDualEncoderConfig', 'VisionTextDualEncoderModel', 'VisionTextDualEncoderProcessor', 'VisualBertConfig', 'VisualBertForMultipleChoice', 'VisualBertForPreTraining', 'VisualBertForQuestionAnswering', 'VisualBertForRegionToPhraseAlignment', 'VisualBertForVisualReasoning', 'VisualBertLayer', 'VisualBertModel', 'VisualBertPreTrainedModel', 'VisualQuestionAnsweringPipeline', 'VivitConfig', 'VivitForVideoClassification', 'VivitImageProcessor', 'VivitModel', 'VivitPreTrainedModel', 'WAV2VEC2_CONFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAV2VEC2_CONFORMER_PRETRAINED_MODEL_ARCHIVE_LIST', 'WAVLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAVLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'WAV_2_VEC_2_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WAV_2_VEC_2_PRETRAINED_MODEL_ARCHIVE_LIST', 'WEIGHTS_NAME', 'WHISPER_PRETRAINED_CONFIG_ARCHIVE_MAP', 'WHISPER_PRETRAINED_MODEL_ARCHIVE_LIST', 'WarmUp', 'Wav2Vec2CTCTokenizer', 'Wav2Vec2Config', 'Wav2Vec2ConformerConfig', 'Wav2Vec2ConformerForAudioFrameClassification', 'Wav2Vec2ConformerForCTC', 'Wav2Vec2ConformerForPreTraining', 'Wav2Vec2ConformerForSequenceClassification', 'Wav2Vec2ConformerForXVector', 'Wav2Vec2ConformerModel', 'Wav2Vec2ConformerPreTrainedModel', 'Wav2Vec2FeatureExtractor', 'Wav2Vec2ForAudioFrameClassification', 'Wav2Vec2ForCTC', 'Wav2Vec2ForMaskedLM', 'Wav2Vec2ForPreTraining', 'Wav2Vec2ForSequenceClassification', 'Wav2Vec2ForXVector', 'Wav2Vec2Model', 'Wav2Vec2PhonemeCTCTokenizer', 'Wav2Vec2PreTrainedModel', 'Wav2Vec2Processor', 'Wav2Vec2ProcessorWithLM', 'Wav2Vec2Tokenizer', 'WavLMConfig', 'WavLMForAudioFrameClassification', 'WavLMForCTC', 'WavLMForSequenceClassification', 'WavLMForXVector', 'WavLMModel', 'WavLMPreTrainedModel', 'WhisperConfig', 'WhisperFeatureExtractor', 'WhisperForAudioClassification', 'WhisperForConditionalGeneration', 'WhisperModel', 'WhisperPreTrainedModel', 'WhisperProcessor', 'WhisperTokenizer', 'WhisperTokenizerFast', 'WordpieceTokenizer', 'XCLIPConfig', 'XCLIPModel', 'XCLIPPreTrainedModel', 'XCLIPProcessor', 'XCLIPTextConfig', 'XCLIPTextModel', 'XCLIPVisionConfig', 'XCLIPVisionModel', 'XCLIP_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XCLIP_PRETRAINED_MODEL_ARCHIVE_LIST', 'XGLMConfig', 'XGLMForCausalLM', 'XGLMModel', 'XGLMPreTrainedModel', 'XGLMTokenizer', 'XGLMTokenizerFast', 'XGLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XGLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLMConfig', 'XLMForMultipleChoice', 'XLMForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMForSequenceClassification', 'XLMForTokenClassification', 'XLMModel', 'XLMPreTrainedModel', 'XLMProphetNetConfig', 'XLMProphetNetDecoder', 'XLMProphetNetEncoder', 'XLMProphetNetForCausalLM', 'XLMProphetNetForConditionalGeneration', 'XLMProphetNetModel', 'XLMProphetNetPreTrainedModel', 'XLMProphetNetTokenizer', 'XLMRobertaConfig', 'XLMRobertaForCausalLM', 'XLMRobertaForMaskedLM', 'XLMRobertaForMultipleChoice', 'XLMRobertaForQuestionAnswering', 'XLMRobertaForSequenceClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaModel', 'XLMRobertaPreTrainedModel', 'XLMRobertaTokenizer', 'XLMRobertaTokenizerFast', 'XLMRobertaXLConfig', 'XLMRobertaXLForCausalLM', 'XLMRobertaXLForMaskedLM', 'XLMRobertaXLForMultipleChoice', 'XLMRobertaXLForQuestionAnswering', 'XLMRobertaXLForSequenceClassification', 'XLMRobertaXLForTokenClassification', 'XLMRobertaXLModel', 'XLMRobertaXLPreTrainedModel', 'XLMTokenizer', 'XLMWithLMHeadModel', 'XLM_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_PROPHETNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_PROPHETNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLM_ROBERTA_XL_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLM_ROBERTA_XL_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XLNET_PRETRAINED_MODEL_ARCHIVE_LIST', 'XLNetConfig', 'XLNetForMultipleChoice', 'XLNetForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XLNetForSequenceClassification', 'XLNetForTokenClassification', 'XLNetLMHeadModel', 'XLNetModel', 'XLNetPreTrainedModel', 'XLNetTokenizer', 'XLNetTokenizerFast', 'XMOD_PRETRAINED_CONFIG_ARCHIVE_MAP', 'XMOD_PRETRAINED_MODEL_ARCHIVE_LIST', 'XmodConfig', 'XmodForCausalLM', 'XmodForMaskedLM', 'XmodForMultipleChoice', 'XmodForQuestionAnswering', 'XmodForSequenceClassification', 'XmodForTokenClassification', 'XmodModel', 'XmodPreTrainedModel', 'YOLOS_PRETRAINED_CONFIG_ARCHIVE_MAP', 'YOLOS_PRETRAINED_MODEL_ARCHIVE_LIST', 'YOSO_PRETRAINED_CONFIG_ARCHIVE_MAP', 'YOSO_PRETRAINED_MODEL_ARCHIVE_LIST', 'YolosConfig', 'YolosFeatureExtractor', 'YolosForObjectDetection', 'YolosImageProcessor', 'YolosModel', 'YolosPreTrainedModel', 'YosoConfig', 'YosoForMaskedLM', 'YosoForMultipleChoice', 'YosoForQuestionAnswering', 'YosoForSequenceClassification', 'YosoForTokenClassification', 'YosoLayer', 'YosoModel', 'YosoPreTrainedModel', 'ZeroShotAudioClassificationPipeline', 'ZeroShotClassificationPipeline', 'ZeroShotImageClassificationPipeline', 'ZeroShotObjectDetectionPipeline', '__all__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_class_to_module', '_import_structure', '_modules', '_name', '_objects', 'activations', 'activations_tf', 'add_end_docstrings', 'add_start_docstrings', 'apply_chunking_to_forward', 'audio_utils', 'benchmark', 'benchmark.benchmark', 'benchmark.benchmark_args', 'benchmark.benchmark_args_tf', 'benchmark.benchmark_tf', 'commands', 'configuration_utils', 'convert_graph_to_onnx', 'convert_slow_tokenizer', 'convert_slow_tokenizers_checkpoints_to_fast', 'convert_tf_hub_seq_to_seq_bert_to_pytorch', 'convert_tf_weight_name_to_pt_weight_name', 'create_optimizer', 'data', 'data.data_collator', 'data.datasets', 'data.metrics', 'data.processors', 'debug_utils', 'deepspeed', 'default_data_collator', 'dependency_versions_check', 'dependency_versions_table', 'dynamic_module_utils', 'enable_full_determinism', 'feature_extraction_sequence_utils', 'feature_extraction_utils', 'file_utils', 'generation', 'generation_flax_utils', 'generation_tf_utils', 'generation_utils', 'get_constant_schedule', 'get_constant_schedule_with_warmup', 'get_cosine_schedule_with_warmup', 'get_cosine_with_hard_restarts_schedule_with_warmup', 'get_inverse_sqrt_schedule', 'get_linear_schedule_with_warmup', 'get_polynomial_decay_schedule_with_warmup', 'get_scheduler', 'glue_compute_metrics', 'glue_convert_examples_to_features', 'glue_output_modes', 'glue_processors', 'glue_tasks_num_labels', 'hf_argparser', 'hyperparameter_search', 'image_processing_utils', 'image_transforms', 'image_utils', 'integrations', 'is_apex_available', 'is_bitsandbytes_available', 'is_clearml_available', 'is_comet_available', 'is_datasets_available', 'is_decord_available', 'is_faiss_available', 'is_flax_available', 'is_keras_nlp_available', 'is_neptune_available', 'is_optuna_available', 'is_phonemizer_available', 'is_psutil_available', 'is_py3nvml_available', 'is_pyctcdecode_available', 'is_ray_available', 'is_ray_tune_available', 'is_safetensors_available', 'is_scipy_available', 'is_sentencepiece_available', 'is_sigopt_available', 'is_sklearn_available', 'is_speech_available', 'is_tensorboard_available', 'is_tensorflow_text_available', 'is_tf_available', 'is_timm_available', 'is_tokenizers_available', 'is_torch_available', 'is_torch_neuroncore_available', 'is_torch_tpu_available', 'is_torchvision_available', 'is_vision_available', 'is_wandb_available', 'keras_callbacks', 'launch_gradio_demo', 'load_pytorch_checkpoint_in_tf2_model', 'load_pytorch_model_in_tf2_model', 'load_pytorch_weights_in_tf2_model', 'load_tf2_checkpoint_in_pytorch_model', 'load_tf2_model_in_pytorch_model', 'load_tf2_weights_in_pytorch_model', 'load_tf_weights_in_albert', 'load_tf_weights_in_bert', 'load_tf_weights_in_bert_generation', 'load_tf_weights_in_big_bird', 'load_tf_weights_in_canine', 'load_tf_weights_in_convbert', 'load_tf_weights_in_electra', 'load_tf_weights_in_funnel', 'load_tf_weights_in_gpt2', 'load_tf_weights_in_gpt_neo', 'load_tf_weights_in_imagegpt', 'load_tf_weights_in_mobilebert', 'load_tf_weights_in_mobilenet_v1', 'load_tf_weights_in_mobilenet_v2', 'load_tf_weights_in_openai_gpt', 'load_tf_weights_in_qdqbert', 'load_tf_weights_in_realm', 'load_tf_weights_in_rembert', 'load_tf_weights_in_roc_bert', 'load_tf_weights_in_roformer', 'load_tf_weights_in_t5', 'load_tf_weights_in_tapas', 'load_tf_weights_in_transfo_xl', 'load_tf_weights_in_xlnet', 'load_tool', 'logging', 'modelcard', 'modeling_flax_outputs', 'modeling_flax_utils', 'modeling_outputs', 'modeling_tf_outputs', 'modeling_tf_pytorch_utils', 'modeling_tf_utils', 'modeling_utils', 'models', 'models.albert', 'models.align', 'models.altclip', 'models.audio_spectrogram_transformer', 'models.auto', 'models.autoformer', 'models.bark', 'models.bart', 'models.barthez', 'models.bartpho', 'models.beit', 'models.bert', 'models.bert_generation', 'models.bert_japanese', 'models.bertweet', 'models.big_bird', 'models.bigbird_pegasus', 'models.biogpt', 'models.bit', 'models.blenderbot', 'models.blenderbot_small', 'models.blip', 'models.blip_2', 'models.bloom', 'models.bridgetower', 'models.byt5', 'models.camembert', 'models.canine', 'models.chinese_clip', 'models.clap', 'models.clip', 'models.clipseg', 'models.codegen', 'models.conditional_detr', 'models.convbert', 'models.convnext', 'models.convnextv2', 'models.cpm', 'models.cpmant', 'models.ctrl', 'models.cvt', 'models.data2vec', 'models.deberta', 'models.deberta_v2', 'models.decision_transformer', 'models.deformable_detr', 'models.deit', 'models.deprecated', 'models.deprecated.bort', 'models.deprecated.mctct', 'models.deprecated.mmbt', 'models.deprecated.retribert', 'models.deprecated.tapex', 'models.deprecated.trajectory_transformer', 'models.deprecated.van', 'models.deta', 'models.detr', 'models.dialogpt', 'models.dinat', 'models.distilbert', 'models.dit', 'models.donut', 'models.dpr', 'models.dpt', 'models.efficientformer', 'models.efficientnet', 'models.electra', 'models.encodec', 'models.encoder_decoder', 'models.ernie', 'models.ernie_m', 'models.esm', 'models.falcon', 'models.flaubert', 'models.flava', 'models.fnet', 'models.focalnet', 'models.fsmt', 'models.funnel', 'models.git', 'models.glpn', 'models.gpt2', 'models.gpt_bigcode', 'models.gpt_neo', 'models.gpt_neox', 'models.gpt_neox_japanese', 'models.gpt_sw3', 'models.gptj', 'models.gptsan_japanese', 'models.graphormer', 'models.groupvit', 'models.herbert', 'models.hubert', 'models.ibert', 'models.imagegpt', 'models.informer', 'models.instructblip', 'models.jukebox', 'models.layoutlm', 'models.layoutlmv2', 'models.layoutlmv3', 'models.layoutxlm', 'models.led', 'models.levit', 'models.lilt', 'models.llama', 'models.longformer', 'models.longt5', 'models.luke', 'models.lxmert', 'models.m2m_100', 'models.marian', 'models.markuplm', 'models.mask2former', 'models.maskformer', 'models.mbart', 'models.mbart50', 'models.mega', 'models.megatron_bert', 'models.megatron_gpt2', 'models.mgp_str', 'models.mluke', 'models.mobilebert', 'models.mobilenet_v1', 'models.mobilenet_v2', 'models.mobilevit', 'models.mobilevitv2', 'models.mpnet', 'models.mra', 'models.mt5', 'models.musicgen', 'models.mvp', 'models.nat', 'models.nezha', 'models.nllb', 'models.nllb_moe', 'models.nystromformer', 'models.oneformer', 'models.open_llama', 'models.openai', 'models.opt', 'models.owlvit', 'models.pegasus', 'models.pegasus_x', 'models.perceiver', 'models.phobert', 'models.pix2struct', 'models.plbart', 'models.poolformer', 'models.prophetnet', 'models.qdqbert', 'models.rag', 'models.realm', 'models.reformer', 'models.regnet', 'models.rembert', 'models.resnet', 'models.roberta', 'models.roberta_prelayernorm', 'models.roc_bert', 'models.roformer', 'models.rwkv', 'models.sam', 'models.segformer', 'models.sew', 'models.sew_d', 'models.speech_encoder_decoder', 'models.speech_to_text', 'models.speech_to_text_2', 'models.speecht5', 'models.splinter', 'models.squeezebert', 'models.swiftformer', 'models.swin', 'models.swin2sr', 'models.swinv2', 'models.switch_transformers', 'models.t5', 'models.table_transformer', 'models.tapas', 'models.time_series_transformer', 'models.timesformer', 'models.timm_backbone', 'models.transfo_xl', 'models.trocr', 'models.tvlt', 'models.umt5', 'models.unispeech', 'models.unispeech_sat', 'models.upernet', 'models.videomae', 'models.vilt', 'models.vision_encoder_decoder', 'models.vision_text_dual_encoder', 'models.visual_bert', 'models.vit', 'models.vit_hybrid', 'models.vit_mae', 'models.vit_msn', 'models.vivit', 'models.wav2vec2', 'models.wav2vec2_conformer', 'models.wav2vec2_phoneme', 'models.wav2vec2_with_lm', 'models.wavlm', 'models.whisper', 'models.x_clip', 'models.xglm', 'models.xlm', 'models.xlm_prophetnet', 'models.xlm_roberta', 'models.xlm_roberta_xl', 'models.xlnet', 'models.xmod', 'models.yolos', 'models.yoso', 'onnx', 'optimization', 'optimization_tf', 'pipeline', 'pipelines', 'processing_utils', 'prune_layer', 'pytorch_utils', 'requires_backends', 'sagemaker', 'set_seed', 'shape_list', 'squad_convert_examples_to_features', 'testing_utils', 'tf_top_k_top_p_filtering', 'tf_utils', 'time_series_utils', 'tokenization_utils', 'tokenization_utils_base', 'tokenization_utils_fast', 'tools', 'top_k_top_p_filtering', 'torch_distributed_zero_first', 'trainer', 'trainer_callback', 'trainer_pt_utils', 'trainer_seq2seq', 'trainer_tf', 'trainer_utils', 'training_args', 'training_args_seq2seq', 'training_args_tf', 'utils', 'utils.bitsandbytes', 'utils.dummy_keras_nlp_objects', 'utils.dummy_tensorflow_text_objects', 'utils.quantization_config', 'xnli_compute_metrics', 'xnli_output_modes', 'xnli_processors', 'xnli_tasks_num_labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VisionEncoderDecoderConfig"
      ],
      "metadata": {
        "id": "o--M4oZ4iU0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cc5ccb-f915-4f2e-8af2-aca134460ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.vision_encoder_decoder.configuration_vision_encoder_decoder.VisionEncoderDecoderConfig"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(VisionEncoderDecoderConfig))"
      ],
      "metadata": {
        "id": "AifIi8kmYnHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74dadce-6f24-49c3-d488-c1fc4d3c8dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_create_repo', '_dict_from_json_file', '_get_config_dict', '_get_files_timestamps', '_set_token_in_kwargs', '_upload_modified_files', 'attribute_map', 'dict_torch_dtype_to_str', 'from_dict', 'from_encoder_decoder_configs', 'from_json_file', 'from_pretrained', 'get_config_dict', 'is_composition', 'model_type', 'name_or_path', 'num_labels', 'push_to_hub', 'register_for_auto_class', 'save_pretrained', 'to_dict', 'to_diff_dict', 'to_json_file', 'to_json_string', 'update', 'update_from_string', 'use_return_dict']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of parameter acceptance in VEDC\n",
        "params_vedc = inspect.signature(VisionEncoderDecoderConfig).parameters\n",
        "param_names_vedc = [param for param in params_vedc.keys()]\n",
        "\n",
        "param_names_vedc"
      ],
      "metadata": {
        "id": "ueuaVwNUa6iK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea2a30b-8b1e-4554-c059-31891d766479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kwargs']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(VisionEncoderDecoderModel))"
      ],
      "metadata": {
        "id": "TB360DEFYnEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7484449-cfe9-49c8-df65-d4ab6d248734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_auto_class', '_backward_compatibility_gradient_checkpointing', '_call_impl', '_convert_head_mask_to_5d', '_create_repo', '_expand_inputs_for_generation', '_extract_past_from_model_output', '_from_config', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_decoder_start_token_id', '_get_files_timestamps', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_initialize_weights', '_keep_in_fp32_modules', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_maybe_initialize_input_ids_for_generation', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_named_members', '_no_split_modules', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_model_inputs', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_token_embeddings', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_skip_keys_device_placement', '_slow_forward', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_tied_weights_keys', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_model_class', '_validate_model_kwargs', '_version', 'add_memory_hooks', 'add_module', 'adjust_logits_during_generation', 'apply', 'assisted_decoding', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search', 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'children', 'compute_transition_scores', 'config_class', 'constrained_beam_search', 'contrastive_search', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'device', 'disable_input_require_grads', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'enable_input_require_grads', 'estimate_tokens', 'eval', 'extra_repr', 'float', 'floating_point_ops', 'forward', 'framework', 'from_encoder_decoder_pretrained', 'from_pretrained', 'generate', 'get_buffer', 'get_decoder', 'get_encoder', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights', 'invert_attention_mask', 'ipu', 'is_gradient_checkpointing', 'is_parallelizable', 'load_state_dict', 'main_input_name', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_decoder_input_ids_from_labels', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'reverse_bettertransformer', 'sample', 'save_pretrained', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'type', 'warn_if_padding_and_no_attention_mask', 'xpu', 'zero_grad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VisionEncoderDecoderModel"
      ],
      "metadata": {
        "id": "B5ZTQkemsIAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19907cee-500c-4edb-d422-60899cc020c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.vision_encoder_decoder.modeling_vision_encoder_decoder.VisionEncoderDecoderModel"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_vedm = inspect.signature(VisionEncoderDecoderModel).parameters\n",
        "param_names_vedm = [param for param in params_vedm.keys()]\n",
        "\n",
        "param_names_vedm"
      ],
      "metadata": {
        "id": "hXk1Hfqka6fh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e97ad81-40b6-4b51-f2b7-4db0e0e84f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['config', 'encoder', 'decoder']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Difference between VisionEncoderDecoderConfig and VisionEncoderDecoderModel:\n",
        "- VisionEncoderDecoderConfig and VisionEncoderDecoderModel are two classes from the transformers library that are used for computer vision tasks, specifically for image captioning and object detection.\n",
        "\n",
        "- VisionEncoderDecoderConfig is a configuration class that defines the hyperparameters for a VisionEncoderDecoderModel. It contains attributes such as the number of layers, hidden size, attention heads, dropout rate, and activation function. You can use this class to define a specific configuration for your VisionEncoderDecoderModel.\n",
        "\n",
        "- VisionEncoderDecoderModel is a PyTorch module that implements an encoder-decoder architecture for image captioning and object detection. It consists of two main components: an image encoder and a text decoder. The image encoder is a pre-trained convolutional neural network that extracts image features, while the text decoder is a transformer that generates a sequence of tokens based on the input image features.\n",
        "\n",
        "- In summary, VisionEncoderDecoderConfig is a configuration class that defines the hyperparameters for a VisionEncoderDecoderModel, while VisionEncoderDecoderModel is a PyTorch module that implements an encoder-decoder architecture for computer vision tasks."
      ],
      "metadata": {
        "id": "jX6j0jM3d5Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Configuaration class and files:\n",
        "In the context of machine learning models, a configuration class is a Python class that contains a set of hyperparameters or settings that define how a model should behave during training and inference. A configuration class typically includes parameters such as learning rate, number of epochs, optimizer, activation function, etc.\n",
        "\n",
        "A config file is a file that stores the values of these hyperparameters in a specific format, such as YAML, JSON or INI. Config files are used to easily customize the behavior of a model without changing its code. They allow users to experiment with different hyperparameters, compare different configurations, and reproduce experimental results.\n",
        "\n",
        "The need for configuration classes and files is driven by the fact that machine learning models often have many hyperparameters that need to be tuned in order to obtain good performance. Manually changing these hyperparameters in code can be time-consuming, error-prone, and require recompiling the model. Config files provide an efficient way to change the hyperparameters without modifying the code, which makes it easier to experiment with different configurations and evaluate their performance.\n",
        "\n",
        "Additionally, configuration files enable the reuse of the same model architecture with different sets of hyperparameters. For example, you can train the same model architecture for a specific task with different learning rates, optimizer, and batch sizes by using different configuration files. This saves time and reduces code duplication.\n",
        "\n",
        "In summary, configuration classes and files are used to store the hyperparameters and settings of a machine learning model, allowing users to customize its behavior without modifying the code. They make it easier to experiment with different hyperparameters, compare different configurations, and reproduce experimental results."
      ],
      "metadata": {
        "id": "WDeDWxvso9si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q: When to use configuaration class and config file also at which situation?\n",
        "A configuration class and a config file are separate concepts, although they are often used together in machine learning frameworks and libraries.\n",
        "\n",
        "A configuration class defines a set of hyperparameters or settings that determine the behavior of a machine learning model during training and inference. It is usually defined as a Python class and can be used to create instances of a model with specific hyperparameters.\n",
        "\n",
        "A config file, on the other hand, is a file that stores the hyperparameters and settings for a machine learning model in a specific format such as YAML, JSON or INI. It can be used to load the hyperparameters and settings into a model and change its behavior without modifying the code.\n",
        "\n",
        "The choice of whether to use a configuration class or a config file depends on the requirements of the machine learning project and the framework being used. In some cases, a configuration class may be more suitable when the hyperparameters are specific to the implementation of a particular model or architecture. In other cases, a config file may be more flexible when the hyperparameters need to be changed often or when they are shared across different models or architectures.\n",
        "\n",
        "In general, configuration classes are more tightly coupled with the implementation of a particular model or architecture, while config files are more loosely coupled and provide more flexibility to customize the hyperparameters and settings of a machine learning model."
      ],
      "metadata": {
        "id": "dhOgfQZSs53M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comment:\n",
        "- 'max_length' refers to the maximum length of the generated text, which is an important hyperparameter in text generation tasks. It specifies the maximum number of tokens that the decoder can generate for each input image. In the example you provided, max_length is set to 128, which means that the decoder can generate a text with a maximum length of 128 tokens.\n",
        "- 'image_size' refers to the size of the input image, which is an important parameter in vision tasks. It specifies the height and width of the input image in pixels. In the example you provided, image_size is set to [1280, 960], which means that the input image has a height of 1280 pixels and a width of 960 pixels."
      ],
      "metadata": {
        "id": "4pAv_4NO8Mjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating config object and modify hyperparameter 'image_size' from encoder and 'max_length' from decoder keys"
      ],
      "metadata": {
        "id": "pkdZ3UpzAL3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = VisionEncoderDecoderConfig.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "L-JUwoA9a6aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(config)"
      ],
      "metadata": {
        "id": "35W79VsaYnBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eec2620-caff-4b97-82ec-b5e4b139a3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.vision_encoder_decoder.configuration_vision_encoder_decoder.VisionEncoderDecoderConfig"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "id": "V1bh93iCu4H9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa133ef-3f9c-4c27-b52d-584310435d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionEncoderDecoderConfig {\n",
            "  \"_commit_hash\": \"a959cf33c20e09215873e338299c900f57047c61\",\n",
            "  \"architectures\": [\n",
            "    \"VisionEncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"_name_or_path\": \"\",\n",
            "    \"activation_dropout\": 0.0,\n",
            "    \"activation_function\": \"gelu\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"add_final_layer_norm\": true,\n",
            "    \"architectures\": null,\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"begin_suppress_tokens\": null,\n",
            "    \"bos_token_id\": 0,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"classifier_dropout\": 0.0,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"d_model\": 1024,\n",
            "    \"decoder_attention_heads\": 16,\n",
            "    \"decoder_ffn_dim\": 4096,\n",
            "    \"decoder_layerdrop\": 0.0,\n",
            "    \"decoder_layers\": 4,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"dropout\": 0.1,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_attention_heads\": 16,\n",
            "    \"encoder_ffn_dim\": 4096,\n",
            "    \"encoder_layerdrop\": 0.0,\n",
            "    \"encoder_layers\": 12,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": 2,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"init_std\": 0.02,\n",
            "    \"is_decoder\": true,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 1536,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"mbart\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": 1,\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"scale_embedding\": true,\n",
            "    \"sep_token_id\": null,\n",
            "    \"suppress_tokens\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tf_legacy_loss\": false,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.31.0\",\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 57525\n",
            "  },\n",
            "  \"encoder\": {\n",
            "    \"_name_or_path\": \"\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": null,\n",
            "    \"attention_probs_dropout_prob\": 0.0,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"begin_suppress_tokens\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"depths\": [\n",
            "      2,\n",
            "      2,\n",
            "      14,\n",
            "      2\n",
            "    ],\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"drop_path_rate\": 0.1,\n",
            "    \"early_stopping\": false,\n",
            "    \"embed_dim\": 128,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.0,\n",
            "    \"hidden_size\": 1024,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"image_size\": [\n",
            "      2560,\n",
            "      1920\n",
            "    ],\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"layer_norm_eps\": 1e-05,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"min_length\": 0,\n",
            "    \"mlp_ratio\": 4.0,\n",
            "    \"model_type\": \"donut-swin\",\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_channels\": 3,\n",
            "    \"num_heads\": [\n",
            "      4,\n",
            "      8,\n",
            "      16,\n",
            "      32\n",
            "    ],\n",
            "    \"num_layers\": 4,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": null,\n",
            "    \"patch_size\": 4,\n",
            "    \"path_norm\": true,\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"qkv_bias\": true,\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"suppress_tokens\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tf_legacy_loss\": false,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": null,\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.31.0\",\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_absolute_embeddings\": false,\n",
            "    \"use_bfloat16\": false,\n",
            "    \"window_size\": 10\n",
            "  },\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"model_type\": \"vision-encoder-decoder\",\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": null\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## To see list of keys\n",
        "keys = dir(config)\n",
        "print(keys)"
      ],
      "metadata": {
        "id": "xhRe-sWSu39o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53058441-d8f2-4921-cfd3-e20354cbcc2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_commit_hash', '_create_repo', '_dict_from_json_file', '_get_config_dict', '_get_files_timestamps', '_name_or_path', '_set_token_in_kwargs', '_upload_modified_files', 'add_cross_attention', 'architectures', 'attribute_map', 'bad_words_ids', 'begin_suppress_tokens', 'bos_token_id', 'chunk_size_feed_forward', 'cross_attention_hidden_size', 'decoder', 'decoder_start_token_id', 'dict_torch_dtype_to_str', 'diversity_penalty', 'do_sample', 'early_stopping', 'encoder', 'encoder_no_repeat_ngram_size', 'eos_token_id', 'exponential_decay_length_penalty', 'finetuning_task', 'forced_bos_token_id', 'forced_eos_token_id', 'from_dict', 'from_encoder_decoder_configs', 'from_json_file', 'from_pretrained', 'get_config_dict', 'id2label', 'is_composition', 'is_decoder', 'is_encoder_decoder', 'label2id', 'length_penalty', 'max_length', 'min_length', 'model_type', 'name_or_path', 'no_repeat_ngram_size', 'num_beam_groups', 'num_beams', 'num_labels', 'num_return_sequences', 'output_attentions', 'output_hidden_states', 'output_scores', 'pad_token_id', 'prefix', 'problem_type', 'pruned_heads', 'push_to_hub', 'register_for_auto_class', 'remove_invalid_values', 'repetition_penalty', 'return_dict', 'return_dict_in_generate', 'save_pretrained', 'sep_token_id', 'suppress_tokens', 'task_specific_params', 'temperature', 'tf_legacy_loss', 'tie_encoder_decoder', 'tie_word_embeddings', 'to_dict', 'to_diff_dict', 'to_json_file', 'to_json_string', 'tokenizer_class', 'top_k', 'top_p', 'torch_dtype', 'torchscript', 'transformers_version', 'typical_p', 'update', 'update_from_string', 'use_bfloat16', 'use_return_dict']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comm: From here we want to access encoder and decoder keys and wqant to modify it"
      ],
      "metadata": {
        "id": "fwuQKLOC_lT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dynamically access the default values of the keys\n",
        "\n",
        "# Way- 1\n",
        "print(getattr(config.encoder, \"image_size\"))\n",
        "print(getattr(config.decoder, \"max_length\"))\n",
        "\n",
        "print('************************************************')\n",
        "# Way- 2\n",
        "print(config.encoder.image_size)\n",
        "print(config.decoder.max_length)"
      ],
      "metadata": {
        "id": "IKr-GRGNu3zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fd11f2-94cd-4366-baa5-665fe37a51cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2560, 1920]\n",
            "20\n",
            "************************************************\n",
            "[2560, 1920]\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up max_length and image_size for our task\n",
        "max_length = 128        # maximum length of the generated text / tokens that the decoder can generate for each input image\n",
        "image_size = [1280, 960]    # size of the input image. It specifies the height and width of the input image in pixels"
      ],
      "metadata": {
        "id": "a3X1_Zo3a6c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating cofig value"
      ],
      "metadata": {
        "id": "nUtoHkfjB-KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Updating the configuration of a pre-trained VisionEncoderDecoderConfig instance\n",
        "\n",
        "config.encoder.image_size  = image_size    # (height, width)\n",
        "config.decoder.max_length  = max_length  # update max_length of the decoder (for generation of text)"
      ],
      "metadata": {
        "id": "d5g1Uf8Pu3gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking new and updating value\n",
        "print(config.encoder.image_size)\n",
        "print(config.decoder.max_length)"
      ],
      "metadata": {
        "id": "yZqODpUBDxFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163ea60e-05d5-43fb-bd0e-dd549151c493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1280, 960]\n",
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Encoder and Decoder Model Initialization\n"
      ],
      "metadata": {
        "id": "A9Drc4O6DhJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionEncoderDecoderModel.from_pretrained(model_checkpoint, config=config)"
      ],
      "metadata": {
        "id": "c6TGm7JMu3XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config == config    ## both are same"
      ],
      "metadata": {
        "id": "j1VC3INgbRPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ca67ac-d95d-445b-f1b7-c87ccd340276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.decoder_start_token_id"
      ],
      "metadata": {
        "id": "K6n4vsrz-ho0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add special tokens\n"
      ],
      "metadata": {
        "id": "pH1BYBUDR6Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List"
      ],
      "metadata": {
        "id": "KgpnFJRsasLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(processor.tokenizer)"
      ],
      "metadata": {
        "id": "4zEI2sS71KDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af1995b5-a509-4605-e616-b263435a976e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57526"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = ['abc', 'xyz']\n",
        "print(l)\n",
        "nwl = processor.tokenizer.add_tokens(l)\n",
        "print(nwl)"
      ],
      "metadata": {
        "id": "a3UWPQNwEpmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f387deea-6789-4fd1-a1c5-97a7ca1c8afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abc', 'xyz']\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = ['', '']\n",
        "print(l1)\n",
        "nwl1 = processor.tokenizer.add_tokens(l1)\n",
        "print(nwl1)"
      ],
      "metadata": {
        "id": "kll05Qm3yyKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d6f045-db0e-4355-9847-cc5861726781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '']\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: add_tokens: Empty strings\n",
        "- 'add_tokens' method returns the number of newly added tokens, which is stored in the newly added variable (nwl or nwl1)\n",
        "- add_tokens method only adds new tokens to the tokenizer's vocabulary if the tokens are not already present in the vocabulary. Since an empty string (\"\") is already a valid token in the tokenizer's vocabulary, passing it to the add_tokens method does not actually add any new tokens to the vocabulary.\n",
        "\n",
        "- So even though the list [\"\", \"\"] is passed as an argument to the add_tokens method, the method does not actually add any new tokens to the tokenizer's vocabulary, as both empty strings are already valid tokens.\n",
        "- Since the pre-trained tokenizer already has a large vocabulary, it is possible that the new tokens you added are already in the vocabulary. In that case, the add_tokens method will not add any new tokens to the vocabulary and will return 0.\n",
        "\n",
        "- In your case, the add_tokens method returned 0 because it did not add any new tokens to the vocabulary. This could be because the tokens 'abc' and 'xyz' are already present in the pre-trained tokenizer's vocabulary."
      ],
      "metadata": {
        "id": "Fwc5y9G3ydv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q: Why empty strings is not considered as a token in add_token from tokenizer?\n",
        "Empty strings (\"\") are not considered as new tokens because they do not add any additional information to the existing tokens. Empty strings are essentially the absence of any characters, so adding them as new tokens would not provide any new information for the model to learn from.\n",
        "\n",
        "Moreover, empty strings are not valid tokens because they do not have any meaning or semantic value in natural language. Therefore, they cannot be used to represent any information in the text.\n",
        "\n",
        "When adding tokens to a tokenizer, each token should represent a distinct concept or entity that appears in the text. Adding empty strings as tokens would not serve any useful purpose and would only increase the size of the token vocabulary unnecessarily."
      ],
      "metadata": {
        "id": "0_tTery0zpQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_tokens(list_of_tokens: List[str]):\n",
        "  '''\n",
        "  Add tokens to the tokenizer and resize the token embeddings\n",
        "  newly_added_num -> add_tokens method returns the number of newly added tokens, which is stored in the newly_added_num\n",
        "  resize_token_embeddings -> the size of the token embeddings in the model's decoder is resized to match the new size of the tokenizer\n",
        "  model.decoder.resize_token_embeddings() method, which takes the length of the tokenizer as input\n",
        "  '''\n",
        "  newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)   # returns number of newly added tokens\n",
        "\n",
        "  if newly_added_num > 0:   # if neewly added number is greater than 0 then token embedding inside model decoder will be resized\n",
        "    model.decoder.resize_position_embeddings(len(processor.tokenizer))"
      ],
      "metadata": {
        "id": "Wh4QLhtDEpt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "additional_tokens = [\"<yes/>\", \"<no/>\"]"
      ],
      "metadata": {
        "id": "eqrQ9YMaEpj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Objective of Donut Model: Explanation of decoder_input_ids and labels tensor\n",
        "- Obj: Donut model, the goal is to generate a textual answer to a visual question. The model takes in an image and a prompt, which consists of a question and an optional answer. The input is processed by the encoder, which generates a fixed-length vector representation of the image and the prompt. This representation is then fed into the decoder to generate the output sequence, which is the textual answer.\n",
        "- The image would be encoded into a feature vector using the vision encoder, and this feature vector would be used as the input to the language decoder. The language decoder would then generate the answer text token by token, using a masked language modeling objective.\n",
        "\n",
        "- 'decoder_input_ids' tensor is an important component of the decoding process. It represents the input tokens that are fed into the decoder to generate the next set of tokens. The decoder_input_ids tensor is constructed from the previous set of generated tokens and is used to provide context to the decoder during the generation process.\n",
        "- In the case of question answering, the prompt consists of both the question and the answer. The goal of the model is to generate the answer given the question and the image. However, we don't want the model to simply memorize the answer and regurgitate (to repeat information without understanding it) it during training. Instead, we want the model to learn how to generate the answer from the question and the image.\n",
        "- The decoder_input_ids tensor represents the input to the language decoder. It consists of the special [CLS] token followed by the token IDs for the answer text. During training, the language decoder would use this input to generate the answer text one token at a time, using a masked language modeling objective.\n",
        "- 'labels' tensor we created earlier, we set the tokens for the question to -100, which tells the model to ignore these tokens during training. The labels tensor is used as the target during training to compute the loss and update the model parameters.\n",
        "- The decoder_input_ids tensor represents the input to the language decoder. The 'decoder_input_ids' tensor only contains the token IDs for the answer portion of the prompt, and does not include the question portion.\n",
        "- To summarize, during training in the Donut vision encoder-decoder model, only the answer portion of the prompt is used as the target for the model to predict, and the question portion is ignored. The decoder_input_ids tensor only contains the token IDs for the answer portion of the prompt. The model is trained using cross-entropy loss to predict the correct token IDs for the answer portion of the prompt, given the encoded image and the decoder input IDs.\n",
        "- During training, the model generates text only based on the decoder_input_ids tensor, which contains the token IDs for the answer portion of the prompt. The model is trained to generate the correct sequence of tokens for the answer based on the encoded image and the decoder_input_ids. The question portion of the prompt is ignored during training, and the model does not generate any text based on it.\n",
        "\n",
        "  - ** Q: During training as there is no role of question part inside the whole prompt then why we includes always both question and answer part inside the prompt structure?\n",
        "    - Including the question portion of the prompt along with the answer portion serves as a kind of \"context\" for the model during training. Even though the model is only trained to generate the answer portion of the prompt, having the question portion as part of the input helps the model understand the context in which the answer is being given. This can improve the model's ability to generate relevant and accurate answers.\n",
        "    - In addition, including the question portion in the prompt allows for the possibility of using the same pre-trained model to generate answers to new questions that were not seen during training. By providing the model with the full prompt, which includes both the question and answer portions, the model can better understand the context of the question being asked and generate more accurate answers.\n",
        "  - ** Q: Role of 'labels' parameter?\n",
        "    - The role of the labels parameter is to specify which tokens in the decoder_input_ids tensor should be used as targets for the model during training. In particular, the labels tensor contains the same token sequence as the decoder_input_ids tensor, but with some tokens replaced by a special padding token (-100 in this case). Specifically, the tokens in the labels tensor that correspond to the question portion of the prompt are replaced by -100, indicating that these tokens should be ignored during training.\n",
        "\n",
        "    - So by specifying the labels tensor in this way, we are essentially telling the model to focus on generating the answer portion of the prompt and to ignore the question portion."
      ],
      "metadata": {
        "id": "FtaL6MJmPf5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x_OTjfDrFD0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRHYFOOIgu6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}